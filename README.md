# Projeto Airflow GCP & PostgreSQL

## ğŸ“š SumÃ¡rio
- [DescriÃ§Ã£o do projeto](#descriÃ§Ã£o-do-projeto)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [ConfiguraÃ§Ã£o do projeto](#configuraÃ§Ã£o-do-projeto)
  - [VariÃ¡veis para GCP](#variÃ¡veis-para-gcp)
  - [ConexÃµes no Airflow](#conexÃµes-no-airflow)
  - [Arquivos SQL](#arquivos-sql)
- [ExecuÃ§Ã£o](#execuÃ§Ã£o)
- [PersonalizaÃ§Ã£o](#personalizaÃ§Ã£o)
  - [ConexÃµes PostgreSQL](#conexÃµes-postgresql)
  - [Arquivos SQL customizados](#arquivos-sql-customizados)
  - [Buckets e tabelas no GCP](#buckets-e-tabelas-no-gcp)
- [Fluxo GCS to BQ](#fluxo-gcs-to-bq)
- [Contato](#contato)

---

## ğŸ“ DescriÃ§Ã£o do projeto

Este repositÃ³rio contÃ©m o cÃ³digo-fonte de dois fluxos de trabalho desenvolvidos com o Apache Airflow:

- **Infraestrutura no Google Cloud Platform (GCP):**
  - Cria buckets no Google Cloud Storage (GCS).
  - Cria datasets e tabelas no BigQuery.
  - Organiza o pipeline em uma DAG chamada `create_infra`.

- **Banco de dados PostgreSQL:**
  - Cria tabelas e insere dados em um banco PostgreSQL.
  - Utiliza scripts SQL versionados e controlados via Airflow.

---

## ğŸš€ PrÃ©-requisitos

Para executar este projeto, Ã© necessÃ¡rio ter:

- [Apache Airflow](https://airflow.apache.org/) configurado e rodando no seu ambiente.
- [Google Cloud SDK](https://cloud.google.com/sdk) instalado e autenticado.
- Conta no Google Cloud Platform com permissÃµes para criar buckets no GCS e datasets no BigQuery.

---

## âš™ï¸ ConfiguraÃ§Ã£o do projeto

### ğŸ”§ VariÃ¡veis para GCP

No seu DAG ou ambiente, configure as seguintes variÃ¡veis:

- `project_id`: ID do projeto no Google Cloud Platform.
- `region`: RegiÃ£o onde o bucket serÃ¡ criado.
- `bucket_name_raw`: Nome do bucket no GCS para arquivos de entrada e saÃ­da.
- `dataset_name`: Nome do dataset no BigQuery.
- `tabela_name`: Nome da tabela no BigQuery.
- `bucket_name_datalake`: Nome do bucket na zona de ingestÃ£o (GCS).

### ğŸ”Œ ConexÃµes no Airflow

#### PostgreSQL
- Configure uma conexÃ£o no Airflow com o ID `postgres-airflow` apontando para seu banco de dados PostgreSQL.
- Se desejar, altere o `postgres_conn_id` no cÃ³digo para corresponder ao ID da sua conexÃ£o.

#### Google Cloud
- Configure a conexÃ£o `google_cloud_default` no Airflow com seu arquivo JSON de chave de serviÃ§o do GCP.

### ğŸ“‚ Arquivos SQL

Garanta que os seguintes arquivos estejam no diretÃ³rio configurado em `template_searchpath` no seu DAG do Airflow:

- `create_table_db.sql`
- `insere_dados_table_db.sql`
- `create_table_ed.sql`
- `insere_dados_tb_data_engineering.sql`

---

## â–¶ï¸ ExecuÃ§Ã£o

Para rodar o pipeline:

1. Certifique-se de que o Airflow estÃ¡ em execuÃ§Ã£o (`airflow webserver` e `airflow scheduler`).
2. Confirme que as conexÃµes e variÃ¡veis estÃ£o configuradas corretamente no Airflow.
3. O fluxo principal `create_infra` serÃ¡ executado com o seguinte encadeamento:


create_bucket_raw >> create_dataset >> create_table_bq


4. O pipeline do PostgreSQL cria as tabelas e insere dados conforme os arquivos SQL, obedecendo o `start_date` e `schedule_interval` definidos no DAG.

---

## ğŸ› ï¸ PersonalizaÃ§Ã£o

### ğŸ”„ ConexÃµes PostgreSQL
Caso utilize outro ID de conexÃ£o, ajuste no seu cÃ³digo:
```python
postgres_conn_id="postgres-airflow"
para o ID correspondente ao configurado no Airflow.

âœï¸ Arquivos SQL customizados
Edite ou substitua os arquivos .sql mencionados para refletir o seu modelo de dados ou novas regras de negÃ³cio.

â˜ï¸ Buckets e tabelas no GCP
Personalize as variÃ¡veis no DAG para utilizar outros buckets e tabelas:

bucket_name_raw e bucket_name_datalake no GCS.

dataset_name e tabela_name no BigQuery.

â›“ï¸ Fluxo GCS to BQ
Este projeto tambÃ©m contÃ©m um pipeline para copiar dados do Google Cloud Storage (GCS) para o BigQuery:

IngestÃ£o: copia arquivos do bucket datalake_jr para o bucket_raw_jr.

Carregamento: importa dados para o dataset my-dataset (ou venda) e para a tabela tb_vendas.

Como executar o GCS to BQ
Certifique-se de que o Airflow esteja rodando.

Configure as variÃ¡veis bucket_name_datalake, bucket_name_raw, DATASET_CURATED e TABELA_CURATED no DAG conforme seu ambiente.

O fluxo serÃ¡ acionado automaticamente pelo schedule_interval definido.

ğŸ“¬ Contato
Caso tenha dÃºvidas, sugestÃµes ou queira contribuir, entre em contato pelo e-mail:

css
Copiar
Editar
valterlafuentejunior@gmail.com
ğŸš€ Projeto desenvolvido para facilitar pipelines de dados usando o Airflow em conjunto com GCP e PostgreSQL.
