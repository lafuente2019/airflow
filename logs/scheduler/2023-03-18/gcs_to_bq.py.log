[2023-03-18T00:00:21.491+0000] {processor.py:153} INFO - Started process (PID=24698) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:00:21.493+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:00:21.495+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:00:21.495+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:00:21.537+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:00:21.533+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:00:21.539+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:00:21.635+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.149 seconds
[2023-03-18T00:00:51.878+0000] {processor.py:153} INFO - Started process (PID=24770) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:00:51.880+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:00:51.881+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:00:51.881+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:00:51.918+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:00:51.917+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:00:51.920+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:00:51.956+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-18T00:01:22.157+0000] {processor.py:153} INFO - Started process (PID=24843) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:01:22.159+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:01:22.160+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:01:22.160+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:01:22.187+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:01:22.186+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:01:22.188+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:01:22.215+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-18T00:01:52.673+0000] {processor.py:153} INFO - Started process (PID=24917) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:01:52.675+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:01:52.677+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:01:52.676+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:01:52.711+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:01:52.709+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:01:52.713+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:01:52.745+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-18T00:02:23.890+0000] {processor.py:153} INFO - Started process (PID=24990) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:02:23.892+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:02:23.893+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:02:23.893+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:02:23.923+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:02:23.921+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:02:23.929+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:02:23.967+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.084 seconds
[2023-03-18T00:02:54.581+0000] {processor.py:153} INFO - Started process (PID=25070) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:02:54.592+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:02:54.596+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:02:54.596+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:02:54.668+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:02:54.666+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:02:54.670+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:02:54.750+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.180 seconds
[2023-03-18T00:03:25.265+0000] {processor.py:153} INFO - Started process (PID=25136) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:03:25.267+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:03:25.268+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:03:25.268+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:03:25.304+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:03:25.302+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:03:25.306+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:03:25.336+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-18T00:03:55.819+0000] {processor.py:153} INFO - Started process (PID=25216) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:03:55.822+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:03:55.824+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:03:55.823+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:03:55.868+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:03:55.866+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:03:55.869+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:03:55.899+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-18T00:04:26.817+0000] {processor.py:153} INFO - Started process (PID=25297) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:04:26.818+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:04:26.823+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:04:26.822+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:04:26.881+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:04:26.879+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:04:26.882+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:04:26.951+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.140 seconds
[2023-03-18T00:04:57.336+0000] {processor.py:153} INFO - Started process (PID=25370) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:04:57.340+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:04:57.344+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:04:57.343+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:04:57.391+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:04:57.389+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:04:57.393+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:04:57.492+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.164 seconds
[2023-03-18T00:05:28.093+0000] {processor.py:153} INFO - Started process (PID=25443) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:05:28.096+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:05:28.100+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:05:28.100+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:05:28.140+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:05:28.138+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:05:28.141+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:05:28.180+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.093 seconds
[2023-03-18T00:05:58.867+0000] {processor.py:153} INFO - Started process (PID=25515) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:05:58.868+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:05:58.869+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:05:58.869+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:05:58.913+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:05:58.912+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:05:58.914+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:05:58.939+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.075 seconds
[2023-03-18T00:06:29.167+0000] {processor.py:153} INFO - Started process (PID=25588) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:06:29.168+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:06:29.170+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:06:29.169+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:06:29.198+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:06:29.196+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:06:29.199+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:06:29.413+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.251 seconds
[2023-03-18T00:07:00.244+0000] {processor.py:153} INFO - Started process (PID=25677) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:07:00.245+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:07:00.246+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:07:00.246+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:07:00.281+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:07:00.280+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:07:00.282+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:07:00.303+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.064 seconds
[2023-03-18T00:07:30.819+0000] {processor.py:153} INFO - Started process (PID=25750) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:07:30.821+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:07:30.822+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:07:30.822+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:07:30.856+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:07:30.854+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:07:30.857+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:07:30.886+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-18T00:08:01.888+0000] {processor.py:153} INFO - Started process (PID=25839) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:08:01.890+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:08:01.893+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:08:01.893+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:08:01.948+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:08:01.944+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:08:01.950+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:08:02.001+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.119 seconds
[2023-03-18T00:08:32.139+0000] {processor.py:153} INFO - Started process (PID=25912) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:08:32.141+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:08:32.142+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:08:32.142+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:08:32.181+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:08:32.179+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:08:32.183+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:08:32.212+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-18T00:09:02.312+0000] {processor.py:153} INFO - Started process (PID=25983) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:09:02.313+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:09:02.314+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:09:02.314+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:09:02.358+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:09:02.357+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:09:02.360+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:09:02.386+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.079 seconds
[2023-03-18T00:09:32.574+0000] {processor.py:153} INFO - Started process (PID=26056) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:09:32.576+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:09:32.577+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:09:32.577+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:09:32.614+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:09:32.613+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:09:32.615+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:09:32.644+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.074 seconds
[2023-03-18T00:10:02.883+0000] {processor.py:153} INFO - Started process (PID=26128) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:10:02.889+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:10:02.892+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:10:02.892+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:10:02.957+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:10:02.955+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:10:02.959+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:10:03.017+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.148 seconds
[2023-03-18T00:10:33.359+0000] {processor.py:153} INFO - Started process (PID=26201) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:10:33.361+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:10:33.366+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:10:33.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:10:33.406+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:10:33.402+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:10:33.407+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:10:33.458+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.108 seconds
[2023-03-18T00:11:03.838+0000] {processor.py:153} INFO - Started process (PID=26281) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:11:03.840+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:11:03.842+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:11:03.841+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:11:03.876+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:11:03.874+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:11:03.877+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:11:03.917+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-18T00:11:34.075+0000] {processor.py:153} INFO - Started process (PID=26345) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:11:34.076+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:11:34.077+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:11:34.077+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:11:34.107+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:11:34.106+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:11:34.108+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:11:34.136+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-18T00:12:04.954+0000] {processor.py:153} INFO - Started process (PID=26418) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:12:04.956+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:12:04.958+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:12:04.958+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:12:04.999+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:12:04.997+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:12:05.000+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:12:05.041+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-18T00:12:35.603+0000] {processor.py:153} INFO - Started process (PID=26507) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:12:35.604+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:12:35.606+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:12:35.606+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:12:35.646+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:12:35.644+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:12:35.647+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:12:35.679+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.083 seconds
[2023-03-18T00:13:06.097+0000] {processor.py:153} INFO - Started process (PID=26573) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:13:06.112+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:13:06.114+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:13:06.114+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:13:06.185+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:13:06.183+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:13:06.187+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:13:06.313+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.228 seconds
[2023-03-18T00:13:36.384+0000] {processor.py:153} INFO - Started process (PID=26653) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:13:36.386+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:13:36.387+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:13:36.387+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:13:36.427+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:13:36.425+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:13:36.430+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:13:36.457+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-18T00:14:07.195+0000] {processor.py:153} INFO - Started process (PID=26726) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:14:07.196+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:14:07.197+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:14:07.197+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:14:07.239+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:14:07.236+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:14:07.241+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:14:07.261+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-18T00:14:37.379+0000] {processor.py:153} INFO - Started process (PID=26799) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:14:37.381+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:14:37.382+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:14:37.382+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:14:37.406+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:14:37.404+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:14:37.407+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:14:37.429+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.053 seconds
[2023-03-18T00:15:08.014+0000] {processor.py:153} INFO - Started process (PID=26881) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:15:08.020+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:15:08.021+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:15:08.021+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:15:08.052+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:15:08.050+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:15:08.053+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:15:08.125+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.119 seconds
[2023-03-18T00:15:38.867+0000] {processor.py:153} INFO - Started process (PID=26963) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:15:38.868+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:15:38.870+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:15:38.870+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:15:38.906+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:15:38.904+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:15:38.907+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:15:38.945+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-18T00:16:09.453+0000] {processor.py:153} INFO - Started process (PID=27036) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:16:09.455+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:16:09.456+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:16:09.456+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:16:09.502+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:16:09.501+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:16:09.503+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:16:09.658+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.216 seconds
[2023-03-18T00:16:39.779+0000] {processor.py:153} INFO - Started process (PID=27110) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:16:39.781+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:16:39.782+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:16:39.781+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:16:39.807+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:16:39.806+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:16:39.808+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:16:39.833+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.057 seconds
[2023-03-18T00:17:10.705+0000] {processor.py:153} INFO - Started process (PID=27189) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:17:10.706+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:17:10.709+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:17:10.708+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:17:10.752+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:17:10.749+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:17:10.753+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:17:10.787+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.087 seconds
[2023-03-18T00:17:41.688+0000] {processor.py:153} INFO - Started process (PID=27271) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:17:41.696+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:17:41.698+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:17:41.698+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:17:41.802+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:17:41.800+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:17:41.803+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:17:41.849+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.168 seconds
[2023-03-18T00:18:12.379+0000] {processor.py:153} INFO - Started process (PID=27344) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:18:12.380+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:18:12.381+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:18:12.381+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:18:12.404+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:18:12.403+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:18:12.404+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:18:12.422+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.045 seconds
[2023-03-18T00:18:42.588+0000] {processor.py:153} INFO - Started process (PID=27417) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:18:42.590+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:18:42.591+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:18:42.591+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:18:42.613+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:18:42.612+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:18:42.614+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:18:42.631+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.047 seconds
[2023-03-18T00:19:13.217+0000] {processor.py:153} INFO - Started process (PID=27490) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:19:13.219+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:19:13.220+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:19:13.220+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:19:13.254+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:19:13.252+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:19:13.255+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:19:13.283+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-18T00:19:43.695+0000] {processor.py:153} INFO - Started process (PID=27579) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:19:43.696+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:19:43.697+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:19:43.697+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:19:43.725+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:19:43.724+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:19:43.726+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:19:43.765+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-18T00:20:14.384+0000] {processor.py:153} INFO - Started process (PID=27653) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:20:14.385+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:20:14.386+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:20:14.386+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:20:14.424+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:20:14.423+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:20:14.425+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:20:14.450+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-18T00:20:44.924+0000] {processor.py:153} INFO - Started process (PID=27726) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:20:44.926+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:20:44.927+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:20:44.927+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:20:44.956+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:20:44.955+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:20:44.957+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:20:44.980+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-18T00:21:15.081+0000] {processor.py:153} INFO - Started process (PID=27815) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:21:15.082+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:21:15.083+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:21:15.083+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:21:15.123+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:21:15.121+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:21:15.125+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:21:15.158+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.081 seconds
[2023-03-18T00:21:45.712+0000] {processor.py:153} INFO - Started process (PID=27888) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:21:45.713+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:21:45.714+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:21:45.714+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:21:45.751+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:21:45.750+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:21:45.751+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:21:45.805+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.099 seconds
[2023-03-18T00:22:16.097+0000] {processor.py:153} INFO - Started process (PID=27961) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:22:16.098+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:22:16.100+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:22:16.099+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:22:16.135+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:22:16.134+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:22:16.136+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:22:16.271+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.176 seconds
[2023-03-18T00:22:46.678+0000] {processor.py:153} INFO - Started process (PID=28043) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:22:46.680+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:22:46.681+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:22:46.681+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:22:46.716+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:22:46.714+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:22:46.716+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:22:46.752+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.077 seconds
[2023-03-18T00:23:17.716+0000] {processor.py:153} INFO - Started process (PID=28116) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:23:17.726+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:23:17.729+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:23:17.728+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:23:17.794+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:23:17.791+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:23:17.796+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:23:17.856+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.146 seconds
[2023-03-18T00:23:48.113+0000] {processor.py:153} INFO - Started process (PID=28190) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:23:48.115+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:23:48.117+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:23:48.117+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:23:48.162+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:23:48.160+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:23:48.163+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:23:48.223+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.118 seconds
[2023-03-18T00:24:18.445+0000] {processor.py:153} INFO - Started process (PID=28271) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:24:18.446+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:24:18.447+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:24:18.447+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:24:18.476+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:24:18.475+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:24:18.477+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:24:18.500+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.058 seconds
[2023-03-18T00:24:49.213+0000] {processor.py:153} INFO - Started process (PID=28344) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:24:49.215+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:24:49.216+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:24:49.216+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:24:49.255+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:24:49.253+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:24:49.257+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:24:49.296+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-18T00:25:19.526+0000] {processor.py:153} INFO - Started process (PID=28418) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:25:19.527+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:25:19.529+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:25:19.528+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:25:19.567+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:25:19.565+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:25:19.569+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:25:19.608+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-18T00:25:49.653+0000] {processor.py:153} INFO - Started process (PID=28491) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:25:49.655+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:25:49.656+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:25:49.655+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:25:49.684+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:25:49.682+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:25:49.685+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:25:49.707+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.058 seconds
[2023-03-18T00:26:20.634+0000] {processor.py:153} INFO - Started process (PID=28564) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:26:20.635+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:26:20.636+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:26:20.636+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:26:20.673+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:26:20.671+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:26:20.674+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:26:20.741+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.111 seconds
[2023-03-18T00:26:51.607+0000] {processor.py:153} INFO - Started process (PID=28637) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:26:51.608+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:26:51.609+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:26:51.609+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:26:51.630+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:26:51.629+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:26:51.630+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:26:51.650+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.045 seconds
[2023-03-18T00:27:21.803+0000] {processor.py:153} INFO - Started process (PID=28726) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:27:21.805+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:27:21.809+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:27:21.808+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:27:21.851+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:27:21.849+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:27:21.852+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:27:21.901+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.104 seconds
[2023-03-18T00:27:52.754+0000] {processor.py:153} INFO - Started process (PID=28799) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:27:52.756+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:27:52.757+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:27:52.757+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:27:52.789+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:27:52.788+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:27:52.789+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:27:52.810+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.058 seconds
[2023-03-18T00:28:23.378+0000] {processor.py:153} INFO - Started process (PID=28889) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:28:23.379+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:28:23.380+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:28:23.380+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:28:23.396+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:28:23.395+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:28:23.397+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:28:23.414+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.040 seconds
[2023-03-18T00:28:53.637+0000] {processor.py:153} INFO - Started process (PID=28962) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:28:53.639+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:28:53.640+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:28:53.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:28:53.678+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:28:53.676+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:28:53.679+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:28:53.703+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-18T00:29:24.338+0000] {processor.py:153} INFO - Started process (PID=29044) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:29:24.339+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:29:24.341+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:29:24.341+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:29:24.373+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:29:24.371+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:29:24.374+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:29:24.397+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.061 seconds
[2023-03-18T00:29:54.931+0000] {processor.py:153} INFO - Started process (PID=29124) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:29:54.933+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:29:54.934+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:29:54.934+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:29:54.983+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:29:54.982+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:29:54.984+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:29:55.014+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-18T00:30:25.822+0000] {processor.py:153} INFO - Started process (PID=29197) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:30:25.824+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:30:25.825+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:30:25.825+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:30:25.860+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:30:25.858+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:30:25.863+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:30:25.893+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.074 seconds
[2023-03-18T00:30:56.594+0000] {processor.py:153} INFO - Started process (PID=29286) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:30:56.595+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:30:56.596+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:30:56.596+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:30:56.634+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:30:56.633+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:30:56.635+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:30:56.783+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.192 seconds
[2023-03-18T00:31:27.079+0000] {processor.py:153} INFO - Started process (PID=29359) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:31:27.081+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:31:27.082+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:31:27.082+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:31:27.121+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:31:27.120+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:31:27.123+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:31:27.152+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.077 seconds
[2023-03-18T00:31:57.651+0000] {processor.py:153} INFO - Started process (PID=29448) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:31:57.653+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:31:57.654+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:31:57.654+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:31:57.690+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:31:57.689+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:31:57.691+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:31:57.714+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-18T00:32:28.616+0000] {processor.py:153} INFO - Started process (PID=29521) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:32:28.617+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:32:28.618+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:32:28.618+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:32:28.663+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:32:28.662+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:32:28.663+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:32:28.685+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-18T00:32:58.855+0000] {processor.py:153} INFO - Started process (PID=29600) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:32:58.857+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:32:58.858+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:32:58.858+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:32:58.897+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:32:58.896+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:32:58.898+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:32:58.920+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-18T00:33:29.377+0000] {processor.py:153} INFO - Started process (PID=29684) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:33:29.378+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:33:29.379+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:33:29.379+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:33:29.406+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:33:29.405+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:33:29.407+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:33:29.430+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-18T00:33:59.969+0000] {processor.py:153} INFO - Started process (PID=29757) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:33:59.970+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:33:59.971+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:33:59.971+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:34:00.013+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:34:00.011+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:34:00.015+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:34:00.060+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-18T00:34:30.416+0000] {processor.py:153} INFO - Started process (PID=29837) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:34:30.418+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:34:30.421+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:34:30.420+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:34:30.463+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:34:30.460+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:34:30.471+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:34:30.544+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.145 seconds
[2023-03-18T00:51:57.329+0000] {processor.py:153} INFO - Started process (PID=77) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:51:57.339+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:51:57.352+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:51:57.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:51:57.811+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:51:57.787+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:51:57.833+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:51:57.998+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.678 seconds
[2023-03-18T00:52:28.174+0000] {processor.py:153} INFO - Started process (PID=143) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:52:28.179+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:52:28.220+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:52:28.220+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:52:28.787+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:52:28.785+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:52:28.794+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:52:28.895+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.754 seconds
[2023-03-18T00:52:59.893+0000] {processor.py:153} INFO - Started process (PID=210) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:52:59.904+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:52:59.925+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:52:59.925+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:53:00.250+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:53:00.244+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:53:00.254+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:53:00.354+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.491 seconds
[2023-03-18T00:53:30.937+0000] {processor.py:153} INFO - Started process (PID=275) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:53:30.947+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:53:30.951+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:53:30.950+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:53:31.093+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:53:31.089+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:53:31.101+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:53:31.255+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.335 seconds
[2023-03-18T00:54:02.236+0000] {processor.py:153} INFO - Started process (PID=341) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:54:02.238+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:54:02.240+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:54:02.240+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:54:02.334+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:54:02.332+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:54:02.336+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:54:02.419+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.202 seconds
[2023-03-18T00:54:32.855+0000] {processor.py:153} INFO - Started process (PID=405) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:54:32.857+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:54:32.868+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:54:32.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:54:32.938+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:54:32.935+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:54:32.940+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:54:33.050+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.202 seconds
[2023-03-18T00:55:06.845+0000] {processor.py:153} INFO - Started process (PID=476) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:55:06.935+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:55:06.991+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:55:06.991+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:55:08.203+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:55:08.201+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:55:08.215+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:55:08.366+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 1.538 seconds
[2023-03-18T00:55:42.057+0000] {processor.py:153} INFO - Started process (PID=539) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:55:42.179+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:55:42.213+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:55:42.212+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:55:43.859+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:55:43.857+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:55:43.890+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:55:44.035+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 1.997 seconds
[2023-03-18T00:56:14.935+0000] {processor.py:153} INFO - Started process (PID=603) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:56:14.948+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:56:14.966+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:56:14.965+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:56:15.187+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:56:15.184+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:56:15.192+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:56:15.291+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.364 seconds
[2023-03-18T00:56:46.064+0000] {processor.py:153} INFO - Started process (PID=676) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:56:46.088+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:56:46.104+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:56:46.104+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:56:46.389+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:56:46.386+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:56:46.391+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:56:46.500+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.455 seconds
[2023-03-18T00:57:16.869+0000] {processor.py:153} INFO - Started process (PID=733) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:57:16.880+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:57:16.883+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:57:16.883+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:57:16.966+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:57:16.964+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:57:16.968+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:57:17.023+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.162 seconds
[2023-03-18T00:57:47.299+0000] {processor.py:153} INFO - Started process (PID=807) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:57:47.302+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:57:47.313+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:57:47.312+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:57:47.422+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:57:47.420+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:57:47.428+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:57:47.523+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.237 seconds
[2023-03-18T00:58:17.927+0000] {processor.py:153} INFO - Started process (PID=863) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:58:17.939+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:58:17.975+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:58:17.974+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:58:18.038+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:58:18.036+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:58:18.041+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:58:18.134+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.218 seconds
[2023-03-18T00:58:48.610+0000] {processor.py:153} INFO - Started process (PID=935) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:58:48.624+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:58:48.642+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:58:48.626+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:58:48.776+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:58:48.774+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:58:48.778+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:58:48.899+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.308 seconds
[2023-03-18T00:59:19.076+0000] {processor.py:153} INFO - Started process (PID=990) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:59:19.097+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:59:19.099+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:59:19.099+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:59:19.181+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:59:19.178+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:59:19.182+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:59:19.298+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.241 seconds
[2023-03-18T00:59:49.471+0000] {processor.py:153} INFO - Started process (PID=1061) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:59:49.480+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T00:59:49.488+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:59:49.488+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:59:49.765+0000] {logging_mixin.py:137} INFO - [2023-03-18T00:59:49.762+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T00:59:49.766+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T00:59:49.830+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.366 seconds
[2023-03-18T01:00:20.182+0000] {processor.py:153} INFO - Started process (PID=1117) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:00:20.185+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:00:20.187+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:00:20.187+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:00:20.270+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:00:20.268+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:00:20.276+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:00:20.347+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.184 seconds
[2023-03-18T01:00:50.726+0000] {processor.py:153} INFO - Started process (PID=1187) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:00:50.740+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:00:50.744+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:00:50.742+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:00:50.947+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:00:50.944+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:00:50.949+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:00:51.023+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.317 seconds
[2023-03-18T01:01:21.399+0000] {processor.py:153} INFO - Started process (PID=1251) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:01:21.403+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:01:21.410+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:01:21.409+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:01:21.488+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:01:21.484+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:01:21.489+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:01:21.550+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.157 seconds
[2023-03-18T01:01:51.946+0000] {processor.py:153} INFO - Started process (PID=1317) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:01:51.953+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:01:51.957+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:01:51.956+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:01:52.028+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:01:52.020+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:01:52.030+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:01:52.119+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.183 seconds
[2023-03-18T01:02:22.454+0000] {processor.py:153} INFO - Started process (PID=1383) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:02:22.457+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:02:22.459+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:02:22.459+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:02:22.558+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:02:22.556+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:02:22.561+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:02:22.610+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.176 seconds
[2023-03-18T01:02:52.971+0000] {processor.py:153} INFO - Started process (PID=1445) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:02:52.975+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:02:52.979+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:02:52.978+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:02:53.243+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:02:53.240+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:02:53.245+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:02:53.375+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.420 seconds
[2023-03-18T01:03:24.234+0000] {processor.py:153} INFO - Started process (PID=1510) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:03:24.265+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:03:24.268+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:03:24.268+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:03:24.382+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:03:24.380+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:03:24.387+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:03:24.462+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.236 seconds
[2023-03-18T01:03:56.027+0000] {processor.py:153} INFO - Started process (PID=1574) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:03:56.041+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:03:56.044+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:03:56.044+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:03:56.485+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:03:56.482+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:03:56.494+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:03:56.610+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.590 seconds
[2023-03-18T01:04:27.421+0000] {processor.py:153} INFO - Started process (PID=1648) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:04:27.425+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:04:27.427+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:04:27.426+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:04:27.492+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:04:27.488+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:04:27.494+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:04:27.567+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.171 seconds
[2023-03-18T01:04:58.138+0000] {processor.py:153} INFO - Started process (PID=1705) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:04:58.149+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:04:58.152+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:04:58.151+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:04:58.211+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:04:58.209+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:04:58.214+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:04:58.281+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.151 seconds
[2023-03-18T01:05:28.506+0000] {processor.py:153} INFO - Started process (PID=1779) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:05:28.519+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:05:28.522+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:05:28.521+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:05:28.597+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:05:28.595+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:05:28.599+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:05:28.713+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.222 seconds
[2023-03-18T01:05:59.287+0000] {processor.py:153} INFO - Started process (PID=1836) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:05:59.290+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:05:59.300+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:05:59.299+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:05:59.420+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:05:59.418+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:05:59.422+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:05:59.501+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.221 seconds
[2023-03-18T01:06:29.701+0000] {processor.py:153} INFO - Started process (PID=1907) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:06:29.704+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:06:29.706+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:06:29.706+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:06:29.809+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:06:29.807+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:06:29.811+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:06:29.941+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.256 seconds
[2023-03-18T01:07:00.144+0000] {processor.py:153} INFO - Started process (PID=1963) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:07:00.146+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:07:00.158+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:07:00.157+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:07:00.247+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:07:00.245+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:07:00.251+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:07:00.315+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.179 seconds
[2023-03-18T01:07:31.068+0000] {processor.py:153} INFO - Started process (PID=2037) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:07:31.118+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:07:31.134+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:07:31.134+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:07:31.401+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:07:31.398+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:07:31.418+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:07:31.553+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.491 seconds
[2023-03-18T01:08:01.723+0000] {processor.py:153} INFO - Started process (PID=2093) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:08:01.725+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:08:01.727+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:08:01.727+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:08:02.120+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:08:02.118+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:08:02.125+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:08:02.278+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.575 seconds
[2023-03-18T01:08:32.708+0000] {processor.py:153} INFO - Started process (PID=2165) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:08:32.713+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:08:32.716+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:08:32.715+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:08:32.832+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:08:32.830+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:08:32.834+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:08:32.932+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.244 seconds
[2023-03-18T01:09:03.458+0000] {processor.py:153} INFO - Started process (PID=2228) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:09:03.464+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:09:03.474+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:09:03.473+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:09:03.658+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:09:03.656+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:09:03.665+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:09:03.727+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.287 seconds
[2023-03-18T01:09:33.910+0000] {processor.py:153} INFO - Started process (PID=2293) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:09:33.918+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:09:33.921+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:09:33.920+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:09:34.119+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:09:34.115+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:09:34.124+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:09:34.188+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.291 seconds
[2023-03-18T01:10:04.866+0000] {processor.py:153} INFO - Started process (PID=2359) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:10:04.868+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:10:04.873+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:10:04.872+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:10:05.005+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:10:05.002+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:10:05.008+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:10:05.094+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.245 seconds
[2023-03-18T01:10:35.346+0000] {processor.py:153} INFO - Started process (PID=2422) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:10:35.354+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:10:35.360+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:10:35.360+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:10:35.439+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:10:35.437+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:10:35.441+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:10:35.506+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.168 seconds
[2023-03-18T01:11:05.964+0000] {processor.py:153} INFO - Started process (PID=2486) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:11:05.974+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:11:05.976+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:11:05.976+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:11:06.028+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:11:06.026+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:11:06.030+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:11:06.118+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.163 seconds
[2023-03-18T01:11:36.580+0000] {processor.py:153} INFO - Started process (PID=2549) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:11:36.592+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:11:36.595+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:11:36.594+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:11:36.690+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:11:36.688+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:11:36.692+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:11:36.788+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.215 seconds
[2023-03-18T01:12:07.146+0000] {processor.py:153} INFO - Started process (PID=2622) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:12:07.149+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:12:07.153+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:12:07.153+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:12:07.220+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:12:07.216+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:12:07.222+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:12:07.328+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.198 seconds
[2023-03-18T01:12:38.211+0000] {processor.py:153} INFO - Started process (PID=2678) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:12:38.221+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:12:38.225+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:12:38.224+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:12:38.411+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:12:38.409+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:12:38.414+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:12:38.491+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.291 seconds
[2023-03-18T01:13:09.130+0000] {processor.py:153} INFO - Started process (PID=2752) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:13:09.136+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:13:09.167+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:13:09.166+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:13:09.416+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:13:09.410+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:13:09.419+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:13:09.489+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.377 seconds
[2023-03-18T01:13:39.942+0000] {processor.py:153} INFO - Started process (PID=2809) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:13:39.945+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:13:39.949+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:13:39.948+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:13:40.025+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:13:40.021+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:13:40.027+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:13:40.094+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.163 seconds
[2023-03-18T01:14:10.650+0000] {processor.py:153} INFO - Started process (PID=2882) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:14:10.654+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:14:10.657+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:14:10.656+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:14:10.860+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:14:10.857+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:14:10.877+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:14:10.964+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.333 seconds
[2023-03-18T01:14:41.742+0000] {processor.py:153} INFO - Started process (PID=2949) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:14:41.747+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:14:41.753+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:14:41.753+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:14:41.810+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:14:41.808+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:14:41.812+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:14:41.880+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.146 seconds
[2023-03-18T01:15:12.100+0000] {processor.py:153} INFO - Started process (PID=3013) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:15:12.103+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:15:12.115+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:15:12.114+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:15:12.200+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:15:12.198+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:15:12.202+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:15:12.291+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.199 seconds
[2023-03-18T01:15:42.774+0000] {processor.py:153} INFO - Started process (PID=3079) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:15:42.785+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:15:42.789+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:15:42.789+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:15:42.915+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:15:42.912+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:15:42.929+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:15:42.998+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.231 seconds
[2023-03-18T01:16:13.421+0000] {processor.py:153} INFO - Started process (PID=3143) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:16:13.423+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:16:13.426+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:16:13.425+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:16:13.493+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:16:13.491+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:16:13.495+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:16:13.570+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.165 seconds
[2023-03-18T01:16:43.872+0000] {processor.py:153} INFO - Started process (PID=3210) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:16:43.881+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:16:43.883+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:16:43.883+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:16:44.092+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:16:44.090+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:16:44.101+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:16:44.188+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.335 seconds
[2023-03-18T01:17:14.459+0000] {processor.py:153} INFO - Started process (PID=3273) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:17:14.461+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:17:14.464+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:17:14.463+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:17:14.533+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:17:14.531+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:17:14.535+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:17:14.682+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.241 seconds
[2023-03-18T01:17:45.617+0000] {processor.py:153} INFO - Started process (PID=3339) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:17:45.620+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:17:45.632+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:17:45.632+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:17:45.749+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:17:45.747+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:17:45.751+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:17:45.858+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.250 seconds
[2023-03-18T01:18:16.189+0000] {processor.py:153} INFO - Started process (PID=3403) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:18:16.199+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:18:16.217+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:18:16.216+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:18:16.295+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:18:16.291+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:18:16.311+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:18:16.400+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.219 seconds
[2023-03-18T01:18:46.925+0000] {processor.py:153} INFO - Started process (PID=3476) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:18:46.934+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:18:46.937+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:18:46.936+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:18:46.997+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:18:46.995+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:18:46.999+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:18:47.078+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.160 seconds
[2023-03-18T01:19:17.569+0000] {processor.py:153} INFO - Started process (PID=3534) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:19:17.573+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:19:17.576+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:19:17.575+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:19:17.642+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:19:17.640+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:19:17.644+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:19:17.708+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.143 seconds
[2023-03-18T01:19:50.807+0000] {processor.py:153} INFO - Started process (PID=3607) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:19:51.022+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:19:51.059+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:19:51.058+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:19:51.594+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:19:51.591+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:19:51.610+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:19:51.735+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.933 seconds
[2023-03-18T01:20:22.209+0000] {processor.py:153} INFO - Started process (PID=3673) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:20:22.220+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:20:22.223+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:20:22.222+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:20:22.406+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:20:22.403+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:20:22.408+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:20:22.487+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.285 seconds
[2023-03-18T01:20:53.344+0000] {processor.py:153} INFO - Started process (PID=3737) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:20:53.347+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:20:53.349+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:20:53.349+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:20:53.437+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:20:53.435+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:20:53.440+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:20:53.508+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.176 seconds
[2023-03-18T01:21:24.361+0000] {processor.py:153} INFO - Started process (PID=3803) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:21:24.375+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:21:24.385+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:21:24.384+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:21:24.587+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:21:24.585+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:21:24.589+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:21:24.707+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.366 seconds
[2023-03-18T01:21:55.150+0000] {processor.py:153} INFO - Started process (PID=3867) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:21:55.152+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:21:55.162+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:21:55.161+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:21:55.230+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:21:55.227+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:21:55.232+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:21:55.297+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.155 seconds
[2023-03-18T01:22:25.953+0000] {processor.py:153} INFO - Started process (PID=3940) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:22:25.956+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:22:25.957+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:22:25.957+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:22:26.027+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:22:26.025+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:22:26.030+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:22:26.104+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.168 seconds
[2023-03-18T01:22:56.295+0000] {processor.py:153} INFO - Started process (PID=3997) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:22:56.303+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:22:56.305+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:22:56.305+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:22:56.376+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:22:56.374+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:22:56.378+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:22:56.416+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.127 seconds
[2023-03-18T01:23:27.125+0000] {processor.py:153} INFO - Started process (PID=4070) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:23:27.128+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:23:27.140+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:23:27.140+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:23:27.205+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:23:27.203+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:23:27.207+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:23:27.279+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.162 seconds
[2023-03-18T01:23:58.178+0000] {processor.py:153} INFO - Started process (PID=4135) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:23:58.189+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:23:58.191+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:23:58.191+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:23:58.257+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:23:58.255+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:23:58.258+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:23:58.334+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.163 seconds
[2023-03-18T01:24:28.775+0000] {processor.py:153} INFO - Started process (PID=4201) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:24:28.777+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:24:28.781+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:24:28.781+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:24:28.903+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:24:28.901+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:24:28.906+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:24:28.961+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.192 seconds
[2023-03-18T01:24:59.630+0000] {processor.py:153} INFO - Started process (PID=4276) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:24:59.633+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:24:59.642+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:24:59.642+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:24:59.757+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:24:59.755+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:24:59.759+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:24:59.852+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.227 seconds
[2023-03-18T01:25:30.316+0000] {processor.py:153} INFO - Started process (PID=4333) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:25:30.318+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:25:30.324+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:25:30.324+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:25:30.420+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:25:30.418+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:25:30.422+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:25:30.472+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.163 seconds
[2023-03-18T01:26:00.925+0000] {processor.py:153} INFO - Started process (PID=4405) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:26:00.934+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:26:00.939+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:26:00.938+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:26:01.169+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:26:01.167+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:26:01.179+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:26:01.245+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.336 seconds
[2023-03-18T01:26:31.963+0000] {processor.py:153} INFO - Started process (PID=4462) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:26:31.967+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:26:31.969+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:26:31.969+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:26:32.042+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:26:32.040+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:26:32.044+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:26:32.112+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.166 seconds
[2023-03-18T01:27:03.435+0000] {processor.py:153} INFO - Started process (PID=4535) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:27:03.438+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:27:03.449+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:27:03.448+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:27:03.773+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:27:03.770+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:27:03.774+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:27:03.925+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.509 seconds
[2023-03-18T01:27:35.022+0000] {processor.py:153} INFO - Started process (PID=4598) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:27:35.063+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:27:35.069+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:27:35.069+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:27:35.234+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:27:35.231+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:27:35.239+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:27:35.325+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.309 seconds
[2023-03-18T01:28:05.696+0000] {processor.py:153} INFO - Started process (PID=4665) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:28:05.699+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:28:05.709+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:28:05.708+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:28:05.781+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:28:05.779+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:28:05.784+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:28:05.852+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.163 seconds
[2023-03-18T01:28:36.386+0000] {processor.py:153} INFO - Started process (PID=4738) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:28:36.389+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:28:36.393+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:28:36.392+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:28:36.552+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:28:36.549+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:28:36.554+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:28:36.652+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.278 seconds
[2023-03-18T01:29:06.847+0000] {processor.py:153} INFO - Started process (PID=4804) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:29:06.849+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:29:06.851+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:29:06.851+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:29:06.898+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:29:06.897+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:29:06.900+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:29:06.952+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.111 seconds
[2023-03-18T01:29:37.484+0000] {processor.py:153} INFO - Started process (PID=4868) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:29:37.487+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:29:37.489+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:29:37.489+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:29:37.540+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:29:37.539+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:29:37.542+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:29:37.594+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.117 seconds
[2023-03-18T01:30:08.185+0000] {processor.py:153} INFO - Started process (PID=4940) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:30:08.187+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:30:08.190+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:30:08.190+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:30:08.267+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:30:08.265+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:30:08.269+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:30:08.332+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.165 seconds
[2023-03-18T01:30:39.325+0000] {processor.py:153} INFO - Started process (PID=5013) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:30:39.331+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:30:39.333+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:30:39.332+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:30:39.507+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:30:39.504+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:30:39.510+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:30:39.601+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.293 seconds
[2023-03-18T01:31:10.023+0000] {processor.py:153} INFO - Started process (PID=5070) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:31:10.026+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:31:10.028+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:31:10.027+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:31:10.099+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:31:10.097+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:31:10.100+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:31:10.160+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.143 seconds
[2023-03-18T01:31:40.592+0000] {processor.py:153} INFO - Started process (PID=5143) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:31:40.599+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:31:40.602+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:31:40.601+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:31:40.701+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:31:40.699+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:31:40.704+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:31:40.761+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.189 seconds
[2023-03-18T01:32:11.172+0000] {processor.py:153} INFO - Started process (PID=5217) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:32:11.202+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:32:11.207+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:32:11.206+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:32:11.319+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:32:11.317+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:32:11.321+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:32:11.388+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.223 seconds
[2023-03-18T01:32:41.806+0000] {processor.py:153} INFO - Started process (PID=5283) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:32:41.808+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:32:41.820+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:32:41.819+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:32:41.936+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:32:41.933+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:32:41.955+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:32:42.061+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.269 seconds
[2023-03-18T01:33:12.556+0000] {processor.py:153} INFO - Started process (PID=5345) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:33:12.559+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:33:12.561+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:33:12.561+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:33:12.618+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:33:12.616+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:33:12.620+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:33:12.685+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.139 seconds
[2023-03-18T01:33:43.524+0000] {processor.py:153} INFO - Started process (PID=5418) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:33:43.526+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:33:43.527+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:33:43.527+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:33:43.572+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:33:43.570+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:33:43.574+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:33:43.623+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.105 seconds
[2023-03-18T01:34:14.048+0000] {processor.py:153} INFO - Started process (PID=5491) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:34:14.051+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:34:14.053+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:34:14.053+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:34:14.117+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:34:14.115+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:34:14.119+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:34:14.154+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.111 seconds
[2023-03-18T01:34:44.531+0000] {processor.py:153} INFO - Started process (PID=5564) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:34:44.534+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:34:44.536+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:34:44.535+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:34:44.576+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:34:44.575+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:34:44.578+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:34:44.617+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.093 seconds
[2023-03-18T01:35:14.954+0000] {processor.py:153} INFO - Started process (PID=5637) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:35:14.965+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:35:14.967+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:35:14.967+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:35:15.032+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:35:15.031+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:35:15.034+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:35:15.075+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.128 seconds
[2023-03-18T01:35:45.770+0000] {processor.py:153} INFO - Started process (PID=5712) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:35:45.773+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:35:45.775+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:35:45.775+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:35:45.876+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:35:45.874+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:35:45.877+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:35:45.935+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.179 seconds
[2023-03-18T01:36:16.922+0000] {processor.py:153} INFO - Started process (PID=5778) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:36:16.927+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:36:16.940+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:36:16.940+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:36:16.991+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:36:16.989+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:36:16.993+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:36:17.038+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.124 seconds
[2023-03-18T01:36:47.189+0000] {processor.py:153} INFO - Started process (PID=5852) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:36:47.191+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:36:47.193+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:36:47.192+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:36:47.245+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:36:47.243+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:36:47.246+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:36:47.302+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.119 seconds
[2023-03-18T01:37:17.642+0000] {processor.py:153} INFO - Started process (PID=5916) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:37:17.645+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:37:17.646+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:37:17.646+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:37:17.701+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:37:17.699+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:37:17.703+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:37:17.743+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.105 seconds
[2023-03-18T01:37:48.006+0000] {processor.py:153} INFO - Started process (PID=5989) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:37:48.014+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:37:48.016+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:37:48.016+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:37:48.072+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:37:48.070+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:37:48.073+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:37:48.103+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.105 seconds
[2023-03-18T01:38:19.998+0000] {processor.py:153} INFO - Started process (PID=6060) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:38:20.003+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:38:20.009+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:38:20.008+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:38:20.086+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:38:20.083+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:38:20.089+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:38:20.225+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.237 seconds
[2023-03-18T01:38:50.696+0000] {processor.py:153} INFO - Started process (PID=6130) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:38:50.706+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:38:50.710+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:38:50.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:38:50.867+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:38:50.864+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:38:50.869+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:38:51.000+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.322 seconds
[2023-03-18T01:39:21.414+0000] {processor.py:153} INFO - Started process (PID=6186) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:39:21.418+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:39:21.420+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:39:21.420+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:39:21.492+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:39:21.488+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:39:21.494+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:39:21.547+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.153 seconds
[2023-03-18T01:39:51.645+0000] {processor.py:153} INFO - Started process (PID=6259) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:39:51.651+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:39:51.653+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:39:51.653+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:39:51.704+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:39:51.702+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:39:51.706+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:39:51.743+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.110 seconds
[2023-03-18T01:40:22.453+0000] {processor.py:153} INFO - Started process (PID=6325) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:40:22.457+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:40:22.460+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:40:22.460+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:40:22.529+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:40:22.527+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:40:22.530+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:40:22.615+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.174 seconds
[2023-03-18T01:40:53.161+0000] {processor.py:153} INFO - Started process (PID=6389) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:40:53.164+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:40:53.166+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:40:53.166+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:40:53.229+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:40:53.227+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:40:53.235+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:40:53.366+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.215 seconds
[2023-03-18T01:41:23.448+0000] {processor.py:153} INFO - Started process (PID=6460) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:41:23.450+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:41:23.455+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:41:23.455+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:41:23.519+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:41:23.516+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:41:23.521+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:41:23.608+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.171 seconds
[2023-03-18T01:41:53.776+0000] {processor.py:153} INFO - Started process (PID=6526) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:41:53.779+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:41:53.782+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:41:53.782+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:41:53.836+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:41:53.833+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:41:53.838+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:41:53.883+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.115 seconds
[2023-03-18T01:42:24.592+0000] {processor.py:153} INFO - Started process (PID=6591) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:42:24.599+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:42:24.606+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:42:24.605+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:42:24.712+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:42:24.708+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:42:24.715+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:42:24.804+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.224 seconds
[2023-03-18T01:42:55.268+0000] {processor.py:153} INFO - Started process (PID=6664) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:42:55.273+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:42:55.278+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:42:55.276+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:42:55.406+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:42:55.403+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:42:55.409+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:42:55.485+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.231 seconds
[2023-03-18T01:43:26.007+0000] {processor.py:153} INFO - Started process (PID=6719) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:43:26.011+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:43:26.023+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:43:26.014+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:43:26.085+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:43:26.082+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:43:26.087+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:43:26.182+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.187 seconds
[2023-03-18T01:43:56.655+0000] {processor.py:153} INFO - Started process (PID=6792) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:43:56.659+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:43:56.663+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:43:56.662+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:43:56.727+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:43:56.723+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:43:56.729+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:43:56.854+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.213 seconds
[2023-03-18T01:44:27.645+0000] {processor.py:153} INFO - Started process (PID=6849) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:44:27.649+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:44:27.668+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:44:27.667+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:44:27.793+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:44:27.788+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:44:27.795+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:44:27.912+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.286 seconds
[2023-03-18T01:44:59.990+0000] {processor.py:153} INFO - Started process (PID=6922) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:44:59.997+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:45:00.001+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:45:00.000+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:45:00.231+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:45:00.227+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:45:00.249+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:45:00.405+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.436 seconds
[2023-03-18T01:45:31.188+0000] {processor.py:153} INFO - Started process (PID=6988) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:45:31.198+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:45:31.209+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:45:31.209+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:45:31.366+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:45:31.344+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:45:31.377+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:45:31.506+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.329 seconds
[2023-03-18T01:46:01.908+0000] {processor.py:153} INFO - Started process (PID=7052) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:46:01.909+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:46:01.911+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:46:01.911+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:46:01.942+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:46:01.940+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:46:01.943+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:46:01.987+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-18T01:46:32.149+0000] {processor.py:153} INFO - Started process (PID=7125) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:46:32.150+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:46:32.151+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:46:32.151+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:46:32.172+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:46:32.171+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:46:32.173+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:46:32.196+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.050 seconds
[2023-03-18T01:47:02.533+0000] {processor.py:153} INFO - Started process (PID=7194) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:47:02.535+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:47:02.536+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:47:02.536+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:47:02.571+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:47:02.569+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:47:02.573+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:47:02.605+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.075 seconds
[2023-03-18T01:47:33.333+0000] {processor.py:153} INFO - Started process (PID=7283) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:47:33.335+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:47:33.337+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:47:33.337+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:47:33.375+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:47:33.374+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:47:33.377+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:47:33.411+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-18T01:48:03.714+0000] {processor.py:153} INFO - Started process (PID=7356) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:48:03.716+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:48:03.717+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:48:03.717+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:48:03.747+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:48:03.745+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:48:03.748+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:48:03.777+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-18T01:48:34.191+0000] {processor.py:153} INFO - Started process (PID=7439) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:48:34.192+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:48:34.194+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:48:34.194+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:48:34.228+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:48:34.226+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:48:34.229+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:48:34.254+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.067 seconds
[2023-03-18T01:49:04.952+0000] {processor.py:153} INFO - Started process (PID=7519) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:49:04.954+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:49:04.955+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:49:04.955+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:49:04.989+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:49:04.987+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:49:04.990+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:49:05.035+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-18T01:49:35.527+0000] {processor.py:153} INFO - Started process (PID=7591) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:49:35.529+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:49:35.530+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:49:35.530+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:49:35.566+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:49:35.565+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:49:35.567+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:49:35.597+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.074 seconds
[2023-03-18T01:50:05.798+0000] {processor.py:153} INFO - Started process (PID=7664) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:50:05.800+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:50:05.801+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:50:05.801+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:50:05.841+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:50:05.840+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:50:05.842+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:50:05.867+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-18T01:50:36.506+0000] {processor.py:153} INFO - Started process (PID=7746) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:50:36.508+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:50:36.510+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:50:36.509+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:50:36.555+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:50:36.554+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:50:36.557+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:50:36.594+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-18T01:51:06.865+0000] {processor.py:153} INFO - Started process (PID=7826) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:51:06.866+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:51:06.868+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:51:06.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:51:06.921+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:51:06.919+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:51:06.922+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:51:06.947+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.086 seconds
[2023-03-18T01:51:37.134+0000] {processor.py:153} INFO - Started process (PID=7900) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:51:37.136+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:51:37.137+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:51:37.137+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:51:37.177+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:51:37.174+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:51:37.178+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:51:37.220+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-18T01:52:08.021+0000] {processor.py:153} INFO - Started process (PID=7972) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:52:08.022+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:52:08.023+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:52:08.023+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:52:08.057+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:52:08.055+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:52:08.058+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:52:08.094+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.079 seconds
[2023-03-18T01:52:38.450+0000] {processor.py:153} INFO - Started process (PID=8045) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:52:38.453+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:52:38.456+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:52:38.456+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:52:38.495+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:52:38.493+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:52:38.497+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:52:38.528+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-18T01:53:08.665+0000] {processor.py:153} INFO - Started process (PID=8118) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:53:08.667+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:53:08.668+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:53:08.668+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:53:08.708+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:53:08.707+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:53:08.709+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:53:08.740+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.079 seconds
[2023-03-18T01:53:39.121+0000] {processor.py:153} INFO - Started process (PID=8208) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:53:39.124+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:53:39.125+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:53:39.125+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:53:39.161+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:53:39.160+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:53:39.163+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:53:39.191+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-18T01:54:10.201+0000] {processor.py:153} INFO - Started process (PID=8281) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:54:10.202+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:54:10.203+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:54:10.203+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:54:10.232+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:54:10.230+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:54:10.233+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:54:10.283+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.087 seconds
[2023-03-18T01:54:41.060+0000] {processor.py:153} INFO - Started process (PID=8363) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:54:41.062+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:54:41.063+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:54:41.063+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:54:41.099+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:54:41.098+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:54:41.100+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:54:41.130+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-18T01:55:11.402+0000] {processor.py:153} INFO - Started process (PID=8443) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:55:11.404+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:55:11.406+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:55:11.406+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:55:11.453+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:55:11.452+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:55:11.454+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:55:11.486+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-18T01:55:41.546+0000] {processor.py:153} INFO - Started process (PID=8516) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:55:41.548+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:55:41.550+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:55:41.550+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:55:41.588+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:55:41.586+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:55:41.589+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:55:41.620+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-18T01:56:12.322+0000] {processor.py:153} INFO - Started process (PID=8605) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:56:12.328+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:56:12.353+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:56:12.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:56:12.442+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:56:12.439+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:56:12.443+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:56:12.520+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.204 seconds
[2023-03-18T01:56:43.379+0000] {processor.py:153} INFO - Started process (PID=8679) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:56:43.381+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:56:43.384+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:56:43.384+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:56:43.430+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:56:43.429+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:56:43.431+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:56:43.460+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.086 seconds
[2023-03-18T01:57:13.860+0000] {processor.py:153} INFO - Started process (PID=8752) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:57:13.861+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:57:13.862+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:57:13.862+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:57:13.891+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:57:13.890+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:57:13.892+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:57:13.921+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-18T01:57:44.297+0000] {processor.py:153} INFO - Started process (PID=8841) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:57:44.299+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:57:44.301+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:57:44.301+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:57:44.370+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:57:44.369+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:57:44.371+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:57:44.397+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.104 seconds
[2023-03-18T01:58:15.049+0000] {processor.py:153} INFO - Started process (PID=8915) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:58:15.051+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:58:15.056+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:58:15.055+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:58:15.106+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:58:15.104+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:58:15.107+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:58:15.160+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.120 seconds
[2023-03-18T01:58:45.833+0000] {processor.py:153} INFO - Started process (PID=8988) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:58:45.836+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:58:45.840+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:58:45.839+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:58:45.890+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:58:45.887+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:58:45.891+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:58:45.933+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.106 seconds
[2023-03-18T01:59:16.035+0000] {processor.py:153} INFO - Started process (PID=9076) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:59:16.037+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:59:16.038+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:59:16.038+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:59:16.093+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:59:16.091+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:59:16.094+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:59:16.119+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-18T01:59:46.774+0000] {processor.py:153} INFO - Started process (PID=9149) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:59:46.776+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T01:59:46.777+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:59:46.777+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:59:46.816+0000] {logging_mixin.py:137} INFO - [2023-03-18T01:59:46.814+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T01:59:46.817+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T01:59:46.842+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-18T02:00:17.128+0000] {processor.py:153} INFO - Started process (PID=9222) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:00:17.129+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:00:17.130+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:00:17.130+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:00:17.159+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:00:17.158+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:00:17.160+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:00:17.183+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-18T02:00:47.972+0000] {processor.py:153} INFO - Started process (PID=9312) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:00:47.974+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:00:47.975+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:00:47.975+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:00:48.009+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:00:48.006+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:00:48.010+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:00:48.036+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.067 seconds
[2023-03-18T02:01:18.723+0000] {processor.py:153} INFO - Started process (PID=9385) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:01:18.725+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:01:18.726+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:01:18.726+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:01:18.765+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:01:18.763+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:01:18.766+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:01:18.795+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-18T02:01:49.001+0000] {processor.py:153} INFO - Started process (PID=9458) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:01:49.002+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:01:49.003+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:01:49.003+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:01:49.040+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:01:49.038+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:01:49.041+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:01:49.069+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.072 seconds
[2023-03-18T02:02:19.482+0000] {processor.py:153} INFO - Started process (PID=9540) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:02:19.483+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:02:19.484+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:02:19.484+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:02:19.533+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:02:19.532+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:02:19.535+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:02:19.561+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-18T02:02:49.696+0000] {processor.py:153} INFO - Started process (PID=9620) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:02:49.698+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:02:49.699+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:02:49.699+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:02:49.737+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:02:49.736+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:02:49.739+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:02:49.763+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-18T02:03:20.193+0000] {processor.py:153} INFO - Started process (PID=9693) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:03:20.198+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:03:20.201+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:03:20.201+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:03:20.273+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:03:20.271+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:03:20.275+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:03:20.383+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.197 seconds
[2023-03-18T02:03:50.489+0000] {processor.py:153} INFO - Started process (PID=9766) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:03:50.491+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:03:50.493+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:03:50.493+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:03:50.546+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:03:50.543+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:03:50.548+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:03:50.593+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.113 seconds
[2023-03-18T02:04:20.670+0000] {processor.py:153} INFO - Started process (PID=9838) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:04:20.672+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:04:20.673+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:04:20.673+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:04:20.713+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:04:20.711+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:04:20.714+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:04:20.757+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.091 seconds
[2023-03-18T02:04:50.860+0000] {processor.py:153} INFO - Started process (PID=9911) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:04:50.861+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:04:50.862+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:04:50.862+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:04:50.885+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:04:50.884+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:04:50.885+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:04:50.907+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.051 seconds
[2023-03-18T02:05:21.398+0000] {processor.py:153} INFO - Started process (PID=9984) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:05:21.402+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:05:21.405+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:05:21.405+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:05:21.453+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:05:21.451+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:05:21.455+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:05:21.498+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.109 seconds
[2023-03-18T02:05:52.229+0000] {processor.py:153} INFO - Started process (PID=10073) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:05:52.231+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:05:52.234+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:05:52.234+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:05:52.281+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:05:52.280+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:05:52.283+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:05:52.323+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.101 seconds
[2023-03-18T02:06:22.910+0000] {processor.py:153} INFO - Started process (PID=10146) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:06:22.912+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:06:22.916+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:06:22.915+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:06:22.959+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:06:22.958+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:06:22.961+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:06:22.998+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-18T02:06:53.497+0000] {processor.py:153} INFO - Started process (PID=10219) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:06:53.499+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:06:53.506+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:06:53.505+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:06:53.554+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:06:53.552+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:06:53.556+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:06:53.613+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.122 seconds
[2023-03-18T02:07:24.322+0000] {processor.py:153} INFO - Started process (PID=10292) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:07:24.323+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:07:24.324+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:07:24.324+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:07:24.364+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:07:24.363+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:07:24.365+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:07:24.397+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-18T02:07:55.259+0000] {processor.py:153} INFO - Started process (PID=10365) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:07:55.260+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:07:55.262+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:07:55.261+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:07:55.345+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:07:55.343+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:07:55.346+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:07:55.407+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.154 seconds
[2023-03-18T02:08:25.853+0000] {processor.py:153} INFO - Started process (PID=10453) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:08:25.856+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:08:25.858+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:08:25.858+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:08:25.919+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:08:25.917+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:08:25.921+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:08:25.977+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.130 seconds
[2023-03-18T02:08:56.375+0000] {processor.py:153} INFO - Started process (PID=10527) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:08:56.377+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:08:56.378+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:08:56.378+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:08:56.415+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:08:56.413+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:08:56.416+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:08:56.455+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-18T02:09:30.430+0000] {processor.py:153} INFO - Started process (PID=10600) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:09:30.450+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:09:30.505+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:09:30.504+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:09:31.125+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:09:31.123+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:09:31.126+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:09:31.235+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.812 seconds
[2023-03-18T02:10:02.490+0000] {processor.py:153} INFO - Started process (PID=10674) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:10:02.508+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:10:02.510+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:10:02.510+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:10:02.794+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:10:02.792+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:10:02.801+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:10:02.879+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.405 seconds
[2023-03-18T02:10:33.094+0000] {processor.py:153} INFO - Started process (PID=10730) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:10:33.098+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:10:33.101+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:10:33.100+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:10:33.171+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:10:33.168+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:10:33.182+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:10:33.267+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.180 seconds
[2023-03-18T02:11:03.448+0000] {processor.py:153} INFO - Started process (PID=10801) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:11:03.460+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:11:03.462+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:11:03.462+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:11:03.548+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:11:03.546+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:11:03.550+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:11:03.671+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.229 seconds
[2023-03-18T02:11:33.827+0000] {processor.py:153} INFO - Started process (PID=10872) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:11:33.829+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:11:33.831+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:11:33.831+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:11:33.883+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:11:33.882+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:11:33.889+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:11:34.015+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.203 seconds
[2023-03-18T02:12:04.117+0000] {processor.py:153} INFO - Started process (PID=10944) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:12:04.119+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:12:04.121+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:12:04.120+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:12:04.163+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:12:04.161+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:12:04.166+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:12:04.218+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.106 seconds
[2023-03-18T02:12:34.440+0000] {processor.py:153} INFO - Started process (PID=11017) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:12:34.442+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:12:34.443+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:12:34.443+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:12:34.507+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:12:34.505+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:12:34.510+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:12:34.546+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.112 seconds
[2023-03-18T02:13:04.865+0000] {processor.py:153} INFO - Started process (PID=11091) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:13:04.866+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:13:04.868+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:13:04.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:13:04.906+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:13:04.904+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:13:04.908+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:13:04.949+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-18T02:13:35.678+0000] {processor.py:153} INFO - Started process (PID=11171) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:13:35.682+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:13:35.689+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:13:35.689+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:13:35.787+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:13:35.785+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:13:35.788+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:13:35.865+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.197 seconds
[2023-03-18T02:14:06.353+0000] {processor.py:153} INFO - Started process (PID=11253) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:14:06.363+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:14:06.380+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:14:06.380+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:14:06.420+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:14:06.417+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:14:06.421+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:14:06.504+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.158 seconds
[2023-03-18T02:14:37.217+0000] {processor.py:153} INFO - Started process (PID=11327) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:14:37.218+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:14:37.223+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:14:37.222+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:14:37.258+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:14:37.256+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:14:37.259+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:14:37.296+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.087 seconds
[2023-03-18T02:15:07.718+0000] {processor.py:153} INFO - Started process (PID=11400) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:15:07.721+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:15:07.722+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:15:07.722+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:15:07.751+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:15:07.750+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:15:07.752+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:15:07.788+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.081 seconds
[2023-03-18T02:15:37.910+0000] {processor.py:153} INFO - Started process (PID=11473) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:15:37.911+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:15:37.913+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:15:37.913+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:15:37.954+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:15:37.952+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:15:37.955+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:15:37.987+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-18T02:16:08.626+0000] {processor.py:153} INFO - Started process (PID=11546) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:16:08.629+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:16:08.639+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:16:08.639+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:16:08.734+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:16:08.732+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:16:08.736+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:16:08.807+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.188 seconds
[2023-03-18T02:16:38.877+0000] {processor.py:153} INFO - Started process (PID=11619) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:16:38.878+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:16:38.879+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:16:38.879+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:16:38.922+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:16:38.919+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:16:38.923+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:16:38.950+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.079 seconds
[2023-03-18T02:17:09.496+0000] {processor.py:153} INFO - Started process (PID=11692) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:17:09.499+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:17:09.500+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:17:09.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:17:09.551+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:17:09.549+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:17:09.552+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:17:09.587+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.096 seconds
[2023-03-18T02:17:39.840+0000] {processor.py:153} INFO - Started process (PID=11765) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:17:39.842+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:17:39.843+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:17:39.843+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:17:39.882+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:17:39.881+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:17:39.884+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:17:39.915+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.081 seconds
[2023-03-18T02:18:10.406+0000] {processor.py:153} INFO - Started process (PID=11854) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:18:10.408+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:18:10.409+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:18:10.409+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:18:10.449+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:18:10.448+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:18:10.450+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:18:10.478+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-18T02:18:40.865+0000] {processor.py:153} INFO - Started process (PID=11928) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:18:40.866+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:18:40.867+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:18:40.867+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:18:40.900+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:18:40.899+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:18:40.901+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:18:40.928+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.068 seconds
[2023-03-18T02:19:11.339+0000] {processor.py:153} INFO - Started process (PID=12001) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:19:11.341+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:19:11.342+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:19:11.342+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:19:11.380+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:19:11.379+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:19:11.381+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:19:11.410+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.077 seconds
[2023-03-18T02:19:41.871+0000] {processor.py:153} INFO - Started process (PID=12083) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:19:41.872+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:19:41.873+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:19:41.873+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:19:41.904+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:19:41.903+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:19:41.905+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:19:41.931+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-18T02:20:12.138+0000] {processor.py:153} INFO - Started process (PID=12163) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:20:12.140+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:20:12.141+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:20:12.141+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:20:12.171+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:20:12.170+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:20:12.172+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:20:12.201+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-18T02:20:42.585+0000] {processor.py:153} INFO - Started process (PID=12236) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:20:42.588+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:20:42.589+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:20:42.589+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:20:42.623+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:20:42.621+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:20:42.625+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:20:42.652+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-18T02:21:13.065+0000] {processor.py:153} INFO - Started process (PID=12325) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:21:13.067+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:21:13.068+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:21:13.068+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:21:13.104+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:21:13.103+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:21:13.105+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:21:13.136+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.074 seconds
[2023-03-18T02:21:44.262+0000] {processor.py:153} INFO - Started process (PID=12382) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:21:44.263+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:21:44.265+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:21:44.265+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:21:44.319+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:21:44.317+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:21:44.320+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:21:44.383+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.136 seconds
[2023-03-18T02:22:15.076+0000] {processor.py:153} INFO - Started process (PID=12453) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:22:15.082+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:22:15.088+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:22:15.088+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:22:15.147+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:22:15.145+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:22:15.148+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:22:15.224+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.154 seconds
[2023-03-18T02:22:45.478+0000] {processor.py:153} INFO - Started process (PID=12526) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:22:45.485+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:22:45.489+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:22:45.489+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:22:45.541+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:22:45.539+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:22:45.544+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:22:45.614+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.143 seconds
[2023-03-18T02:23:16.439+0000] {processor.py:153} INFO - Started process (PID=12599) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:23:16.445+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:23:16.449+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:23:16.449+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:23:16.492+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:23:16.490+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:23:16.493+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:23:16.534+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.102 seconds
[2023-03-18T02:23:46.667+0000] {processor.py:153} INFO - Started process (PID=12672) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:23:46.668+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:23:46.669+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:23:46.669+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:23:46.711+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:23:46.709+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:23:46.712+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:23:46.747+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-18T02:24:17.406+0000] {processor.py:153} INFO - Started process (PID=12754) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:24:17.408+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:24:17.409+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:24:17.409+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:24:17.454+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:24:17.452+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:24:17.455+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:24:17.493+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-18T02:24:48.364+0000] {processor.py:153} INFO - Started process (PID=12834) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:24:48.369+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:24:48.371+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:24:48.370+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:24:48.454+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:24:48.452+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:24:48.456+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:24:48.534+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.175 seconds
[2023-03-18T02:25:19.578+0000] {processor.py:153} INFO - Started process (PID=12907) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:25:19.589+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:25:19.591+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:25:19.590+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:25:19.643+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:25:19.641+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:25:19.644+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:25:19.709+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.135 seconds
[2023-03-18T02:25:49.882+0000] {processor.py:153} INFO - Started process (PID=12996) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:25:49.885+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:25:49.886+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:25:49.886+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:25:49.924+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:25:49.922+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:25:49.925+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:25:49.958+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-18T02:26:20.749+0000] {processor.py:153} INFO - Started process (PID=13070) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:26:20.750+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:26:20.751+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:26:20.751+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:26:20.791+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:26:20.790+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:26:20.792+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:26:20.815+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.072 seconds
[2023-03-18T02:26:50.993+0000] {processor.py:153} INFO - Started process (PID=13143) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:26:50.994+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:26:50.995+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:26:50.995+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:26:51.044+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:26:51.042+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:26:51.045+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:26:51.081+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.091 seconds
[2023-03-18T02:27:21.283+0000] {processor.py:153} INFO - Started process (PID=13216) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:27:21.284+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:27:21.286+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:27:21.285+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:27:21.340+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:27:21.339+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:27:21.341+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:27:21.368+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-18T02:27:51.519+0000] {processor.py:153} INFO - Started process (PID=13290) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:27:51.521+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:27:51.522+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:27:51.522+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:27:51.597+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:27:51.596+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:27:51.598+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:27:51.635+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.119 seconds
[2023-03-18T02:28:21.946+0000] {processor.py:153} INFO - Started process (PID=13379) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:28:21.948+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:28:21.949+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:28:21.949+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:28:21.984+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:28:21.982+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:28:21.985+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:28:22.026+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-18T02:28:52.397+0000] {processor.py:153} INFO - Started process (PID=13452) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:28:52.398+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:28:52.399+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:28:52.399+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:28:52.445+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:28:52.443+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:28:52.446+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:28:52.477+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.086 seconds
[2023-03-18T02:29:23.152+0000] {processor.py:153} INFO - Started process (PID=13534) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:29:23.157+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:29:23.158+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:29:23.158+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:29:23.202+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:29:23.199+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:29:23.204+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:29:23.230+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.081 seconds
[2023-03-18T02:29:53.391+0000] {processor.py:153} INFO - Started process (PID=13614) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:29:53.393+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T02:29:53.394+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:29:53.394+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:29:53.426+0000] {logging_mixin.py:137} INFO - [2023-03-18T02:29:53.424+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T02:29:53.427+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T02:29:53.457+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-18T05:49:01.602+0000] {processor.py:153} INFO - Started process (PID=13626) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T05:49:01.642+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T05:49:01.659+0000] {logging_mixin.py:137} INFO - [2023-03-18T05:49:01.658+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T05:49:01.706+0000] {logging_mixin.py:137} INFO - [2023-03-18T05:49:01.704+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T05:49:01.707+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T05:49:01.927+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.352 seconds
[2023-03-18T20:19:54.648+0000] {processor.py:153} INFO - Started process (PID=13707) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T20:19:54.997+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T20:19:55.108+0000] {logging_mixin.py:137} INFO - [2023-03-18T20:19:55.107+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T20:19:59.228+0000] {logging_mixin.py:137} INFO - [2023-03-18T20:19:59.118+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T20:19:59.308+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T20:20:03.108+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 9.389 seconds
[2023-03-18T20:36:19.256+0000] {processor.py:153} INFO - Started process (PID=14134) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T20:36:19.806+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T20:36:20.096+0000] {logging_mixin.py:137} INFO - [2023-03-18T20:36:20.096+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T20:36:26.366+0000] {logging_mixin.py:137} INFO - [2023-03-18T20:36:23.076+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T20:36:26.486+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:50:02.911+0000] {processor.py:153} INFO - Started process (PID=75) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:50:02.915+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:50:03.017+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:50:03.015+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:50:03.536+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:50:03.530+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:50:03.537+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:50:03.608+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.702 seconds
[2023-03-18T23:50:34.585+0000] {processor.py:153} INFO - Started process (PID=139) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:50:34.633+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:50:34.655+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:50:34.655+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:50:35.445+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:50:35.443+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:50:35.471+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:50:35.636+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 1.057 seconds
[2023-03-18T23:51:07.960+0000] {processor.py:153} INFO - Started process (PID=204) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:51:07.999+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:51:08.050+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:51:08.049+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:51:09.447+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:51:09.445+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:51:09.465+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:51:09.561+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 1.607 seconds
[2023-03-18T23:51:56.182+0000] {processor.py:153} INFO - Started process (PID=276) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:51:56.188+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:51:56.216+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:51:56.216+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:51:56.547+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:51:56.545+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:51:56.549+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:51:56.691+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.518 seconds
[2023-03-18T23:52:27.799+0000] {processor.py:153} INFO - Started process (PID=349) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:52:27.942+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:52:27.963+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:52:27.962+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:52:28.159+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:52:28.151+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:52:28.201+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:52:28.293+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.512 seconds
[2023-03-18T23:53:07.765+0000] {processor.py:153} INFO - Started process (PID=422) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:53:07.788+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:53:07.809+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:53:07.809+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:53:08.673+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:53:08.668+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:53:08.675+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:53:08.905+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 1.146 seconds
[2023-03-18T23:53:39.729+0000] {processor.py:153} INFO - Started process (PID=488) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:53:39.757+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:53:39.773+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:53:39.772+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:53:40.406+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:53:40.403+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:53:40.408+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:53:40.538+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.829 seconds
[2023-03-18T23:54:10.943+0000] {processor.py:153} INFO - Started process (PID=553) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:54:10.945+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:54:10.947+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:54:10.947+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:54:11.166+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:54:11.163+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:54:11.167+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:54:11.229+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.293 seconds
[2023-03-18T23:54:44.043+0000] {processor.py:153} INFO - Started process (PID=620) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:54:44.076+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:54:44.078+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:54:44.078+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:54:44.411+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:54:44.409+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:54:44.413+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:54:44.524+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.497 seconds
[2023-03-18T23:55:14.874+0000] {processor.py:153} INFO - Started process (PID=678) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:55:14.884+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:55:14.885+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:55:14.885+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:55:14.949+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:55:14.947+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:55:14.950+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:55:15.029+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.161 seconds
[2023-03-18T23:55:45.218+0000] {processor.py:153} INFO - Started process (PID=748) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:55:46.951+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:55:46.955+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:55:46.954+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:55:47.225+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:55:47.205+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:55:47.226+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:55:47.313+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 2.103 seconds
[2023-03-18T23:56:18.125+0000] {processor.py:153} INFO - Started process (PID=805) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:56:18.128+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:56:18.130+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:56:18.129+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:56:18.178+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:56:18.176+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:56:18.180+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:56:18.211+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-18T23:56:49.028+0000] {processor.py:153} INFO - Started process (PID=878) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:56:49.030+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:56:49.040+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:56:49.040+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:56:49.493+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:56:49.483+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:56:49.500+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:56:49.592+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.570 seconds
[2023-03-18T23:57:20.062+0000] {processor.py:153} INFO - Started process (PID=935) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:57:20.081+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:57:20.085+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:57:20.085+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:57:20.388+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:57:20.386+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:57:20.399+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:57:20.512+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.457 seconds
[2023-03-18T23:57:51.046+0000] {processor.py:153} INFO - Started process (PID=1007) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:57:51.051+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:57:51.053+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:57:51.052+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:57:51.351+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:57:51.349+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:57:51.352+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:57:51.415+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.386 seconds
[2023-03-18T23:58:22.317+0000] {processor.py:153} INFO - Started process (PID=1073) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:58:22.366+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:58:22.406+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:58:22.405+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:58:22.769+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:58:22.767+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:58:22.770+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:58:22.862+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.553 seconds
[2023-03-18T23:58:53.095+0000] {processor.py:153} INFO - Started process (PID=1137) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:58:53.097+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:58:53.098+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:58:53.098+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:58:53.129+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:58:53.128+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:58:53.129+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:58:53.160+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-18T23:59:23.985+0000] {processor.py:153} INFO - Started process (PID=1212) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:59:23.989+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:59:23.991+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:59:23.990+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:59:24.066+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:59:24.064+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:59:24.067+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:59:24.130+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.160 seconds
[2023-03-18T23:59:54.443+0000] {processor.py:153} INFO - Started process (PID=1285) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:59:54.445+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-18T23:59:54.446+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:59:54.446+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:59:54.615+0000] {logging_mixin.py:137} INFO - [2023-03-18T23:59:54.612+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-18T23:59:54.617+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-18T23:59:54.703+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.274 seconds
