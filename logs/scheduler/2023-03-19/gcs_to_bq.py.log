[2023-03-19T00:00:24.915+0000] {processor.py:153} INFO - Started process (PID=1352) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:00:24.931+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:00:24.933+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:00:24.932+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:00:25.019+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:00:25.016+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:00:25.020+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:00:25.105+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.197 seconds
[2023-03-19T00:00:55.671+0000] {processor.py:153} INFO - Started process (PID=1417) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:00:55.673+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:00:55.675+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:00:55.675+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:00:55.728+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:00:55.726+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:00:55.730+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:00:55.802+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.146 seconds
[2023-03-19T00:01:26.018+0000] {processor.py:153} INFO - Started process (PID=1490) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:01:26.021+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:01:26.023+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:01:26.023+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:01:26.097+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:01:26.096+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:01:26.099+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:01:26.176+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.175 seconds
[2023-03-19T00:01:56.313+0000] {processor.py:153} INFO - Started process (PID=1545) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:01:56.324+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:01:56.326+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:01:56.325+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:01:56.393+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:01:56.391+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:01:56.395+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:01:56.471+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.164 seconds
[2023-03-19T00:02:26.641+0000] {processor.py:153} INFO - Started process (PID=1617) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:02:26.643+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:02:26.645+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:02:26.644+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:02:26.701+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:02:26.699+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:02:26.703+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:02:26.763+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.131 seconds
[2023-03-19T00:02:57.811+0000] {processor.py:153} INFO - Started process (PID=1682) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:02:57.815+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:02:57.826+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:02:57.818+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:02:57.890+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:02:57.888+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:02:57.892+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:02:58.046+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.252 seconds
[2023-03-19T00:03:28.260+0000] {processor.py:153} INFO - Started process (PID=1748) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:03:28.270+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:03:28.272+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:03:28.272+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:03:28.321+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:03:28.319+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:03:28.323+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:03:28.368+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.114 seconds
[2023-03-19T00:03:58.828+0000] {processor.py:153} INFO - Started process (PID=1814) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:03:58.830+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:03:58.832+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:03:58.832+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:03:58.922+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:03:58.917+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:03:58.929+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:03:59.102+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.290 seconds
[2023-03-19T00:04:29.825+0000] {processor.py:153} INFO - Started process (PID=1878) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:04:29.828+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:04:29.847+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:04:29.846+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:04:29.924+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:04:29.922+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:04:29.926+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:04:29.986+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.169 seconds
[2023-03-19T00:05:00.324+0000] {processor.py:153} INFO - Started process (PID=1952) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:05:00.328+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:05:00.331+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:05:00.330+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:05:00.469+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:05:00.467+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:05:00.471+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:05:00.553+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.235 seconds
[2023-03-19T00:05:30.836+0000] {processor.py:153} INFO - Started process (PID=2009) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:05:30.849+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:05:30.851+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:05:30.850+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:05:30.907+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:05:30.904+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:05:30.909+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:05:31.030+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.212 seconds
[2023-03-19T00:06:01.267+0000] {processor.py:153} INFO - Started process (PID=2081) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:06:01.289+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:06:01.290+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:06:01.290+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:06:01.343+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:06:01.341+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:06:01.349+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:06:01.433+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.172 seconds
[2023-03-19T00:06:31.851+0000] {processor.py:153} INFO - Started process (PID=2147) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:06:31.854+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:06:31.856+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:06:31.856+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:06:31.990+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:06:31.988+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:06:31.992+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:06:32.061+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.231 seconds
[2023-03-19T00:07:02.457+0000] {processor.py:153} INFO - Started process (PID=2211) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:07:02.459+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:07:02.461+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:07:02.460+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:07:02.526+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:07:02.524+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:07:02.527+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:07:02.615+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.164 seconds
[2023-03-19T00:07:33.242+0000] {processor.py:153} INFO - Started process (PID=2284) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:07:33.252+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:07:33.255+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:07:33.255+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:07:33.302+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:07:33.300+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:07:33.303+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:07:33.363+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.131 seconds
[2023-03-19T00:08:03.462+0000] {processor.py:153} INFO - Started process (PID=2342) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:08:03.465+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:08:03.466+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:08:03.466+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:08:03.558+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:08:03.556+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:08:03.560+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:08:03.627+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.171 seconds
[2023-03-19T00:08:34.015+0000] {processor.py:153} INFO - Started process (PID=2412) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:08:34.017+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:08:34.018+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:08:34.018+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:08:34.148+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:08:34.146+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:08:34.168+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:08:34.243+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.234 seconds
[2023-03-19T00:09:04.696+0000] {processor.py:153} INFO - Started process (PID=2478) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:09:04.708+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:09:04.720+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:09:04.720+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:09:04.978+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:09:04.976+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:09:04.990+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:09:05.092+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.409 seconds
[2023-03-19T00:09:35.278+0000] {processor.py:153} INFO - Started process (PID=2543) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:09:35.280+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:09:35.282+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:09:35.282+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:09:35.475+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:09:35.473+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:09:35.477+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:09:35.556+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.284 seconds
[2023-03-19T00:10:06.312+0000] {processor.py:153} INFO - Started process (PID=2616) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:10:06.314+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:10:06.316+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:10:06.316+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:10:06.387+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:10:06.384+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:10:06.389+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:10:06.454+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.149 seconds
[2023-03-19T00:10:36.675+0000] {processor.py:153} INFO - Started process (PID=2682) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:10:36.678+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:10:36.680+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:10:36.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:10:36.774+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:10:36.771+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:10:36.775+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:10:36.860+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.202 seconds
[2023-03-19T00:11:07.369+0000] {processor.py:153} INFO - Started process (PID=2747) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:11:07.380+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:11:07.382+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:11:07.381+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:11:07.439+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:11:07.437+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:11:07.440+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:11:07.513+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.151 seconds
[2023-03-19T00:11:38.343+0000] {processor.py:153} INFO - Started process (PID=2821) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:11:38.345+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:11:38.355+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:11:38.355+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:11:38.404+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:11:38.402+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:11:38.405+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:11:38.491+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.156 seconds
[2023-03-19T00:12:08.814+0000] {processor.py:153} INFO - Started process (PID=2878) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:12:08.816+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:12:08.819+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:12:08.818+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:12:08.895+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:12:08.893+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:12:08.897+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:12:08.965+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.171 seconds
[2023-03-19T00:12:39.204+0000] {processor.py:153} INFO - Started process (PID=2951) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:12:39.208+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:12:39.214+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:12:39.214+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:12:39.271+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:12:39.269+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:12:39.277+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:12:39.352+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.157 seconds
[2023-03-19T00:13:09.560+0000] {processor.py:153} INFO - Started process (PID=3017) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:13:09.572+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:13:09.575+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:13:09.575+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:13:09.646+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:13:09.644+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:13:09.648+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:13:09.726+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.172 seconds
[2023-03-19T00:13:40.343+0000] {processor.py:153} INFO - Started process (PID=3082) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:13:40.345+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:13:40.358+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:13:40.357+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:13:40.567+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:13:40.565+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:13:40.569+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:13:40.646+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.309 seconds
[2023-03-19T00:14:11.323+0000] {processor.py:153} INFO - Started process (PID=3155) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:14:11.325+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:14:11.326+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:14:11.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:14:11.439+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:14:11.436+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:14:11.441+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:14:11.507+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.191 seconds
[2023-03-19T00:14:42.075+0000] {processor.py:153} INFO - Started process (PID=3213) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:14:42.107+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:14:42.156+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:14:42.155+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:14:42.442+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:14:42.437+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:14:42.445+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:14:42.496+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.438 seconds
[2023-03-19T00:15:12.683+0000] {processor.py:153} INFO - Started process (PID=3286) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:15:12.685+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:15:12.687+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:15:12.687+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:15:12.755+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:15:12.754+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:15:12.758+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:15:12.819+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.142 seconds
[2023-03-19T00:15:43.043+0000] {processor.py:153} INFO - Started process (PID=3352) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:15:43.047+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:15:43.050+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:15:43.049+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:15:43.119+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:15:43.117+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:15:43.122+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:15:43.173+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.137 seconds
[2023-03-19T00:16:13.356+0000] {processor.py:153} INFO - Started process (PID=3415) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:16:13.381+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:16:13.385+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:16:13.384+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:16:13.512+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:16:13.509+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:16:13.513+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:16:13.588+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.242 seconds
[2023-03-19T00:16:44.375+0000] {processor.py:153} INFO - Started process (PID=3482) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:16:44.388+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:16:44.390+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:16:44.390+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:16:44.447+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:16:44.445+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:16:44.448+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:16:44.533+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.165 seconds
[2023-03-19T00:17:14.799+0000] {processor.py:153} INFO - Started process (PID=3544) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:17:14.802+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:17:14.809+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:17:14.808+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:17:14.950+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:17:14.948+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:17:14.952+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:17:15.048+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.267 seconds
[2023-03-19T00:17:45.465+0000] {processor.py:153} INFO - Started process (PID=3606) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:17:45.476+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:17:45.499+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:17:45.499+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:17:45.614+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:17:45.612+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:17:45.616+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:17:45.688+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.231 seconds
[2023-03-19T00:18:15.845+0000] {processor.py:153} INFO - Started process (PID=3668) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:18:15.847+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:18:15.849+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:18:15.849+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:18:16.015+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:18:16.013+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:18:16.031+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:18:16.111+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.283 seconds
[2023-03-19T00:18:46.688+0000] {processor.py:153} INFO - Started process (PID=3734) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:18:46.694+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:18:46.697+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:18:46.696+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:18:46.887+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:18:46.885+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:18:46.889+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:18:46.960+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.287 seconds
[2023-03-19T00:19:17.371+0000] {processor.py:153} INFO - Started process (PID=3801) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:19:17.387+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:19:17.390+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:19:17.389+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:19:17.535+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:19:17.533+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:19:17.537+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:19:17.601+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.235 seconds
[2023-03-19T00:19:47.995+0000] {processor.py:153} INFO - Started process (PID=3864) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:19:47.997+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:19:47.998+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:19:47.998+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:19:48.072+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:19:48.069+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:19:48.075+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:19:48.149+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.160 seconds
[2023-03-19T00:20:18.881+0000] {processor.py:153} INFO - Started process (PID=3937) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:20:18.892+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:20:18.900+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:20:18.899+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:20:19.104+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:20:19.101+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:20:19.112+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:20:19.181+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.305 seconds
[2023-03-19T00:20:49.286+0000] {processor.py:153} INFO - Started process (PID=3993) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:20:49.288+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:20:49.290+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:20:49.290+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:20:49.366+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:20:49.364+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:20:49.375+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:20:49.459+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.191 seconds
[2023-03-19T00:21:19.750+0000] {processor.py:153} INFO - Started process (PID=4064) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:21:19.764+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:21:19.766+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:21:19.766+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:21:19.856+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:21:19.852+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:21:19.858+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:21:19.913+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.175 seconds
[2023-03-19T00:21:50.714+0000] {processor.py:153} INFO - Started process (PID=4131) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:21:50.716+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:21:50.718+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:21:50.717+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:21:50.795+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:21:50.792+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:21:50.796+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:21:50.858+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.164 seconds
[2023-03-19T00:22:21.441+0000] {processor.py:153} INFO - Started process (PID=4195) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:22:21.444+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:22:21.445+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:22:21.445+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:22:21.511+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:22:21.510+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:22:21.513+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:22:21.603+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.170 seconds
[2023-03-19T00:22:51.717+0000] {processor.py:153} INFO - Started process (PID=4268) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:22:51.719+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:22:51.721+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:22:51.720+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:22:51.810+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:22:51.808+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:22:51.811+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:22:51.910+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.197 seconds
[2023-03-19T00:23:22.052+0000] {processor.py:153} INFO - Started process (PID=4325) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:23:22.065+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:23:22.074+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:23:22.074+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:23:22.138+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:23:22.135+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:23:22.139+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:23:22.239+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.200 seconds
[2023-03-19T00:23:52.630+0000] {processor.py:153} INFO - Started process (PID=4398) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:23:52.632+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:23:52.634+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:23:52.634+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:23:52.685+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:23:52.682+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:23:52.688+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:23:52.751+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.138 seconds
[2023-03-19T00:24:23.133+0000] {processor.py:153} INFO - Started process (PID=4464) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:24:23.135+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:24:23.137+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:24:23.137+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:24:23.189+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:24:23.182+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:24:23.193+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:24:23.253+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.129 seconds
[2023-03-19T00:24:53.419+0000] {processor.py:153} INFO - Started process (PID=4528) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:24:53.428+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:24:53.435+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:24:53.434+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:24:53.523+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:24:53.521+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:24:53.524+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:24:53.589+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.178 seconds
[2023-03-19T00:25:24.134+0000] {processor.py:153} INFO - Started process (PID=4601) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:25:24.136+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:25:24.140+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:25:24.140+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:25:24.200+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:25:24.198+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:25:24.201+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:25:24.292+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.166 seconds
[2023-03-19T00:25:54.832+0000] {processor.py:153} INFO - Started process (PID=4667) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:25:54.833+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:25:54.835+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:25:54.835+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:25:54.911+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:25:54.908+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:25:54.912+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:25:54.967+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.142 seconds
[2023-03-19T00:26:25.080+0000] {processor.py:153} INFO - Started process (PID=4732) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:26:25.083+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:26:25.085+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:26:25.084+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:26:25.157+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:26:25.155+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:26:25.158+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:26:25.239+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.165 seconds
[2023-03-19T00:26:55.494+0000] {processor.py:153} INFO - Started process (PID=4805) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:26:55.497+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:26:55.498+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:26:55.498+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:26:55.563+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:26:55.561+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:26:55.564+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:26:55.688+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.206 seconds
[2023-03-19T00:27:26.019+0000] {processor.py:153} INFO - Started process (PID=4869) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:27:26.021+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:27:26.023+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:27:26.023+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:27:26.085+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:27:26.082+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:27:26.086+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:27:26.168+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.157 seconds
[2023-03-19T00:27:56.258+0000] {processor.py:153} INFO - Started process (PID=4935) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:27:56.260+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:27:56.261+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:27:56.261+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:27:56.309+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:27:56.307+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:27:56.310+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:27:56.353+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.103 seconds
[2023-03-19T00:28:26.864+0000] {processor.py:153} INFO - Started process (PID=5008) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:28:26.866+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:28:26.868+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:28:26.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:28:26.925+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:28:26.921+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:28:26.926+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:28:27.018+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.170 seconds
[2023-03-19T00:28:57.671+0000] {processor.py:153} INFO - Started process (PID=5065) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:28:57.673+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:28:57.675+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:28:57.674+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:28:57.725+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:28:57.723+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:28:57.727+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:28:57.795+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.138 seconds
[2023-03-19T00:29:28.598+0000] {processor.py:153} INFO - Started process (PID=5139) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:29:28.600+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:29:28.603+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:29:28.603+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:29:28.697+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:29:28.695+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:29:28.699+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:29:28.760+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.167 seconds
[2023-03-19T00:29:58.950+0000] {processor.py:153} INFO - Started process (PID=5195) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:29:58.955+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:29:58.964+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:29:58.963+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:29:59.135+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:29:59.134+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:29:59.138+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:29:59.218+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.283 seconds
[2023-03-19T00:30:29.556+0000] {processor.py:153} INFO - Started process (PID=5269) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:30:29.558+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:30:29.560+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:30:29.560+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:30:29.621+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:30:29.619+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:30:29.622+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:30:29.679+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.142 seconds
[2023-03-19T00:31:00.267+0000] {processor.py:153} INFO - Started process (PID=5342) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:31:00.268+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:31:00.270+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:31:00.269+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:31:00.316+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:31:00.315+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:31:00.318+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:31:00.432+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.172 seconds
[2023-03-19T00:31:30.685+0000] {processor.py:153} INFO - Started process (PID=5409) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:31:30.687+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:31:30.689+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:31:30.688+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:31:30.731+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:31:30.729+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:31:30.733+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:31:30.783+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.103 seconds
[2023-03-19T00:32:01.234+0000] {processor.py:153} INFO - Started process (PID=5473) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:32:01.236+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:32:01.245+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:32:01.243+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:32:01.282+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:32:01.280+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:32:01.283+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:32:01.333+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.104 seconds
[2023-03-19T00:32:31.584+0000] {processor.py:153} INFO - Started process (PID=5546) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:32:31.586+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:32:31.589+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:32:31.587+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:32:31.632+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:32:31.630+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:32:31.633+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:32:31.676+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.097 seconds
[2023-03-19T00:33:01.996+0000] {processor.py:153} INFO - Started process (PID=5619) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:33:01.998+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:33:01.999+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:33:01.999+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:33:02.057+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:33:02.055+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:33:02.059+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:33:02.112+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.121 seconds
[2023-03-19T00:33:32.485+0000] {processor.py:153} INFO - Started process (PID=5692) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:33:32.486+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:33:32.488+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:33:32.487+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:33:32.522+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:33:32.521+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:33:32.523+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:33:32.590+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.110 seconds
[2023-03-19T00:34:02.914+0000] {processor.py:153} INFO - Started process (PID=5765) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:34:02.915+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:34:02.917+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:34:02.916+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:34:02.955+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:34:02.954+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:34:02.957+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:34:03.007+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.099 seconds
[2023-03-19T00:34:33.330+0000] {processor.py:153} INFO - Started process (PID=5838) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:34:33.332+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:34:33.335+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:34:33.335+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:34:33.375+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:34:33.372+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:34:33.376+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:34:33.413+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-19T00:35:03.584+0000] {processor.py:153} INFO - Started process (PID=5911) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:35:03.586+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:35:03.592+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:35:03.590+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:35:03.679+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:35:03.677+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:35:03.681+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:35:03.725+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.146 seconds
[2023-03-19T00:35:34.387+0000] {processor.py:153} INFO - Started process (PID=5977) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:35:34.391+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:35:34.401+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:35:34.400+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:35:34.491+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:35:34.489+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:35:34.492+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:35:34.546+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.164 seconds
[2023-03-19T00:36:05.255+0000] {processor.py:153} INFO - Started process (PID=6041) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:36:05.258+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:36:05.260+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:36:05.260+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:36:05.330+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:36:05.328+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:36:05.331+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:36:05.388+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.142 seconds
[2023-03-19T00:36:35.701+0000] {processor.py:153} INFO - Started process (PID=6114) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:36:35.702+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:36:35.704+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:36:35.703+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:36:35.744+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:36:35.742+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:36:35.746+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:36:35.802+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.106 seconds
[2023-03-19T00:37:06.641+0000] {processor.py:153} INFO - Started process (PID=6188) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:37:06.643+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:37:06.644+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:37:06.644+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:37:06.688+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:37:06.686+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:37:06.689+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:37:06.751+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.113 seconds
[2023-03-19T00:37:37.359+0000] {processor.py:153} INFO - Started process (PID=6262) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:37:37.361+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:37:37.363+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:37:37.363+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:37:37.439+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:37:37.437+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:37:37.445+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:37:37.543+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.190 seconds
[2023-03-19T00:38:07.802+0000] {processor.py:153} INFO - Started process (PID=6335) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:38:07.804+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:38:07.809+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:38:07.809+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:38:07.892+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:38:07.890+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:38:07.894+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:38:07.958+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.162 seconds
[2023-03-19T00:38:38.147+0000] {processor.py:153} INFO - Started process (PID=6402) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:38:38.149+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:38:38.153+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:38:38.153+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:38:38.200+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:38:38.198+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:38:38.202+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:38:38.257+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.115 seconds
[2023-03-19T00:39:08.966+0000] {processor.py:153} INFO - Started process (PID=6476) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:39:08.968+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:39:08.971+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:39:08.971+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:39:09.030+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:39:09.029+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:39:09.032+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:39:09.099+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.142 seconds
[2023-03-19T00:39:39.607+0000] {processor.py:153} INFO - Started process (PID=6550) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:39:39.609+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:39:39.612+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:39:39.611+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:39:39.683+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:39:39.681+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:39:39.685+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:39:39.749+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.147 seconds
[2023-03-19T00:40:10.125+0000] {processor.py:153} INFO - Started process (PID=6622) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:40:10.127+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:40:10.129+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:40:10.129+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:40:10.180+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:40:10.178+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:40:10.181+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:40:10.238+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.119 seconds
[2023-03-19T00:40:40.684+0000] {processor.py:153} INFO - Started process (PID=6688) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:40:40.686+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:40:40.695+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:40:40.693+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:40:40.744+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:40:40.742+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:40:40.745+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:40:40.828+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.149 seconds
[2023-03-19T00:41:11.001+0000] {processor.py:153} INFO - Started process (PID=6760) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:41:11.002+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:41:11.004+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:41:11.003+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:41:11.052+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:41:11.050+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:41:11.053+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:41:11.094+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.098 seconds
[2023-03-19T00:41:41.612+0000] {processor.py:153} INFO - Started process (PID=6834) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:41:41.613+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:41:41.615+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:41:41.614+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:41:41.656+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:41:41.655+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:41:41.658+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:41:41.705+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.099 seconds
[2023-03-19T00:42:11.977+0000] {processor.py:153} INFO - Started process (PID=6907) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:42:11.979+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:42:11.990+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:42:11.989+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:42:12.040+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:42:12.038+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:42:12.041+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:42:12.116+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.144 seconds
[2023-03-19T00:42:42.892+0000] {processor.py:153} INFO - Started process (PID=6973) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:42:42.893+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:42:42.895+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:42:42.895+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:42:42.949+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:42:42.948+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:42:42.951+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:42:43.013+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.129 seconds
[2023-03-19T00:43:13.574+0000] {processor.py:153} INFO - Started process (PID=7037) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:43:13.576+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:43:13.577+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:43:13.577+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:43:13.609+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:43:13.607+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:43:13.610+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:43:13.695+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.130 seconds
[2023-03-19T00:43:44.324+0000] {processor.py:153} INFO - Started process (PID=7110) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:43:44.325+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:43:44.326+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:43:44.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:43:44.389+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:43:44.387+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:43:44.393+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:43:44.466+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.148 seconds
[2023-03-19T00:44:14.822+0000] {processor.py:153} INFO - Started process (PID=7183) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:44:14.824+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:44:14.826+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:44:14.826+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:44:14.873+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:44:14.871+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:44:14.874+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:44:14.922+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.115 seconds
[2023-03-19T00:44:45.124+0000] {processor.py:153} INFO - Started process (PID=7257) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:44:45.125+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:44:45.135+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:44:45.132+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:44:45.243+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:44:45.242+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:44:45.248+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:44:45.320+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.202 seconds
[2023-03-19T00:45:15.768+0000] {processor.py:153} INFO - Started process (PID=7323) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:45:15.770+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:45:15.771+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:45:15.771+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:45:15.811+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:45:15.809+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:45:15.812+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:45:15.903+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.153 seconds
[2023-03-19T00:45:46.322+0000] {processor.py:153} INFO - Started process (PID=7387) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:45:46.324+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:45:46.326+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:45:46.325+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:45:46.375+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:45:46.373+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:45:46.376+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:45:46.459+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.143 seconds
[2023-03-19T00:46:16.885+0000] {processor.py:153} INFO - Started process (PID=7460) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:46:16.891+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:46:16.918+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:46:16.917+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:46:16.968+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:46:16.966+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:46:16.971+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:46:17.033+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.155 seconds
[2023-03-19T00:46:47.886+0000] {processor.py:153} INFO - Started process (PID=7534) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:46:47.887+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:46:47.891+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:46:47.890+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:46:47.945+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:46:47.942+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:46:47.946+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:46:47.985+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.108 seconds
[2023-03-19T00:47:18.401+0000] {processor.py:153} INFO - Started process (PID=7607) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:47:18.412+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:47:18.413+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:47:18.413+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:47:18.454+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:47:18.452+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:47:18.456+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:47:18.510+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.115 seconds
[2023-03-19T00:47:48.644+0000] {processor.py:153} INFO - Started process (PID=7664) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:47:48.647+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:47:48.648+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:47:48.648+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:47:48.703+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:47:48.701+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:47:48.705+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:47:48.758+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.120 seconds
[2023-03-19T00:48:19.729+0000] {processor.py:153} INFO - Started process (PID=7737) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:48:19.730+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:48:19.731+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:48:19.731+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:48:19.758+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:48:19.757+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:48:19.759+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:48:19.783+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.058 seconds
[2023-03-19T00:48:50.020+0000] {processor.py:153} INFO - Started process (PID=7810) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:48:50.021+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:48:50.021+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:48:50.021+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:48:50.055+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:48:50.054+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:48:50.056+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:48:50.075+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T00:49:20.896+0000] {processor.py:153} INFO - Started process (PID=7883) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:49:20.898+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:49:20.898+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:49:20.898+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:49:20.964+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:49:20.963+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:49:20.965+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:49:20.988+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-19T00:49:51.066+0000] {processor.py:153} INFO - Started process (PID=7956) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:49:51.067+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:49:51.068+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:49:51.068+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:49:51.095+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:49:51.094+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:49:51.096+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:49:51.119+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T00:50:21.278+0000] {processor.py:153} INFO - Started process (PID=8029) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:50:21.281+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:50:21.284+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:50:21.282+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:50:21.347+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:50:21.345+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:50:21.349+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:50:21.487+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.211 seconds
[2023-03-19T00:50:51.956+0000] {processor.py:153} INFO - Started process (PID=8111) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:50:51.958+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:50:51.960+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:50:51.960+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:50:52.010+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:50:52.008+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:50:52.011+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:50:52.069+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.128 seconds
[2023-03-19T00:51:22.530+0000] {processor.py:153} INFO - Started process (PID=8184) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:51:22.532+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:51:22.534+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:51:22.534+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:51:22.572+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:51:22.570+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:51:22.573+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:51:22.614+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.091 seconds
[2023-03-19T00:51:52.690+0000] {processor.py:153} INFO - Started process (PID=8252) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:51:52.692+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:51:52.693+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:51:52.693+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:51:52.727+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:51:52.725+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:51:52.728+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:51:52.772+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-19T00:52:23.182+0000] {processor.py:153} INFO - Started process (PID=8323) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:52:23.197+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:52:23.201+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:52:23.201+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:52:23.258+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:52:23.256+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:52:23.261+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:52:23.362+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.193 seconds
[2023-03-19T00:52:53.443+0000] {processor.py:153} INFO - Started process (PID=8396) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:52:53.444+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:52:53.445+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:52:53.445+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:52:53.479+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:52:53.478+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:52:53.480+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:52:53.510+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.072 seconds
[2023-03-19T00:53:24.135+0000] {processor.py:153} INFO - Started process (PID=8476) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:53:24.136+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:53:24.138+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:53:24.137+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:53:24.172+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:53:24.171+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:53:24.173+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:53:24.199+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.067 seconds
[2023-03-19T00:53:54.813+0000] {processor.py:153} INFO - Started process (PID=8549) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:53:54.814+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:53:54.815+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:53:54.815+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:53:54.852+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:53:54.851+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:53:54.853+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:53:54.883+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.074 seconds
[2023-03-19T00:54:25.440+0000] {processor.py:153} INFO - Started process (PID=8638) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:54:25.441+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:54:25.442+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:54:25.442+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:54:25.466+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:54:25.465+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:54:25.466+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:54:25.487+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.049 seconds
[2023-03-19T00:54:56.456+0000] {processor.py:153} INFO - Started process (PID=8712) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:54:56.457+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:54:56.458+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:54:56.458+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:54:56.502+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:54:56.500+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:54:56.503+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:54:56.529+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.077 seconds
[2023-03-19T00:55:26.767+0000] {processor.py:153} INFO - Started process (PID=8801) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:55:26.768+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:55:26.770+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:55:26.769+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:55:26.796+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:55:26.794+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:55:26.796+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:55:26.833+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-19T00:55:57.302+0000] {processor.py:153} INFO - Started process (PID=8874) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:55:57.304+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:55:57.305+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:55:57.305+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:55:57.353+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:55:57.351+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:55:57.355+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:55:57.388+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-19T00:56:27.559+0000] {processor.py:153} INFO - Started process (PID=8963) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:56:27.560+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:56:27.561+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:56:27.561+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:56:27.599+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:56:27.598+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:56:27.600+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:56:27.622+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-19T00:56:57.677+0000] {processor.py:153} INFO - Started process (PID=9037) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:56:57.678+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:56:57.679+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:56:57.679+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:56:57.704+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:56:57.703+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:56:57.704+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:56:57.722+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.048 seconds
[2023-03-19T00:57:27.968+0000] {processor.py:153} INFO - Started process (PID=9127) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:57:27.969+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:57:27.970+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:57:27.970+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:57:27.995+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:57:27.994+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:57:27.996+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:57:28.018+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.053 seconds
[2023-03-19T00:57:58.396+0000] {processor.py:153} INFO - Started process (PID=9200) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:57:58.397+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:57:58.398+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:57:58.398+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:57:58.434+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:57:58.433+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:57:58.435+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:57:58.470+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-19T00:58:28.497+0000] {processor.py:153} INFO - Started process (PID=9273) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:58:28.498+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:58:28.499+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:58:28.498+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:58:28.519+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:58:28.518+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:58:28.520+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:58:28.539+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.047 seconds
[2023-03-19T00:58:58.947+0000] {processor.py:153} INFO - Started process (PID=9361) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:58:58.948+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:58:58.949+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:58:58.949+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:58:58.972+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:58:58.971+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:58:58.973+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:58:58.994+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.052 seconds
[2023-03-19T00:59:29.976+0000] {processor.py:153} INFO - Started process (PID=9434) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:59:29.977+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T00:59:29.978+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:59:29.978+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:59:30.018+0000] {logging_mixin.py:137} INFO - [2023-03-19T00:59:30.016+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T00:59:30.018+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T00:59:30.038+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-19T01:00:00.737+0000] {processor.py:153} INFO - Started process (PID=9524) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:00:00.739+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:00:00.739+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:00:00.739+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:00:00.770+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:00:00.768+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:00:00.771+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:00:00.796+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-19T01:00:31.433+0000] {processor.py:153} INFO - Started process (PID=9597) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:00:31.434+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:00:31.435+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:00:31.435+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:00:31.457+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:00:31.456+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:00:31.457+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:00:31.484+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.054 seconds
[2023-03-19T01:01:01.842+0000] {processor.py:153} INFO - Started process (PID=9670) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:01:01.843+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:01:01.845+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:01:01.845+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:01:01.889+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:01:01.887+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:01:01.891+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:01:01.928+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-19T01:01:32.084+0000] {processor.py:153} INFO - Started process (PID=9759) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:01:32.085+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:01:32.086+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:01:32.086+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:01:32.122+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:01:32.121+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:01:32.123+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:01:32.151+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-19T01:02:02.550+0000] {processor.py:153} INFO - Started process (PID=9832) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:02:02.551+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:02:02.552+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:02:02.551+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:02:02.572+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:02:02.571+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:02:02.573+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:02:02.597+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.050 seconds
[2023-03-19T01:02:32.937+0000] {processor.py:153} INFO - Started process (PID=9921) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:02:32.938+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:02:32.939+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:02:32.939+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:02:32.978+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:02:32.976+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:02:32.979+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:02:33.014+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.083 seconds
[2023-03-19T01:03:03.657+0000] {processor.py:153} INFO - Started process (PID=9994) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:03:03.659+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:03:03.660+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:03:03.660+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:03:03.681+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:03:03.680+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:03:03.682+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:03:03.702+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.048 seconds
[2023-03-19T01:03:34.116+0000] {processor.py:153} INFO - Started process (PID=10067) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:03:34.118+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:03:34.119+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:03:34.119+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:03:34.146+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:03:34.145+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:03:34.147+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:03:34.175+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-19T01:04:04.316+0000] {processor.py:153} INFO - Started process (PID=10156) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:04:04.318+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:04:04.319+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:04:04.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:04:04.354+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:04:04.353+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:04:04.355+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:04:04.391+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-19T01:04:34.575+0000] {processor.py:153} INFO - Started process (PID=10229) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:04:34.576+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:04:34.577+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:04:34.577+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:04:34.611+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:04:34.609+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:04:34.612+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:04:34.652+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-19T01:05:04.834+0000] {processor.py:153} INFO - Started process (PID=10310) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:05:04.836+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:05:04.837+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:05:04.837+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:05:04.867+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:05:04.866+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:05:04.868+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:05:04.906+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-19T01:05:34.970+0000] {processor.py:153} INFO - Started process (PID=10392) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:05:34.971+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:05:34.972+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:05:34.972+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:05:35.009+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:05:35.007+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:05:35.010+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:05:35.032+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-19T01:06:05.431+0000] {processor.py:153} INFO - Started process (PID=10465) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:06:05.433+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:06:05.434+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:06:05.434+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:06:05.478+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:06:05.477+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:06:05.480+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:06:05.501+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-19T01:06:35.613+0000] {processor.py:153} INFO - Started process (PID=10545) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:06:35.614+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:06:35.615+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:06:35.615+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:06:35.638+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:06:35.637+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:06:35.639+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:06:35.662+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.054 seconds
[2023-03-19T01:07:06.025+0000] {processor.py:153} INFO - Started process (PID=10623) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:07:06.027+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:07:06.029+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:07:06.028+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:07:06.069+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:07:06.067+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:07:06.070+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:07:06.118+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.100 seconds
[2023-03-19T01:07:36.727+0000] {processor.py:153} INFO - Started process (PID=10697) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:07:36.728+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:07:36.729+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:07:36.729+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:07:36.764+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:07:36.761+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:07:36.765+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:07:36.801+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-19T01:08:06.914+0000] {processor.py:153} INFO - Started process (PID=10786) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:08:06.915+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:08:06.917+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:08:06.916+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:08:07.001+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:08:07.000+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:08:07.002+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:08:07.024+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.115 seconds
[2023-03-19T01:08:37.145+0000] {processor.py:153} INFO - Started process (PID=10859) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:08:37.146+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:08:37.148+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:08:37.147+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:08:37.165+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:08:37.164+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:08:37.165+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:08:37.184+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.042 seconds
[2023-03-19T01:09:07.511+0000] {processor.py:153} INFO - Started process (PID=10948) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:09:07.512+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:09:07.514+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:09:07.514+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:09:07.559+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:09:07.557+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:09:07.559+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:09:07.588+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.083 seconds
[2023-03-19T01:09:37.771+0000] {processor.py:153} INFO - Started process (PID=11021) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:09:37.772+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:09:37.774+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:09:37.773+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:09:37.807+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:09:37.806+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:09:37.808+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:09:37.837+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-19T01:10:08.166+0000] {processor.py:153} INFO - Started process (PID=11103) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:10:08.168+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:10:08.169+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:10:08.168+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:10:08.188+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:10:08.187+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:10:08.189+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:10:08.211+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.050 seconds
[2023-03-19T01:10:39.085+0000] {processor.py:153} INFO - Started process (PID=11183) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:10:39.087+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:10:39.088+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:10:39.088+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:10:39.117+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:10:39.115+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:10:39.118+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:10:39.199+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.119 seconds
[2023-03-19T01:11:09.992+0000] {processor.py:153} INFO - Started process (PID=11265) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:11:09.993+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:11:09.995+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:11:09.994+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:11:10.027+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:11:10.026+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:11:10.028+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:11:10.051+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.064 seconds
[2023-03-19T01:11:40.508+0000] {processor.py:153} INFO - Started process (PID=11345) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:11:40.509+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:11:40.510+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:11:40.510+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:11:40.541+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:11:40.540+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:11:40.543+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:11:40.573+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-19T01:12:10.857+0000] {processor.py:153} INFO - Started process (PID=11418) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:12:10.860+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:12:10.861+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:12:10.861+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:12:10.887+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:12:10.886+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:12:10.888+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:12:10.919+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.067 seconds
[2023-03-19T01:12:41.422+0000] {processor.py:153} INFO - Started process (PID=11508) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:12:41.423+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:12:41.425+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:12:41.425+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:12:41.474+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:12:41.473+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:12:41.476+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:12:41.548+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.136 seconds
[2023-03-19T01:13:12.322+0000] {processor.py:153} INFO - Started process (PID=11581) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:13:12.323+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:13:12.324+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:13:12.324+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:13:12.364+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:13:12.362+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:13:12.365+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:13:12.388+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.068 seconds
[2023-03-19T01:13:42.542+0000] {processor.py:153} INFO - Started process (PID=11670) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:13:42.544+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:13:42.545+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:13:42.545+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:13:42.571+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:13:42.569+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:13:42.572+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:13:42.602+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.064 seconds
[2023-03-19T01:14:12.645+0000] {processor.py:153} INFO - Started process (PID=11742) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:14:12.646+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:14:12.647+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:14:12.647+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:14:12.671+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:14:12.670+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:14:12.672+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:14:12.702+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.061 seconds
[2023-03-19T01:14:42.759+0000] {processor.py:153} INFO - Started process (PID=11814) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:14:42.760+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:14:42.761+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:14:42.761+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:14:42.778+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:14:42.777+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:14:42.778+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:14:42.846+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-19T01:15:12.992+0000] {processor.py:153} INFO - Started process (PID=11901) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:15:12.993+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:15:12.994+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:15:12.994+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:15:13.016+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:15:13.015+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:15:13.017+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:15:13.037+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.048 seconds
[2023-03-19T01:15:43.310+0000] {processor.py:153} INFO - Started process (PID=11975) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:15:43.311+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:15:43.312+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:15:43.312+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:15:43.330+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:15:43.329+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:15:43.330+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:15:43.351+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.045 seconds
[2023-03-19T01:16:13.701+0000] {processor.py:153} INFO - Started process (PID=12064) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:16:13.702+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:16:13.703+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:16:13.703+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:16:13.735+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:16:13.734+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:16:13.735+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:16:13.753+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T01:16:44.031+0000] {processor.py:153} INFO - Started process (PID=12137) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:16:44.033+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:16:44.034+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:16:44.034+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:16:44.061+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:16:44.060+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:16:44.062+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:16:44.145+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.119 seconds
[2023-03-19T01:17:14.249+0000] {processor.py:153} INFO - Started process (PID=12226) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:17:14.250+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:17:14.251+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:17:14.251+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:17:14.297+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:17:14.296+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:17:14.298+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:17:14.319+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-19T01:17:44.856+0000] {processor.py:153} INFO - Started process (PID=12299) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:17:44.857+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:17:44.859+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:17:44.859+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:17:44.891+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:17:44.890+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:17:44.892+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:17:44.915+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-19T01:18:15.191+0000] {processor.py:153} INFO - Started process (PID=12381) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:18:15.192+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:18:15.194+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:18:15.193+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:18:15.219+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:18:15.218+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:18:15.220+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:18:15.251+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-19T01:18:45.809+0000] {processor.py:153} INFO - Started process (PID=12461) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:18:45.810+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:18:45.811+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:18:45.811+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:18:45.841+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:18:45.839+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:18:45.842+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:18:45.870+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-19T01:19:16.038+0000] {processor.py:153} INFO - Started process (PID=12534) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:19:16.039+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:19:16.040+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:19:16.040+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:19:16.066+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:19:16.064+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:19:16.067+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:19:16.090+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T01:19:46.628+0000] {processor.py:153} INFO - Started process (PID=12624) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:19:46.658+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:19:46.663+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:19:46.663+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:19:46.764+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:19:46.763+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:19:46.766+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:19:46.850+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.229 seconds
[2023-03-19T01:20:17.172+0000] {processor.py:153} INFO - Started process (PID=12680) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:20:17.175+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:20:17.177+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:20:17.177+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:20:17.215+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:20:17.211+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:20:17.219+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:20:17.291+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.132 seconds
[2023-03-19T01:20:47.586+0000] {processor.py:153} INFO - Started process (PID=12753) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:20:47.588+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:20:47.589+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:20:47.589+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:20:47.660+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:20:47.656+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:20:47.661+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:20:47.707+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.127 seconds
[2023-03-19T01:21:17.761+0000] {processor.py:153} INFO - Started process (PID=12827) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:21:17.763+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:21:17.764+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:21:17.764+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:21:17.797+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:21:17.796+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:21:17.798+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:21:17.827+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-19T01:21:48.209+0000] {processor.py:153} INFO - Started process (PID=12910) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:21:48.210+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:21:48.211+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:21:48.211+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:21:48.240+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:21:48.239+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:21:48.241+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:21:48.264+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T01:22:18.484+0000] {processor.py:153} INFO - Started process (PID=12991) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:22:18.486+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:22:18.486+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:22:18.486+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:22:18.516+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:22:18.515+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:22:18.517+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:22:18.537+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-19T01:22:48.625+0000] {processor.py:153} INFO - Started process (PID=13062) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:22:48.626+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:22:48.627+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:22:48.627+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:22:48.654+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:22:48.653+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:22:48.654+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:22:48.682+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.060 seconds
[2023-03-19T01:23:18.855+0000] {processor.py:153} INFO - Started process (PID=13148) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:23:18.857+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:23:18.858+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:23:18.858+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:23:18.884+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:23:18.883+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:23:18.885+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:23:18.918+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.067 seconds
[2023-03-19T01:23:49.265+0000] {processor.py:153} INFO - Started process (PID=13221) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:23:49.266+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:23:49.267+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:23:49.267+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:23:49.297+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:23:49.296+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:23:49.298+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:23:49.318+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T01:24:19.484+0000] {processor.py:153} INFO - Started process (PID=13310) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:24:19.485+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:24:19.486+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:24:19.486+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:24:19.515+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:24:19.514+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:24:19.516+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:24:19.543+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-19T01:24:50.005+0000] {processor.py:153} INFO - Started process (PID=13383) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:24:50.007+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:24:50.008+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:24:50.007+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:24:50.034+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:24:50.033+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:24:50.035+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:24:50.058+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T01:25:20.229+0000] {processor.py:153} INFO - Started process (PID=13465) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:25:20.230+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:25:20.232+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:25:20.232+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:25:20.262+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:25:20.261+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:25:20.263+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:25:20.289+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.064 seconds
[2023-03-19T01:25:51.074+0000] {processor.py:153} INFO - Started process (PID=13545) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:25:51.075+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:25:51.076+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:25:51.076+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:25:51.104+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:25:51.103+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:25:51.105+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:25:51.129+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.058 seconds
[2023-03-19T01:26:22.169+0000] {processor.py:153} INFO - Started process (PID=13618) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:26:22.170+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:26:22.171+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:26:22.171+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:26:22.197+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:26:22.195+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:26:22.197+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:26:22.222+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.057 seconds
[2023-03-19T01:26:52.462+0000] {processor.py:153} INFO - Started process (PID=13707) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:26:52.463+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:26:52.463+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:26:52.463+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:26:52.491+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:26:52.490+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:26:52.492+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:26:52.516+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.057 seconds
[2023-03-19T01:27:22.607+0000] {processor.py:153} INFO - Started process (PID=13780) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:27:22.608+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:27:22.609+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:27:22.609+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:27:22.632+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:27:22.631+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:27:22.633+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:27:22.655+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.052 seconds
[2023-03-19T01:27:53.331+0000] {processor.py:153} INFO - Started process (PID=13869) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:27:53.333+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:27:53.335+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:27:53.335+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:27:53.371+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:27:53.369+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:27:53.372+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:27:53.409+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.086 seconds
[2023-03-19T01:28:23.557+0000] {processor.py:153} INFO - Started process (PID=13942) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:28:23.559+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:28:23.560+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:28:23.560+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:28:23.592+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:28:23.591+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:28:23.593+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:28:23.618+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-19T01:28:54.141+0000] {processor.py:153} INFO - Started process (PID=14025) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:28:54.143+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:28:54.144+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:28:54.144+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:28:54.170+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:28:54.169+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:28:54.171+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:28:54.198+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.060 seconds
[2023-03-19T01:29:25.003+0000] {processor.py:153} INFO - Started process (PID=14105) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:29:25.004+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:29:25.005+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:29:25.004+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:29:25.023+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:29:25.022+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:29:25.023+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:29:25.047+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.048 seconds
[2023-03-19T01:29:55.240+0000] {processor.py:153} INFO - Started process (PID=14185) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:29:55.241+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:29:55.242+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:29:55.242+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:29:55.273+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:29:55.272+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:29:55.274+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:29:55.301+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.064 seconds
[2023-03-19T01:30:25.716+0000] {processor.py:153} INFO - Started process (PID=14267) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:30:25.717+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:30:25.718+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:30:25.718+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:30:25.748+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:30:25.746+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:30:25.749+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:30:25.775+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-19T01:30:56.162+0000] {processor.py:153} INFO - Started process (PID=14340) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:30:56.164+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:30:56.165+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:30:56.165+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:30:56.193+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:30:56.191+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:30:56.194+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:30:56.222+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.064 seconds
[2023-03-19T01:31:26.423+0000] {processor.py:153} INFO - Started process (PID=14430) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:31:26.425+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:31:26.425+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:31:26.425+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:31:26.459+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:31:26.457+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:31:26.461+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:31:26.489+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-19T01:31:56.837+0000] {processor.py:153} INFO - Started process (PID=14503) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:31:56.838+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:31:56.839+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:31:56.839+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:31:56.869+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:31:56.868+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:31:56.870+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:31:56.894+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.061 seconds
[2023-03-19T01:32:27.341+0000] {processor.py:153} INFO - Started process (PID=14592) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:32:27.343+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:32:27.344+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:32:27.344+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:32:27.383+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:32:27.381+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:32:27.384+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:32:27.418+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.084 seconds
[2023-03-19T01:32:57.605+0000] {processor.py:153} INFO - Started process (PID=14665) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:32:57.606+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:32:57.607+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:32:57.607+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:32:57.635+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:32:57.634+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:32:57.636+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:32:57.656+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T01:33:27.918+0000] {processor.py:153} INFO - Started process (PID=14738) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:33:27.919+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:33:27.920+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:33:27.920+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:33:27.942+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:33:27.941+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:33:27.943+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:33:27.961+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.047 seconds
[2023-03-19T01:33:58.206+0000] {processor.py:153} INFO - Started process (PID=14827) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:33:58.207+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:33:58.208+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:33:58.208+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:33:58.234+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:33:58.232+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:33:58.234+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:33:58.258+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-19T01:34:28.544+0000] {processor.py:153} INFO - Started process (PID=14900) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:34:28.545+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:34:28.546+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:34:28.546+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:34:28.572+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:34:28.570+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:34:28.573+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:34:28.596+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-19T01:34:59.135+0000] {processor.py:153} INFO - Started process (PID=14989) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:34:59.137+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:34:59.138+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:34:59.138+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:34:59.168+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:34:59.167+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:34:59.170+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:34:59.197+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-19T01:35:29.303+0000] {processor.py:153} INFO - Started process (PID=15062) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:35:29.304+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:35:29.305+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:35:29.305+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:35:29.327+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:35:29.326+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:35:29.328+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:35:29.359+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T01:35:59.855+0000] {processor.py:153} INFO - Started process (PID=15152) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:35:59.857+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:35:59.861+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:35:59.860+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:35:59.940+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:35:59.938+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:35:59.942+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:35:59.994+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.147 seconds
[2023-03-19T01:36:30.106+0000] {processor.py:153} INFO - Started process (PID=15225) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:36:30.108+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:36:30.109+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:36:30.109+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:36:30.152+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:36:30.151+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:36:30.153+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:36:30.183+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-19T01:37:01.125+0000] {processor.py:153} INFO - Started process (PID=15298) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:37:01.127+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:37:01.128+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:37:01.128+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:37:01.162+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:37:01.160+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:37:01.164+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:37:01.197+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-19T01:37:31.901+0000] {processor.py:153} INFO - Started process (PID=15387) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:37:31.903+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:37:31.905+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:37:31.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:37:31.938+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:37:31.936+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:37:31.939+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:37:31.982+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-19T01:38:02.171+0000] {processor.py:153} INFO - Started process (PID=15460) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:38:02.172+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:38:02.173+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:38:02.173+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:38:02.203+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:38:02.202+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:38:02.204+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:38:02.230+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-19T01:38:32.397+0000] {processor.py:153} INFO - Started process (PID=15549) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:38:32.399+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:38:32.399+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:38:32.399+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:38:32.450+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:38:32.448+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:38:32.451+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:38:32.478+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-19T01:39:03.181+0000] {processor.py:153} INFO - Started process (PID=15622) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:39:03.183+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:39:03.184+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:39:03.184+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:39:03.214+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:39:03.212+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:39:03.215+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:39:03.246+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.072 seconds
[2023-03-19T01:39:33.552+0000] {processor.py:153} INFO - Started process (PID=15695) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:39:33.553+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:39:33.554+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:39:33.554+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:39:33.584+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:39:33.583+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:39:33.585+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:39:33.607+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.058 seconds
[2023-03-19T01:40:03.716+0000] {processor.py:153} INFO - Started process (PID=15784) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:40:03.717+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:40:03.718+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:40:03.718+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:40:03.755+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:40:03.754+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:40:03.756+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:40:03.778+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-19T01:40:33.921+0000] {processor.py:153} INFO - Started process (PID=15858) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:40:33.922+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:40:33.923+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:40:33.923+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:40:33.950+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:40:33.949+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:40:33.951+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:40:33.973+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.057 seconds
[2023-03-19T01:41:04.071+0000] {processor.py:153} INFO - Started process (PID=15947) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:41:04.073+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:41:04.075+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:41:04.075+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:41:04.114+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:41:04.112+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:41:04.115+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:41:04.158+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.094 seconds
[2023-03-19T01:41:34.845+0000] {processor.py:153} INFO - Started process (PID=16020) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:41:34.853+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:41:34.856+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:41:34.855+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:41:34.941+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:41:34.938+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:41:34.942+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:41:35.038+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.204 seconds
[2023-03-19T01:42:05.166+0000] {processor.py:153} INFO - Started process (PID=16093) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:42:05.167+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:42:05.168+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:42:05.168+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:42:05.192+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:42:05.191+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:42:05.193+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:42:05.214+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.051 seconds
[2023-03-19T01:42:35.807+0000] {processor.py:153} INFO - Started process (PID=16182) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:42:35.809+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:42:35.810+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:42:35.810+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:42:35.840+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:42:35.838+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:42:35.841+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:42:35.867+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-19T01:43:06.165+0000] {processor.py:153} INFO - Started process (PID=16255) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:43:06.167+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:43:06.168+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:43:06.168+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:43:06.187+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:43:06.185+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:43:06.187+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:43:06.205+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.043 seconds
[2023-03-19T01:43:36.828+0000] {processor.py:153} INFO - Started process (PID=16345) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:43:36.830+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:43:36.831+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:43:36.831+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:43:36.865+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:43:36.864+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:43:36.867+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:43:36.894+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-19T01:44:07.602+0000] {processor.py:153} INFO - Started process (PID=16419) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:44:07.603+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:44:07.604+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:44:07.604+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:44:07.633+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:44:07.632+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:44:07.634+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:44:07.662+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.064 seconds
[2023-03-19T01:44:38.362+0000] {processor.py:153} INFO - Started process (PID=16492) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:44:38.364+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:44:38.366+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:44:38.366+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:44:38.386+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:44:38.385+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:44:38.387+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:44:38.407+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-19T01:45:08.861+0000] {processor.py:153} INFO - Started process (PID=16581) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:45:08.862+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:45:08.863+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:45:08.863+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:45:08.897+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:45:08.895+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:45:08.898+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:45:08.920+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-19T01:45:39.127+0000] {processor.py:153} INFO - Started process (PID=16654) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:45:39.129+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:45:39.130+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:45:39.130+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:45:39.155+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:45:39.154+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:45:39.156+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:45:39.179+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-19T01:46:09.812+0000] {processor.py:153} INFO - Started process (PID=16743) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:46:09.814+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:46:09.816+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:46:09.816+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:46:09.857+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:46:09.855+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:46:09.858+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:46:09.893+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.087 seconds
[2023-03-19T01:46:40.792+0000] {processor.py:153} INFO - Started process (PID=16816) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:46:40.794+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:46:40.795+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:46:40.795+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:46:40.828+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:46:40.827+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:46:40.829+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:46:40.850+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.061 seconds
[2023-03-19T01:47:11.716+0000] {processor.py:153} INFO - Started process (PID=16905) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:47:11.718+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:47:11.720+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:47:11.719+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:47:11.753+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:47:11.752+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:47:11.754+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:47:11.777+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-19T01:47:41.900+0000] {processor.py:153} INFO - Started process (PID=16978) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:47:41.902+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:47:41.904+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:47:41.903+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:47:41.939+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:47:41.937+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:47:41.940+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:47:42.004+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.108 seconds
[2023-03-19T01:48:12.248+0000] {processor.py:153} INFO - Started process (PID=17051) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:48:12.250+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:48:12.253+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:48:12.252+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:48:12.304+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:48:12.302+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:48:12.306+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:48:12.339+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.099 seconds
[2023-03-19T01:48:42.897+0000] {processor.py:153} INFO - Started process (PID=17131) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:48:42.909+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:48:42.911+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:48:42.911+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:48:43.032+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:48:43.030+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:48:43.039+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:48:43.108+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.218 seconds
[2023-03-19T01:49:13.706+0000] {processor.py:153} INFO - Started process (PID=17196) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:49:13.707+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:49:13.708+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:49:13.708+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:49:13.726+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:49:13.725+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:49:13.727+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:49:13.752+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.050 seconds
[2023-03-19T01:49:43.995+0000] {processor.py:153} INFO - Started process (PID=17286) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:49:43.996+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:49:43.998+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:49:43.998+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:49:44.029+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:49:44.026+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:49:44.030+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:49:44.064+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-19T01:50:14.174+0000] {processor.py:153} INFO - Started process (PID=17359) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:50:14.175+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:50:14.177+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:50:14.176+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:50:14.219+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:50:14.217+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:50:14.220+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:50:14.310+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.141 seconds
[2023-03-19T01:50:44.708+0000] {processor.py:153} INFO - Started process (PID=17434) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:50:44.709+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:50:44.710+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:50:44.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:50:44.737+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:50:44.735+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:50:44.738+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:50:44.770+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.068 seconds
[2023-03-19T01:51:15.126+0000] {processor.py:153} INFO - Started process (PID=17521) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:51:15.127+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:51:15.128+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:51:15.128+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:51:15.160+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:51:15.159+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:51:15.161+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:51:15.184+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-19T01:51:45.740+0000] {processor.py:153} INFO - Started process (PID=17594) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:51:45.741+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:51:45.742+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:51:45.742+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:51:45.774+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:51:45.773+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:51:45.776+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:51:45.800+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.064 seconds
[2023-03-19T01:52:16.534+0000] {processor.py:153} INFO - Started process (PID=17683) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:52:16.544+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:52:16.550+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:52:16.548+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:52:16.627+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:52:16.624+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:52:16.628+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:52:16.669+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.144 seconds
[2023-03-19T01:52:47.040+0000] {processor.py:153} INFO - Started process (PID=17756) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:52:47.041+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:52:47.042+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:52:47.042+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:52:47.068+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:52:47.066+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:52:47.068+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:52:47.093+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T01:53:17.494+0000] {processor.py:153} INFO - Started process (PID=17829) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:53:17.497+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:53:17.499+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:53:17.499+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:53:17.521+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:53:17.519+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:53:17.521+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:53:17.544+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-19T01:53:48.287+0000] {processor.py:153} INFO - Started process (PID=17918) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:53:48.289+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:53:48.290+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:53:48.290+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:53:48.335+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:53:48.333+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:53:48.336+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:53:48.367+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.083 seconds
[2023-03-19T01:54:19.266+0000] {processor.py:153} INFO - Started process (PID=17991) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:54:19.267+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:54:19.268+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:54:19.268+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:54:19.290+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:54:19.289+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:54:19.291+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:54:19.320+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T01:54:49.461+0000] {processor.py:153} INFO - Started process (PID=18073) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:54:49.463+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:54:49.465+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:54:49.464+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:54:49.493+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:54:49.491+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:54:49.494+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:54:49.523+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-19T01:55:19.795+0000] {processor.py:153} INFO - Started process (PID=18153) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:55:19.797+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:55:19.798+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:55:19.798+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:55:19.821+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:55:19.820+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:55:19.822+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:55:19.845+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T01:55:50.331+0000] {processor.py:153} INFO - Started process (PID=18226) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:55:50.332+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:55:50.333+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:55:50.333+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:55:50.355+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:55:50.354+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:55:50.356+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:55:50.384+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.057 seconds
[2023-03-19T01:56:20.796+0000] {processor.py:153} INFO - Started process (PID=18315) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:56:20.797+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:56:20.798+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:56:20.798+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:56:20.823+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:56:20.822+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:56:20.824+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:56:20.848+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-19T01:56:50.980+0000] {processor.py:153} INFO - Started process (PID=18388) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:56:50.982+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:56:50.983+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:56:50.983+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:56:51.024+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:56:51.023+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:56:51.025+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:56:51.048+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.072 seconds
[2023-03-19T01:57:21.570+0000] {processor.py:153} INFO - Started process (PID=18477) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:57:21.572+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:57:21.573+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:57:21.572+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:57:21.598+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:57:21.596+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:57:21.599+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:57:21.661+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-19T01:57:51.995+0000] {processor.py:153} INFO - Started process (PID=18550) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:57:51.996+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:57:51.998+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:57:51.997+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:57:52.035+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:57:52.033+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:57:52.038+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:57:52.076+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.084 seconds
[2023-03-19T01:58:22.233+0000] {processor.py:153} INFO - Started process (PID=18639) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:58:22.235+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:58:22.236+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:58:22.235+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:58:22.275+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:58:22.274+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:58:22.276+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:58:22.304+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.075 seconds
[2023-03-19T01:58:52.625+0000] {processor.py:153} INFO - Started process (PID=18712) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:58:52.626+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:58:52.628+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:58:52.627+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:58:52.654+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:58:52.653+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:58:52.655+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:58:52.681+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T01:59:23.213+0000] {processor.py:153} INFO - Started process (PID=18801) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:59:23.215+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:59:23.216+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:59:23.216+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:59:23.246+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:59:23.244+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:59:23.247+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:59:23.310+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.104 seconds
[2023-03-19T01:59:53.431+0000] {processor.py:153} INFO - Started process (PID=18874) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:59:53.432+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T01:59:53.433+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:59:53.433+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:59:53.461+0000] {logging_mixin.py:137} INFO - [2023-03-19T01:59:53.460+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T01:59:53.462+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T01:59:53.483+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T02:00:24.090+0000] {processor.py:153} INFO - Started process (PID=18956) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:00:24.091+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:00:24.092+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:00:24.092+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:00:24.120+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:00:24.119+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:00:24.121+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:00:24.143+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T02:00:54.263+0000] {processor.py:153} INFO - Started process (PID=19036) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:00:54.265+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:00:54.266+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:00:54.265+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:00:54.295+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:00:54.292+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:00:54.296+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:00:54.323+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-19T02:01:24.501+0000] {processor.py:153} INFO - Started process (PID=19109) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:01:24.502+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:01:24.503+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:01:24.503+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:01:24.535+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:01:24.534+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:01:24.536+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:01:24.560+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-19T02:01:54.842+0000] {processor.py:153} INFO - Started process (PID=19198) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:01:54.843+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:01:54.844+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:01:54.844+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:01:54.886+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:01:54.885+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:01:54.887+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:01:54.911+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-19T02:02:25.732+0000] {processor.py:153} INFO - Started process (PID=19272) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:02:25.733+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:02:25.734+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:02:25.734+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:02:25.772+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:02:25.770+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:02:25.773+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:02:25.801+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-19T02:02:55.999+0000] {processor.py:153} INFO - Started process (PID=19361) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:02:56.001+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:02:56.002+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:02:56.002+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:02:56.041+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:02:56.039+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:02:56.042+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:02:56.085+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-19T02:03:26.384+0000] {processor.py:153} INFO - Started process (PID=19434) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:03:26.387+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:03:26.388+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:03:26.388+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:03:26.425+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:03:26.423+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:03:26.426+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:03:26.450+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.072 seconds
[2023-03-19T02:03:56.604+0000] {processor.py:153} INFO - Started process (PID=19507) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:03:56.605+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:03:56.606+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:03:56.606+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:03:56.624+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:03:56.623+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:03:56.625+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:03:56.644+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.043 seconds
[2023-03-19T02:04:27.007+0000] {processor.py:153} INFO - Started process (PID=19596) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:04:27.024+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:04:27.028+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:04:27.028+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:04:27.086+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:04:27.084+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:04:27.088+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:04:27.118+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.124 seconds
[2023-03-19T02:04:57.537+0000] {processor.py:153} INFO - Started process (PID=19669) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:04:57.538+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:04:57.539+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:04:57.539+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:04:57.576+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:04:57.574+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:04:57.577+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:04:57.609+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-19T02:05:27.864+0000] {processor.py:153} INFO - Started process (PID=19758) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:05:27.865+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:05:27.866+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:05:27.866+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:05:27.895+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:05:27.894+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:05:27.896+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:05:27.920+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-19T02:05:57.975+0000] {processor.py:153} INFO - Started process (PID=19831) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:05:57.976+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:05:57.978+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:05:57.977+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:05:58.003+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:05:58.001+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:05:58.004+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:05:58.032+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.061 seconds
[2023-03-19T02:06:28.376+0000] {processor.py:153} INFO - Started process (PID=19920) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:06:28.377+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:06:28.378+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:06:28.378+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:06:28.405+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:06:28.404+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:06:28.406+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:06:28.431+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.058 seconds
[2023-03-19T02:06:58.762+0000] {processor.py:153} INFO - Started process (PID=19993) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:06:58.764+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:06:58.765+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:06:58.765+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:06:58.802+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:06:58.801+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:06:58.803+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:06:58.828+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-19T02:07:29.706+0000] {processor.py:153} INFO - Started process (PID=20082) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:07:29.708+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:07:29.709+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:07:29.709+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:07:29.743+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:07:29.741+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:07:29.744+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:07:29.786+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-19T02:07:59.858+0000] {processor.py:153} INFO - Started process (PID=20156) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:07:59.860+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:07:59.861+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:07:59.861+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:07:59.887+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:07:59.886+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:07:59.887+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:07:59.904+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.050 seconds
[2023-03-19T02:08:30.168+0000] {processor.py:153} INFO - Started process (PID=20229) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:08:30.169+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:08:30.171+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:08:30.170+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:08:30.198+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:08:30.197+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:08:30.199+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:08:30.219+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-19T02:09:00.370+0000] {processor.py:153} INFO - Started process (PID=20318) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:09:00.371+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:09:00.372+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:09:00.372+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:09:00.388+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:09:00.387+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:09:00.389+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:09:00.405+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.039 seconds
[2023-03-19T02:09:30.945+0000] {processor.py:153} INFO - Started process (PID=20392) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:09:30.946+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:09:30.947+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:09:30.947+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:09:30.991+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:09:30.990+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:09:30.992+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:09:31.015+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-19T02:10:01.359+0000] {processor.py:153} INFO - Started process (PID=20481) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:10:01.360+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:10:01.361+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:10:01.361+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:10:01.408+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:10:01.406+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:10:01.408+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:10:01.426+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-19T02:10:31.955+0000] {processor.py:153} INFO - Started process (PID=20554) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:10:31.956+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:10:31.957+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:10:31.957+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:10:31.985+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:10:31.984+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:10:31.986+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:10:32.008+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T02:11:02.827+0000] {processor.py:153} INFO - Started process (PID=20643) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:11:02.829+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:11:02.830+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:11:02.829+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:11:02.864+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:11:02.863+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:11:02.865+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:11:02.885+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.061 seconds
[2023-03-19T02:11:33.120+0000] {processor.py:153} INFO - Started process (PID=20716) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:11:33.121+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:11:33.122+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:11:33.122+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:11:33.142+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:11:33.141+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:11:33.142+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:11:33.162+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.044 seconds
[2023-03-19T02:12:03.630+0000] {processor.py:153} INFO - Started process (PID=20798) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:12:03.631+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:12:03.632+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:12:03.632+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:12:03.667+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:12:03.666+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:12:03.668+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:12:03.694+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.067 seconds
[2023-03-19T02:12:33.866+0000] {processor.py:153} INFO - Started process (PID=20879) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:12:33.867+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:12:33.867+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:12:33.867+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:12:33.896+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:12:33.895+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:12:33.897+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:12:33.915+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.052 seconds
[2023-03-19T02:13:04.219+0000] {processor.py:153} INFO - Started process (PID=20952) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:13:04.220+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:13:04.220+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:13:04.220+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:13:04.242+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:13:04.241+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:13:04.243+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:13:04.304+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-19T02:13:34.362+0000] {processor.py:153} INFO - Started process (PID=21042) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:13:34.364+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:13:34.365+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:13:34.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:13:34.389+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:13:34.387+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:13:34.390+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:13:34.413+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.054 seconds
[2023-03-19T02:14:04.667+0000] {processor.py:153} INFO - Started process (PID=21115) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:14:04.669+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:14:04.671+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:14:04.671+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:14:04.714+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:14:04.712+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:14:04.715+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:14:04.755+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.094 seconds
[2023-03-19T02:14:35.219+0000] {processor.py:153} INFO - Started process (PID=21204) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:14:35.220+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:14:35.221+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:14:35.221+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:14:35.253+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:14:35.252+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:14:35.253+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:14:35.277+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-19T02:15:05.727+0000] {processor.py:153} INFO - Started process (PID=21277) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:15:05.729+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:15:05.730+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:15:05.729+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:15:05.751+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:15:05.750+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:15:05.752+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:15:05.812+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-19T02:15:36.242+0000] {processor.py:153} INFO - Started process (PID=21366) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:15:36.244+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:15:36.245+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:15:36.245+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:15:36.276+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:15:36.274+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:15:36.277+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:15:36.307+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-19T02:16:07.222+0000] {processor.py:153} INFO - Started process (PID=21439) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:16:07.225+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:16:07.226+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:16:07.226+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:16:07.258+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:16:07.257+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:16:07.259+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:16:07.305+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.087 seconds
[2023-03-19T02:16:37.817+0000] {processor.py:153} INFO - Started process (PID=21521) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:16:37.819+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:16:37.821+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:16:37.820+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:16:37.860+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:16:37.858+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:16:37.861+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:16:37.898+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.086 seconds
[2023-03-19T02:17:08.219+0000] {processor.py:153} INFO - Started process (PID=21601) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:17:08.220+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:17:08.221+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:17:08.221+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:17:08.249+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:17:08.248+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:17:08.250+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:17:08.271+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-19T02:17:38.515+0000] {processor.py:153} INFO - Started process (PID=21675) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:17:38.516+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:17:38.517+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:17:38.517+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:17:38.546+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:17:38.545+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:17:38.547+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:17:38.568+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.057 seconds
[2023-03-19T02:18:08.859+0000] {processor.py:153} INFO - Started process (PID=21764) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:18:08.861+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:18:08.861+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:18:08.861+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:18:08.906+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:18:08.904+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:18:08.907+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:18:08.931+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.075 seconds
[2023-03-19T02:18:39.461+0000] {processor.py:153} INFO - Started process (PID=21837) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:18:39.465+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:18:39.466+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:18:39.466+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:18:39.498+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:18:39.497+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:18:39.499+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:18:39.527+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-19T02:19:09.913+0000] {processor.py:153} INFO - Started process (PID=21926) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:19:09.915+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:19:09.916+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:19:09.916+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:19:09.960+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:19:09.958+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:19:09.961+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:19:09.986+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.081 seconds
[2023-03-19T02:19:40.181+0000] {processor.py:153} INFO - Started process (PID=21999) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:19:40.183+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:19:40.184+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:19:40.184+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:19:40.211+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:19:40.209+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:19:40.211+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:19:40.240+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-19T02:20:10.434+0000] {processor.py:153} INFO - Started process (PID=22088) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:20:10.436+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:20:10.437+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:20:10.437+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:20:10.476+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:20:10.475+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:20:10.477+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:20:10.498+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.067 seconds
[2023-03-19T02:20:40.642+0000] {processor.py:153} INFO - Started process (PID=22161) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:20:40.644+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:20:40.645+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:20:40.645+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:20:40.673+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:20:40.672+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:20:40.674+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:20:40.698+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T02:21:11.023+0000] {processor.py:153} INFO - Started process (PID=22235) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:21:11.025+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:21:11.027+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:21:11.026+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:21:11.057+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:21:11.056+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:21:11.059+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:21:11.092+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.074 seconds
[2023-03-19T02:21:41.222+0000] {processor.py:153} INFO - Started process (PID=22324) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:21:41.224+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:21:41.225+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:21:41.225+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:21:41.268+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:21:41.268+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:21:41.269+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:21:41.289+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-19T02:22:11.517+0000] {processor.py:153} INFO - Started process (PID=22397) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:22:11.519+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:22:11.520+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:22:11.520+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:22:11.580+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:22:11.578+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:22:11.581+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:22:11.616+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.104 seconds
[2023-03-19T02:22:41.838+0000] {processor.py:153} INFO - Started process (PID=22479) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:22:41.840+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:22:41.842+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:22:41.842+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:22:41.886+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:22:41.884+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:22:41.887+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:22:41.920+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.091 seconds
[2023-03-19T02:23:11.966+0000] {processor.py:153} INFO - Started process (PID=22559) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:23:11.967+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:23:11.968+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:23:11.968+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:23:11.997+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:23:11.996+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:23:11.998+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:23:12.019+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T02:23:42.154+0000] {processor.py:153} INFO - Started process (PID=22631) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:23:42.156+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:23:42.157+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:23:42.156+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:23:42.201+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:23:42.199+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:23:42.202+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:23:42.228+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-19T02:24:12.355+0000] {processor.py:153} INFO - Started process (PID=22720) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:24:12.356+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:24:12.357+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:24:12.357+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:24:12.384+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:24:12.382+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:24:12.384+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:24:12.408+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T02:24:42.775+0000] {processor.py:153} INFO - Started process (PID=22793) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:24:42.777+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:24:42.778+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:24:42.778+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:24:42.812+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:24:42.811+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:24:42.813+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:24:42.879+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.113 seconds
[2023-03-19T02:25:13.824+0000] {processor.py:153} INFO - Started process (PID=22882) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:25:13.825+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:25:13.826+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:25:13.826+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:25:13.862+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:25:13.860+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:25:13.864+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:25:13.886+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-19T02:25:44.753+0000] {processor.py:153} INFO - Started process (PID=22955) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:25:44.756+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:25:44.760+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:25:44.760+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:25:44.793+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:25:44.791+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:25:44.794+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:25:44.832+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-19T02:26:14.980+0000] {processor.py:153} INFO - Started process (PID=23044) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:26:14.981+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:26:14.982+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:26:14.982+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:26:15.012+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:26:15.011+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:26:15.014+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:26:15.049+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.075 seconds
[2023-03-19T02:26:45.147+0000] {processor.py:153} INFO - Started process (PID=23116) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:26:45.149+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:26:45.154+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:26:45.152+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:26:45.182+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:26:45.180+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:26:45.182+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:26:45.204+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-19T02:27:15.654+0000] {processor.py:153} INFO - Started process (PID=23198) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:27:15.656+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:27:15.657+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:27:15.657+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:27:15.691+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:27:15.690+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:27:15.693+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:27:15.720+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-19T02:27:46.024+0000] {processor.py:153} INFO - Started process (PID=23278) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:27:46.025+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:27:46.026+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:27:46.026+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:27:46.067+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:27:46.066+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:27:46.068+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:27:46.092+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-19T02:28:16.583+0000] {processor.py:153} INFO - Started process (PID=23351) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:28:16.584+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:28:16.585+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:28:16.585+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:28:16.615+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:28:16.614+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:28:16.616+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:28:16.640+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.060 seconds
[2023-03-19T02:28:47.226+0000] {processor.py:153} INFO - Started process (PID=23440) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:28:47.227+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:28:47.228+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:28:47.228+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:28:47.262+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:28:47.260+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:28:47.263+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:28:47.300+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.079 seconds
[2023-03-19T02:29:17.589+0000] {processor.py:153} INFO - Started process (PID=23513) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:29:17.590+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:29:17.591+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:29:17.591+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:29:17.630+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:29:17.629+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:29:17.631+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:29:17.654+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.068 seconds
[2023-03-19T02:29:47.788+0000] {processor.py:153} INFO - Started process (PID=23602) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:29:47.791+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:29:47.792+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:29:47.792+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:29:47.824+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:29:47.823+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:29:47.825+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:29:47.853+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.068 seconds
[2023-03-19T02:30:18.142+0000] {processor.py:153} INFO - Started process (PID=23676) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:30:18.143+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:30:18.145+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:30:18.145+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:30:18.177+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:30:18.176+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:30:18.178+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:30:18.205+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-19T02:30:48.485+0000] {processor.py:153} INFO - Started process (PID=23758) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:30:48.486+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:30:48.487+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:30:48.486+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:30:48.522+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:30:48.520+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:30:48.523+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:30:48.550+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.068 seconds
[2023-03-19T02:31:18.639+0000] {processor.py:153} INFO - Started process (PID=23837) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:31:18.640+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:31:18.641+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:31:18.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:31:18.668+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:31:18.666+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:31:18.668+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:31:18.690+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.054 seconds
[2023-03-19T02:31:48.825+0000] {processor.py:153} INFO - Started process (PID=23910) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:31:48.826+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:31:48.827+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:31:48.826+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:31:48.856+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:31:48.854+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:31:48.856+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:31:48.883+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.064 seconds
[2023-03-19T02:32:19.148+0000] {processor.py:153} INFO - Started process (PID=23999) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:32:19.149+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:32:19.150+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:32:19.150+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:32:19.177+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:32:19.175+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:32:19.178+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:32:19.202+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.058 seconds
[2023-03-19T02:32:49.666+0000] {processor.py:153} INFO - Started process (PID=24073) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:32:49.668+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:32:49.670+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:32:49.670+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:32:49.710+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:32:49.708+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:32:49.711+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:32:49.756+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-19T02:33:20.287+0000] {processor.py:153} INFO - Started process (PID=24155) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:33:20.288+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:33:20.289+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:33:20.289+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:33:20.316+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:33:20.314+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:33:20.317+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:33:20.337+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.054 seconds
[2023-03-19T02:33:50.982+0000] {processor.py:153} INFO - Started process (PID=24235) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:33:50.983+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:33:50.984+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:33:50.984+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:33:51.003+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:33:51.002+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:33:51.004+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:33:51.025+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.046 seconds
[2023-03-19T02:34:21.981+0000] {processor.py:153} INFO - Started process (PID=24317) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:34:21.982+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:34:21.983+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:34:21.983+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:34:22.015+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:34:22.014+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:34:22.015+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:34:22.038+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.060 seconds
[2023-03-19T02:34:52.150+0000] {processor.py:153} INFO - Started process (PID=24397) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:34:52.153+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:34:52.154+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:34:52.154+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:34:52.191+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:34:52.189+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:34:52.192+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:34:52.228+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-19T02:35:22.341+0000] {processor.py:153} INFO - Started process (PID=24469) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:35:22.342+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:35:22.342+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:35:22.342+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:35:22.375+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:35:22.374+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:35:22.375+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:35:22.399+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-19T02:35:52.529+0000] {processor.py:153} INFO - Started process (PID=24558) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:35:52.530+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:35:52.531+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:35:52.531+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:35:52.561+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:35:52.558+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:35:52.562+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:35:52.590+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-19T02:36:22.950+0000] {processor.py:153} INFO - Started process (PID=24629) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:36:22.951+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:36:22.953+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:36:22.952+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:36:22.988+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:36:22.987+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:36:22.989+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:36:23.011+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-19T02:36:53.850+0000] {processor.py:153} INFO - Started process (PID=24719) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:36:53.852+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:36:53.853+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:36:53.853+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:36:53.884+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:36:53.882+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:36:53.885+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:36:53.920+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.077 seconds
[2023-03-19T02:37:24.723+0000] {processor.py:153} INFO - Started process (PID=24793) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:37:24.734+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:37:24.736+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:37:24.735+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:37:24.820+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:37:24.818+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:37:24.822+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:37:24.865+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.148 seconds
[2023-03-19T02:37:55.149+0000] {processor.py:153} INFO - Started process (PID=24875) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:37:55.150+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:37:55.151+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:37:55.151+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:37:55.173+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:37:55.172+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:37:55.173+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:37:55.193+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.047 seconds
[2023-03-19T02:38:25.869+0000] {processor.py:153} INFO - Started process (PID=24955) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:38:25.870+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:38:25.872+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:38:25.871+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:38:25.932+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:38:25.931+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:38:25.933+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:38:25.955+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-19T02:38:56.223+0000] {processor.py:153} INFO - Started process (PID=25035) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:38:56.224+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:38:56.225+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:38:56.225+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:38:56.257+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:38:56.256+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:38:56.258+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:38:56.283+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-19T02:39:27.135+0000] {processor.py:153} INFO - Started process (PID=25117) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:39:27.137+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:39:27.138+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:39:27.138+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:39:27.199+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:39:27.197+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:39:27.200+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:39:27.242+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.113 seconds
[2023-03-19T02:39:57.973+0000] {processor.py:153} INFO - Started process (PID=25190) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:39:57.975+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:39:57.975+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:39:57.975+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:39:58.002+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:39:58.001+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:39:58.003+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:39:58.026+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-19T02:40:28.274+0000] {processor.py:153} INFO - Started process (PID=25279) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:40:28.275+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:40:28.276+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:40:28.276+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:40:28.294+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:40:28.293+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:40:28.295+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:40:28.315+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.044 seconds
[2023-03-19T02:40:58.495+0000] {processor.py:153} INFO - Started process (PID=25352) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:40:58.496+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:40:58.497+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:40:58.497+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:40:58.514+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:40:58.513+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:40:58.515+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:40:58.540+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.049 seconds
[2023-03-19T02:41:29.173+0000] {processor.py:153} INFO - Started process (PID=25441) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:41:29.174+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:41:29.175+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:41:29.175+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:41:29.211+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:41:29.209+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:41:29.212+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:41:29.245+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-19T02:41:59.848+0000] {processor.py:153} INFO - Started process (PID=25514) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:41:59.853+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:41:59.859+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:41:59.858+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:41:59.917+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:41:59.915+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:41:59.919+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:41:59.967+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.125 seconds
[2023-03-19T02:42:30.567+0000] {processor.py:153} INFO - Started process (PID=25603) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:42:30.568+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:42:30.569+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:42:30.569+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:42:30.596+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:42:30.594+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:42:30.597+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:42:30.645+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-19T02:43:01.478+0000] {processor.py:153} INFO - Started process (PID=25676) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:43:01.479+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:43:01.481+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:43:01.481+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:43:01.515+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:43:01.514+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:43:01.516+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:43:01.550+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.081 seconds
[2023-03-19T02:43:31.811+0000] {processor.py:153} INFO - Started process (PID=25765) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:43:31.812+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:43:31.813+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:43:31.813+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:43:31.846+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:43:31.845+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:43:31.847+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:43:31.867+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T02:44:01.921+0000] {processor.py:153} INFO - Started process (PID=25838) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:44:01.922+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:44:01.924+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:44:01.923+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:44:01.965+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:44:01.964+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:44:01.967+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:44:02.003+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-19T02:44:32.179+0000] {processor.py:153} INFO - Started process (PID=25911) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:44:32.181+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:44:32.182+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:44:32.182+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:44:32.219+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:44:32.217+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:44:32.224+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:44:32.253+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.079 seconds
[2023-03-19T02:45:02.416+0000] {processor.py:153} INFO - Started process (PID=26000) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:45:02.417+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:45:02.418+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:45:02.418+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:45:02.441+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:45:02.440+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:45:02.442+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:45:02.466+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.054 seconds
[2023-03-19T02:45:32.600+0000] {processor.py:153} INFO - Started process (PID=26073) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:45:32.601+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:45:32.602+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:45:32.602+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:45:32.647+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:45:32.645+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:45:32.648+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:45:32.668+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-19T02:46:02.877+0000] {processor.py:153} INFO - Started process (PID=26162) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:46:02.878+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:46:02.879+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:46:02.879+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:46:02.907+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:46:02.905+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:46:02.908+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:46:02.938+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-19T02:46:33.880+0000] {processor.py:153} INFO - Started process (PID=26235) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:46:33.881+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:46:33.882+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:46:33.882+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:46:33.918+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:46:33.916+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:46:33.919+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:46:33.957+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-19T02:47:04.147+0000] {processor.py:153} INFO - Started process (PID=26324) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:47:04.149+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:47:04.150+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:47:04.150+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:47:04.184+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:47:04.183+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:47:04.185+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:47:04.219+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-19T02:47:34.576+0000] {processor.py:153} INFO - Started process (PID=26397) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:47:34.577+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:47:34.578+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:47:34.578+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:47:34.606+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:47:34.604+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:47:34.608+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:47:34.630+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T02:48:04.759+0000] {processor.py:153} INFO - Started process (PID=26479) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:48:04.761+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:48:04.762+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:48:04.762+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:48:04.786+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:48:04.784+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:48:04.786+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:48:04.811+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-19T02:48:35.014+0000] {processor.py:153} INFO - Started process (PID=26559) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:48:35.015+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:48:35.017+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:48:35.017+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:48:35.052+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:48:35.050+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:48:35.053+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:48:35.080+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.072 seconds
[2023-03-19T02:49:22.265+0000] {processor.py:153} INFO - Started process (PID=26642) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:49:22.271+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:49:22.284+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:49:22.283+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:49:23.408+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:49:23.405+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:49:23.434+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:49:23.546+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 1.294 seconds
[2023-03-19T02:49:54.072+0000] {processor.py:153} INFO - Started process (PID=26706) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:49:54.083+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:49:54.086+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:49:54.086+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:49:54.152+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:49:54.148+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:49:54.154+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:49:54.230+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.166 seconds
[2023-03-19T02:50:24.560+0000] {processor.py:153} INFO - Started process (PID=26779) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:50:24.615+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:50:24.627+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:50:24.627+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:50:24.978+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:50:24.976+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:50:24.990+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:50:25.056+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.494 seconds
[2023-03-19T02:50:55.361+0000] {processor.py:153} INFO - Started process (PID=26852) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:50:55.363+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:50:55.365+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:50:55.365+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:50:55.397+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:50:55.395+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:50:55.399+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:50:55.432+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-19T02:51:26.035+0000] {processor.py:153} INFO - Started process (PID=26925) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:51:26.044+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:51:26.047+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:51:26.047+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:51:26.147+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:51:26.145+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:51:26.149+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:51:26.234+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.205 seconds
[2023-03-19T02:51:56.485+0000] {processor.py:153} INFO - Started process (PID=27007) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:51:56.486+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:51:56.487+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:51:56.487+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:51:56.511+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:51:56.510+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:51:56.512+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:51:56.541+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-19T02:52:26.894+0000] {processor.py:153} INFO - Started process (PID=27070) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:52:26.909+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:52:26.920+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:52:26.920+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:52:27.325+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:52:27.322+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:52:27.335+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:52:27.412+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.523 seconds
[2023-03-19T02:52:59.622+0000] {processor.py:153} INFO - Started process (PID=27142) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:52:59.635+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:52:59.638+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:52:59.638+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:52:59.782+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:52:59.779+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:52:59.783+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:52:59.906+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.291 seconds
[2023-03-19T02:53:30.207+0000] {processor.py:153} INFO - Started process (PID=27213) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:53:30.212+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:53:30.214+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:53:30.213+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:53:30.324+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:53:30.322+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:53:30.328+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:53:30.412+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.222 seconds
[2023-03-19T02:54:00.670+0000] {processor.py:153} INFO - Started process (PID=27286) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:54:00.672+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:54:00.674+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:54:00.673+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:54:00.769+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:54:00.767+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:54:00.771+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:54:00.821+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.156 seconds
[2023-03-19T02:54:31.541+0000] {processor.py:153} INFO - Started process (PID=27359) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:54:31.543+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:54:31.545+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:54:31.545+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:54:31.603+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:54:31.600+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:54:31.604+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:54:31.661+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.127 seconds
[2023-03-19T02:55:01.901+0000] {processor.py:153} INFO - Started process (PID=27416) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:55:01.903+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:55:01.905+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:55:01.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:55:01.955+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:55:01.953+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:55:01.956+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:55:02.009+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.114 seconds
[2023-03-19T02:55:32.149+0000] {processor.py:153} INFO - Started process (PID=27490) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:55:32.153+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:55:32.156+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:55:32.155+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:55:32.210+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:55:32.208+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:55:32.212+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:55:32.249+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.108 seconds
[2023-03-19T02:56:02.506+0000] {processor.py:153} INFO - Started process (PID=27563) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:56:02.507+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:56:02.509+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:56:02.509+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:56:02.583+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:56:02.580+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:56:02.584+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:56:02.653+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.158 seconds
[2023-03-19T02:56:33.245+0000] {processor.py:153} INFO - Started process (PID=27636) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:56:33.246+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:56:33.248+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:56:33.248+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:56:33.301+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:56:33.299+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:56:33.302+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:56:33.340+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.100 seconds
[2023-03-19T02:57:03.804+0000] {processor.py:153} INFO - Started process (PID=27716) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:57:03.807+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:57:03.808+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:57:03.808+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:57:03.867+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:57:03.865+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:57:03.869+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:57:03.907+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.108 seconds
[2023-03-19T02:57:34.699+0000] {processor.py:153} INFO - Started process (PID=27791) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:57:34.702+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:57:34.715+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:57:34.715+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:57:34.788+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:57:34.786+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:57:34.791+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:57:34.906+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.224 seconds
[2023-03-19T02:58:05.570+0000] {processor.py:153} INFO - Started process (PID=27855) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:58:05.571+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:58:05.574+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:58:05.574+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:58:05.640+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:58:05.638+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:58:05.642+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:58:05.681+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.120 seconds
[2023-03-19T02:58:36.253+0000] {processor.py:153} INFO - Started process (PID=27937) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:58:36.255+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:58:36.257+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:58:36.257+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:58:36.310+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:58:36.308+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:58:36.312+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:58:36.359+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.115 seconds
[2023-03-19T02:59:06.610+0000] {processor.py:153} INFO - Started process (PID=28001) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:59:06.611+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:59:06.613+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:59:06.612+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:59:06.661+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:59:06.660+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:59:06.662+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:59:06.700+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.094 seconds
[2023-03-19T02:59:36.898+0000] {processor.py:153} INFO - Started process (PID=28074) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:59:36.900+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T02:59:36.902+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:59:36.902+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:59:36.953+0000] {logging_mixin.py:137} INFO - [2023-03-19T02:59:36.951+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T02:59:36.954+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T02:59:36.983+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.091 seconds
[2023-03-19T03:00:07.566+0000] {processor.py:153} INFO - Started process (PID=28164) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:00:07.568+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:00:07.569+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:00:07.569+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:00:07.620+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:00:07.618+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:00:07.622+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:00:07.665+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.105 seconds
[2023-03-19T03:00:37.963+0000] {processor.py:153} INFO - Started process (PID=28237) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:00:37.965+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:00:37.973+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:00:37.973+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:00:38.058+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:00:38.055+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:00:38.059+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:00:38.145+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.190 seconds
[2023-03-19T03:01:08.575+0000] {processor.py:153} INFO - Started process (PID=28310) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:01:08.577+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:01:08.578+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:01:08.578+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:01:08.627+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:01:08.625+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:01:08.629+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:01:08.671+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.104 seconds
[2023-03-19T03:01:39.455+0000] {processor.py:153} INFO - Started process (PID=28383) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:01:39.457+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:01:39.458+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:01:39.458+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:01:39.518+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:01:39.517+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:01:39.520+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:01:39.555+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.106 seconds
[2023-03-19T03:02:09.857+0000] {processor.py:153} INFO - Started process (PID=28456) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:02:09.859+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:02:09.861+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:02:09.861+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:02:09.916+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:02:09.914+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:02:09.917+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:02:09.953+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.103 seconds
[2023-03-19T03:02:40.240+0000] {processor.py:153} INFO - Started process (PID=28529) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:02:40.241+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:02:40.242+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:02:40.242+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:02:40.293+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:02:40.291+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:02:40.293+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:02:40.323+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-19T03:03:11.024+0000] {processor.py:153} INFO - Started process (PID=28602) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:03:11.026+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:03:11.028+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:03:11.028+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:03:11.080+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:03:11.079+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:03:11.082+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:03:11.113+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.096 seconds
[2023-03-19T03:03:41.997+0000] {processor.py:153} INFO - Started process (PID=28675) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:03:41.999+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:03:42.001+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:03:42.001+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:03:42.102+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:03:42.098+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:03:42.103+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:03:42.145+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.155 seconds
[2023-03-19T03:04:12.472+0000] {processor.py:153} INFO - Started process (PID=28748) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:04:12.474+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:04:12.475+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:04:12.475+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:04:12.529+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:04:12.528+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:04:12.530+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:04:12.565+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.101 seconds
[2023-03-19T03:04:42.901+0000] {processor.py:153} INFO - Started process (PID=28821) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:04:42.902+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:04:42.904+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:04:42.904+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:04:42.961+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:04:42.959+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:04:42.963+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:04:43.002+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.106 seconds
[2023-03-19T03:05:13.807+0000] {processor.py:153} INFO - Started process (PID=28894) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:05:13.809+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:05:13.810+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:05:13.810+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:05:13.889+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:05:13.887+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:05:13.891+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:05:13.925+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.123 seconds
[2023-03-19T03:05:44.609+0000] {processor.py:153} INFO - Started process (PID=28967) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:05:44.612+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:05:44.613+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:05:44.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:05:44.668+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:05:44.666+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:05:44.670+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:05:44.697+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-19T03:06:15.495+0000] {processor.py:153} INFO - Started process (PID=29040) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:06:15.497+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:06:15.499+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:06:15.499+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:06:15.554+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:06:15.552+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:06:15.555+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:06:15.596+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.108 seconds
[2023-03-19T03:06:45.896+0000] {processor.py:153} INFO - Started process (PID=29113) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:06:45.898+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:06:45.899+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:06:45.899+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:06:45.956+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:06:45.952+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:06:45.961+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:06:45.999+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.108 seconds
[2023-03-19T03:07:16.291+0000] {processor.py:153} INFO - Started process (PID=29193) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:07:16.295+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:07:16.297+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:07:16.297+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:07:16.355+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:07:16.353+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:07:16.356+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:07:16.395+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.111 seconds
[2023-03-19T03:07:46.767+0000] {processor.py:153} INFO - Started process (PID=29275) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:07:46.769+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:07:46.770+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:07:46.770+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:07:46.827+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:07:46.826+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:07:46.829+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:07:46.865+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.103 seconds
[2023-03-19T03:08:17.110+0000] {processor.py:153} INFO - Started process (PID=29348) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:08:17.112+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:08:17.113+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:08:17.113+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:08:17.168+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:08:17.166+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-19T03:08:17.169+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:08:17.196+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-19T03:24:57.539+0000] {processor.py:153} INFO - Started process (PID=29407) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-19T03:24:57.613+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-19T03:24:57.710+0000] {logging_mixin.py:137} INFO - [2023-03-19T03:24:57.710+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-20T00:31:03.170+0000] {logging_mixin.py:137} INFO - [2023-03-20T00:31:02.910+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-20T00:31:03.709+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-20T00:31:06.263+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 19.850 seconds
