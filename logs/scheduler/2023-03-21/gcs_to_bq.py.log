[2023-03-21T00:00:20.678+0000] {processor.py:153} INFO - Started process (PID=11866) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:00:20.679+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:00:20.681+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:00:20.681+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:00:20.716+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:00:20.714+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:00:20.717+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:00:20.743+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-21T00:00:50.930+0000] {processor.py:153} INFO - Started process (PID=11940) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:00:50.932+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:00:50.933+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:00:50.933+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:00:50.962+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:00:50.960+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:00:50.963+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:00:50.996+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-21T00:01:21.308+0000] {processor.py:153} INFO - Started process (PID=12013) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:01:21.310+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:01:21.311+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:01:21.311+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:01:21.341+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:01:21.339+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:01:21.342+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:01:21.374+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-21T00:01:51.874+0000] {processor.py:153} INFO - Started process (PID=12095) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:01:51.876+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:01:51.877+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:01:51.877+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:01:51.910+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:01:51.908+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:01:51.911+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:01:51.965+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.096 seconds
[2023-03-21T00:02:22.115+0000] {processor.py:153} INFO - Started process (PID=12173) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:02:22.119+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:02:22.120+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:02:22.120+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:02:22.152+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:02:22.151+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:02:22.154+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:02:22.183+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-21T00:02:52.662+0000] {processor.py:153} INFO - Started process (PID=12247) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:02:52.664+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:02:52.665+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:02:52.665+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:02:52.696+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:02:52.694+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:02:52.696+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:02:52.723+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-21T00:03:23.204+0000] {processor.py:153} INFO - Started process (PID=12336) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:03:23.205+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:03:23.206+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:03:23.206+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:03:23.233+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:03:23.232+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:03:23.234+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:03:23.257+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-21T00:03:53.541+0000] {processor.py:153} INFO - Started process (PID=12409) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:03:53.542+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:03:53.543+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:03:53.543+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:03:53.589+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:03:53.587+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:03:53.591+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:03:53.619+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-21T00:04:24.535+0000] {processor.py:153} INFO - Started process (PID=12492) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:04:24.536+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:04:24.538+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:04:24.538+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:04:24.572+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:04:24.571+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:04:24.572+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:04:24.595+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-21T00:04:54.711+0000] {processor.py:153} INFO - Started process (PID=12572) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:04:54.713+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:04:54.714+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:04:54.714+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:04:54.744+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:04:54.743+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:04:54.745+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:04:54.764+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.057 seconds
[2023-03-21T00:05:24.927+0000] {processor.py:153} INFO - Started process (PID=12645) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:05:24.928+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:05:24.929+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:05:24.929+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:05:24.948+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:05:24.947+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:05:24.948+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:05:25.013+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-21T00:05:55.535+0000] {processor.py:153} INFO - Started process (PID=12724) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:05:55.536+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:05:55.537+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:05:55.537+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:05:55.561+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:05:55.560+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:05:55.562+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:05:55.589+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.058 seconds
[2023-03-21T00:06:26.167+0000] {processor.py:153} INFO - Started process (PID=12807) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:06:26.169+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:06:26.171+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:06:26.170+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:06:26.215+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:06:26.214+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:06:26.219+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:06:26.254+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-21T00:06:56.635+0000] {processor.py:153} INFO - Started process (PID=12880) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:06:56.638+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:06:56.640+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:06:56.640+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:06:56.691+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:06:56.688+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:06:56.693+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:06:56.735+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.109 seconds
[2023-03-21T00:07:27.139+0000] {processor.py:153} INFO - Started process (PID=12953) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:07:27.141+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:07:27.141+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:07:27.141+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:07:27.169+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:07:27.168+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:07:27.170+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:07:27.189+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.052 seconds
[2023-03-21T00:07:57.328+0000] {processor.py:153} INFO - Started process (PID=13026) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:07:57.329+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:07:57.330+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:07:57.330+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:07:57.360+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:07:57.359+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:07:57.362+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:07:57.383+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.057 seconds
[2023-03-21T00:08:27.929+0000] {processor.py:153} INFO - Started process (PID=13099) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:08:27.931+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:08:27.933+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:08:27.933+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:08:27.964+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:08:27.962+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:08:27.964+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:08:27.990+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-21T00:08:58.583+0000] {processor.py:153} INFO - Started process (PID=13173) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:08:58.585+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:08:58.590+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:08:58.590+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:08:58.634+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:08:58.633+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:08:58.635+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:08:58.675+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.098 seconds
[2023-03-21T00:09:29.116+0000] {processor.py:153} INFO - Started process (PID=13246) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:09:29.118+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:09:29.119+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:09:29.119+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:09:29.146+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:09:29.145+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:09:29.147+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:09:29.173+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-21T00:09:59.842+0000] {processor.py:153} INFO - Started process (PID=13319) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:09:59.843+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:09:59.844+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:09:59.844+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:09:59.870+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:09:59.868+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:09:59.870+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:09:59.895+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.058 seconds
[2023-03-21T00:10:30.799+0000] {processor.py:153} INFO - Started process (PID=13408) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:10:30.801+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:10:30.802+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:10:30.802+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:10:30.884+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:10:30.882+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:10:30.886+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:10:30.921+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.127 seconds
[2023-03-21T00:11:01.618+0000] {processor.py:153} INFO - Started process (PID=13481) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:11:01.620+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:11:01.621+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:11:01.621+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:11:01.656+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:11:01.655+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:11:01.657+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:11:01.703+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.094 seconds
[2023-03-21T00:11:32.053+0000] {processor.py:153} INFO - Started process (PID=13554) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:11:32.054+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:11:32.055+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:11:32.055+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:11:32.084+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:11:32.082+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:11:32.085+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:11:32.164+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.115 seconds
[2023-03-21T00:12:02.338+0000] {processor.py:153} INFO - Started process (PID=13627) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:12:02.340+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:12:02.341+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:12:02.341+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:12:02.378+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:12:02.375+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:12:02.379+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:12:02.410+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.075 seconds
[2023-03-21T00:12:32.471+0000] {processor.py:153} INFO - Started process (PID=13700) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:12:32.472+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:12:32.473+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:12:32.473+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:12:32.516+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:12:32.514+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:12:32.517+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:12:32.547+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.081 seconds
[2023-03-21T00:13:02.665+0000] {processor.py:153} INFO - Started process (PID=13773) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:13:02.666+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:13:02.667+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:13:02.667+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:13:02.693+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:13:02.692+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:13:02.694+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:13:02.716+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.054 seconds
[2023-03-21T00:13:32.860+0000] {processor.py:153} INFO - Started process (PID=13846) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:13:32.862+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:13:32.863+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:13:32.863+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:13:32.887+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:13:32.886+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:13:32.888+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:13:32.911+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.056 seconds
[2023-03-21T00:14:03.103+0000] {processor.py:153} INFO - Started process (PID=13935) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:14:03.107+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:14:03.118+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:14:03.118+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:14:03.174+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:14:03.172+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:14:03.176+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:14:03.218+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.124 seconds
[2023-03-21T00:14:33.490+0000] {processor.py:153} INFO - Started process (PID=14008) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:14:33.492+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:14:33.494+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:14:33.494+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:14:33.533+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:14:33.531+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:14:33.534+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:14:33.570+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.084 seconds
[2023-03-21T00:15:04.271+0000] {processor.py:153} INFO - Started process (PID=14082) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:15:04.272+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:15:04.273+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:15:04.272+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:15:04.305+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:15:04.305+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:15:04.306+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:15:04.327+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-21T00:15:34.678+0000] {processor.py:153} INFO - Started process (PID=14172) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:15:34.679+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:15:34.680+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:15:34.680+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:15:34.718+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:15:34.717+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:15:34.719+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:15:34.741+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.068 seconds
[2023-03-21T00:16:05.075+0000] {processor.py:153} INFO - Started process (PID=14245) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:16:05.076+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:16:05.078+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:16:05.078+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:16:05.105+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:16:05.104+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:16:05.106+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:16:05.131+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.060 seconds
[2023-03-21T00:16:36.070+0000] {processor.py:153} INFO - Started process (PID=14318) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:16:36.071+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:16:36.073+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:16:36.072+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:16:36.111+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:16:36.110+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:16:36.112+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:16:36.154+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-21T00:17:06.327+0000] {processor.py:153} INFO - Started process (PID=14398) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:17:06.339+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:17:06.345+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:17:06.345+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:17:06.414+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:17:06.411+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:17:06.416+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:17:06.495+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.175 seconds
[2023-03-21T00:17:37.113+0000] {processor.py:153} INFO - Started process (PID=14462) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:17:37.117+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:17:37.119+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:17:37.118+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:17:37.163+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:17:37.161+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:17:37.166+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:17:37.194+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-21T00:18:07.950+0000] {processor.py:153} INFO - Started process (PID=14551) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:18:07.955+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:18:07.958+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:18:07.957+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:18:08.001+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:18:07.999+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:18:08.002+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:18:08.037+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.093 seconds
[2023-03-21T00:18:38.120+0000] {processor.py:153} INFO - Started process (PID=14624) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:18:38.121+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:18:38.123+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:18:38.122+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:18:38.156+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:18:38.154+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:18:38.157+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:18:38.194+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-21T00:19:08.901+0000] {processor.py:153} INFO - Started process (PID=14697) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:19:08.903+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:19:08.905+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:19:08.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:19:08.982+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:19:08.979+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:19:08.986+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:19:09.050+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.159 seconds
[2023-03-21T00:19:39.327+0000] {processor.py:153} INFO - Started process (PID=14770) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:19:39.329+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:19:39.332+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:19:39.332+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:19:39.372+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:19:39.371+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:19:39.373+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:19:39.399+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-21T00:20:09.582+0000] {processor.py:153} INFO - Started process (PID=14843) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:20:09.584+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:20:09.585+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:20:09.585+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:20:09.635+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:20:09.632+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:20:09.636+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:20:09.669+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-21T00:20:40.382+0000] {processor.py:153} INFO - Started process (PID=14916) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:20:40.384+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:20:40.388+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:20:40.386+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:20:40.428+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:20:40.426+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:20:40.429+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:20:40.464+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-21T00:21:10.521+0000] {processor.py:153} INFO - Started process (PID=14989) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:21:10.522+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:21:10.523+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:21:10.523+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:21:10.573+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:21:10.572+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:21:10.575+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:21:10.608+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-21T00:21:41.554+0000] {processor.py:153} INFO - Started process (PID=15064) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:21:41.571+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:21:41.573+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:21:41.573+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:21:41.665+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:21:41.662+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:21:41.666+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:21:41.750+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.203 seconds
[2023-03-21T00:22:11.885+0000] {processor.py:153} INFO - Started process (PID=15137) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:22:11.887+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:22:11.889+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:22:11.888+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:22:11.924+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:22:11.923+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:22:11.925+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:22:11.947+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-21T00:22:42.295+0000] {processor.py:153} INFO - Started process (PID=15210) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:22:42.297+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:22:42.299+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:22:42.299+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:22:42.351+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:22:42.349+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:22:42.353+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:22:42.436+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.149 seconds
[2023-03-21T00:23:13.241+0000] {processor.py:153} INFO - Started process (PID=15282) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:23:13.243+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:23:13.245+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:23:13.245+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:23:13.295+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:23:13.293+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:23:13.311+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:23:13.357+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.122 seconds
[2023-03-21T00:23:43.498+0000] {processor.py:153} INFO - Started process (PID=15356) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:23:43.500+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:23:43.501+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:23:43.501+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:23:43.545+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:23:43.542+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:23:43.546+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:23:43.597+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.105 seconds
[2023-03-21T00:24:14.207+0000] {processor.py:153} INFO - Started process (PID=15429) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:24:14.208+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:24:14.210+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:24:14.210+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:24:14.242+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:24:14.240+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:24:14.243+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:24:14.270+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-21T00:24:44.606+0000] {processor.py:153} INFO - Started process (PID=15502) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:24:44.610+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:24:44.613+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:24:44.613+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:24:44.652+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:24:44.650+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:24:44.654+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:24:44.708+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.109 seconds
[2023-03-21T00:25:14.985+0000] {processor.py:153} INFO - Started process (PID=15575) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:25:14.999+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:25:15.021+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:25:15.020+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:25:15.648+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:25:15.646+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:25:15.659+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:25:15.735+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.767 seconds
[2023-03-21T00:25:46.679+0000] {processor.py:153} INFO - Started process (PID=15649) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:25:46.681+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:25:46.683+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:25:46.683+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:25:46.737+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:25:46.733+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:25:46.740+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:25:46.792+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.119 seconds
[2023-03-21T00:26:16.973+0000] {processor.py:153} INFO - Started process (PID=15722) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:26:16.975+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:26:16.985+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:26:16.979+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:26:17.052+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:26:17.050+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:26:17.055+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:26:17.150+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.184 seconds
[2023-03-21T00:26:47.540+0000] {processor.py:153} INFO - Started process (PID=15793) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:26:47.541+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:26:47.542+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:26:47.542+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:26:47.579+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:26:47.578+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:26:47.580+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:26:47.609+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-21T00:27:18.046+0000] {processor.py:153} INFO - Started process (PID=15866) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:27:18.048+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:27:18.052+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:27:18.052+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:27:18.083+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:27:18.081+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:27:18.084+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:27:18.118+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-21T00:27:48.476+0000] {processor.py:153} INFO - Started process (PID=15954) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:27:48.480+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:27:48.482+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:27:48.481+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:27:48.541+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:27:48.538+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:27:48.543+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:27:48.585+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.121 seconds
[2023-03-21T00:28:19.031+0000] {processor.py:153} INFO - Started process (PID=16027) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:28:19.033+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:28:19.034+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:28:19.034+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:28:19.072+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:28:19.070+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:28:19.073+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:28:19.110+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-21T00:28:49.581+0000] {processor.py:153} INFO - Started process (PID=16101) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:28:49.585+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:28:49.587+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:28:49.587+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:28:49.625+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:28:49.623+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:28:49.626+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:28:49.659+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.084 seconds
[2023-03-21T00:29:20.020+0000] {processor.py:153} INFO - Started process (PID=16174) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:29:20.021+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:29:20.022+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:29:20.022+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:29:20.053+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:29:20.052+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:29:20.054+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:29:20.082+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-21T00:29:50.734+0000] {processor.py:153} INFO - Started process (PID=16247) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:29:50.736+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:29:50.737+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:29:50.737+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:29:50.771+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:29:50.769+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:29:50.771+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:29:50.798+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-21T00:30:21.483+0000] {processor.py:153} INFO - Started process (PID=16320) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:30:21.484+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:30:21.485+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:30:21.485+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:30:21.524+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:30:21.521+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:30:21.531+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:30:21.566+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-21T00:30:51.643+0000] {processor.py:153} INFO - Started process (PID=16409) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:30:51.646+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:30:51.647+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:30:51.647+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:30:51.685+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:30:51.683+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:30:51.687+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:30:51.733+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.096 seconds
[2023-03-21T00:31:22.388+0000] {processor.py:153} INFO - Started process (PID=16482) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:31:22.391+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:31:22.411+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:31:22.411+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:31:22.553+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:31:22.551+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:31:22.555+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:31:22.640+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.260 seconds
[2023-03-21T00:31:53.633+0000] {processor.py:153} INFO - Started process (PID=16555) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:31:53.634+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:31:53.636+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:31:53.635+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:31:53.664+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:31:53.662+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:31:53.665+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:31:53.696+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-21T00:32:24.146+0000] {processor.py:153} INFO - Started process (PID=16629) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:32:24.148+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:32:24.150+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:32:24.150+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:32:24.182+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:32:24.181+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:32:24.183+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:32:24.211+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-21T00:32:54.811+0000] {processor.py:153} INFO - Started process (PID=16702) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:32:54.812+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:32:54.813+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:32:54.813+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:32:54.838+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:32:54.836+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:32:54.839+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:32:54.869+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.061 seconds
[2023-03-21T00:33:25.533+0000] {processor.py:153} INFO - Started process (PID=16775) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:33:25.534+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:33:25.536+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:33:25.535+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:33:25.593+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:33:25.591+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:33:25.595+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:33:25.629+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.104 seconds
[2023-03-21T00:33:56.470+0000] {processor.py:153} INFO - Started process (PID=16848) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:33:56.471+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:33:56.474+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:33:56.474+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:33:56.517+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:33:56.515+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:33:56.518+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:33:56.546+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-21T00:34:26.616+0000] {processor.py:153} INFO - Started process (PID=16921) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:34:26.618+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:34:26.619+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:34:26.619+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:34:26.656+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:34:26.654+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:34:26.658+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:34:26.692+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-21T00:34:57.683+0000] {processor.py:153} INFO - Started process (PID=16994) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:34:57.684+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:34:57.685+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:34:57.685+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:34:57.712+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:34:57.710+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:34:57.713+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:34:57.733+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.053 seconds
[2023-03-21T00:35:28.465+0000] {processor.py:153} INFO - Started process (PID=17083) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:35:28.466+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:35:28.468+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:35:28.468+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:35:28.496+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:35:28.495+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:35:28.498+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:35:28.528+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-21T00:35:59.095+0000] {processor.py:153} INFO - Started process (PID=17156) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:35:59.105+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:35:59.111+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:35:59.111+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:35:59.161+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:35:59.159+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:35:59.162+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:35:59.219+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.131 seconds
[2023-03-21T00:36:29.746+0000] {processor.py:153} INFO - Started process (PID=17222) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:36:29.755+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:36:29.759+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:36:29.759+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:36:30.182+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:36:30.179+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:36:30.186+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:36:30.248+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.508 seconds
[2023-03-21T00:37:00.806+0000] {processor.py:153} INFO - Started process (PID=17286) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:37:00.808+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:37:00.811+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:37:00.811+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:37:00.881+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:37:00.879+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:37:00.898+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:37:00.968+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.168 seconds
[2023-03-21T00:37:31.831+0000] {processor.py:153} INFO - Started process (PID=17359) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:37:31.832+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:37:31.833+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:37:31.833+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:37:31.862+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:37:31.861+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:37:31.863+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:37:31.890+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-21T00:38:02.747+0000] {processor.py:153} INFO - Started process (PID=17433) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:38:02.772+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:38:02.805+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:38:02.805+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:38:03.032+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:38:03.030+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:38:03.041+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:38:03.152+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.411 seconds
[2023-03-21T00:38:33.927+0000] {processor.py:153} INFO - Started process (PID=17506) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:38:33.931+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:38:33.933+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:38:33.933+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:38:33.982+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:38:33.980+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:38:33.984+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:38:34.036+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.122 seconds
[2023-03-21T00:39:04.568+0000] {processor.py:153} INFO - Started process (PID=17579) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:39:04.589+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:39:04.591+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:39:04.590+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:39:04.658+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:39:04.656+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:39:04.659+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:39:04.705+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.150 seconds
[2023-03-21T00:39:35.406+0000] {processor.py:153} INFO - Started process (PID=17652) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:39:35.408+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:39:35.411+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:39:35.410+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:39:35.445+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:39:35.444+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:39:35.446+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:39:35.475+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.075 seconds
[2023-03-21T00:40:06.028+0000] {processor.py:153} INFO - Started process (PID=17727) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:40:06.030+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:40:06.032+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:40:06.032+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:40:06.084+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:40:06.082+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:40:06.085+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:40:06.146+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.124 seconds
[2023-03-21T00:40:36.437+0000] {processor.py:153} INFO - Started process (PID=17814) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:40:36.439+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:40:36.441+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:40:36.441+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:40:36.513+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:40:36.511+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:40:36.515+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:40:36.633+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.205 seconds
[2023-03-21T00:41:06.783+0000] {processor.py:153} INFO - Started process (PID=17887) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:41:06.785+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:41:06.788+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:41:06.787+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:41:06.855+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:41:06.852+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:41:06.857+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:41:06.917+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.142 seconds
[2023-03-21T00:41:37.674+0000] {processor.py:153} INFO - Started process (PID=17960) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:41:37.680+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:41:37.683+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:41:37.683+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:41:37.751+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:41:37.749+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:41:37.753+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:41:37.812+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.143 seconds
[2023-03-21T00:42:07.992+0000] {processor.py:153} INFO - Started process (PID=18033) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:42:07.993+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:42:07.995+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:42:07.995+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:42:08.036+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:42:08.033+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:42:08.037+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:42:08.075+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-21T00:42:38.856+0000] {processor.py:153} INFO - Started process (PID=18106) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:42:38.860+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:42:38.862+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:42:38.862+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:42:38.930+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:42:38.927+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:42:38.932+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:42:38.970+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.126 seconds
[2023-03-21T00:43:09.497+0000] {processor.py:153} INFO - Started process (PID=18179) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:43:09.498+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:43:09.500+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:43:09.500+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:43:09.531+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:43:09.529+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:43:09.532+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:43:09.557+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-21T00:43:40.234+0000] {processor.py:153} INFO - Started process (PID=18252) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:43:40.236+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:43:40.237+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:43:40.237+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:43:40.294+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:43:40.292+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:43:40.296+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:43:40.334+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.111 seconds
[2023-03-21T00:44:10.582+0000] {processor.py:153} INFO - Started process (PID=18325) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:44:10.585+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:44:10.588+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:44:10.587+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:44:10.637+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:44:10.634+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:44:10.647+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:44:10.715+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.144 seconds
[2023-03-21T00:44:41.573+0000] {processor.py:153} INFO - Started process (PID=18398) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:44:41.575+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:44:41.576+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:44:41.576+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:44:41.613+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:44:41.611+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:44:41.614+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:44:41.655+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-21T00:45:12.038+0000] {processor.py:153} INFO - Started process (PID=18463) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:45:12.088+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:45:12.150+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:45:12.150+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:45:12.585+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:45:12.583+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:45:12.589+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:45:12.689+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.669 seconds
[2023-03-21T00:45:42.916+0000] {processor.py:153} INFO - Started process (PID=18527) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:45:42.927+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:45:42.936+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:45:42.935+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:45:43.282+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:45:43.279+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:45:43.286+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:45:43.388+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.479 seconds
[2023-03-21T00:46:13.521+0000] {processor.py:153} INFO - Started process (PID=18592) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:46:13.526+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:46:13.528+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:46:13.528+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:46:13.584+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:46:13.582+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:46:13.585+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:46:13.673+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.171 seconds
[2023-03-21T00:46:43.974+0000] {processor.py:153} INFO - Started process (PID=18657) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:46:43.978+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:46:43.986+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:46:43.986+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:46:44.037+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:46:44.035+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:46:44.039+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:46:44.164+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.200 seconds
[2023-03-21T00:47:15.082+0000] {processor.py:153} INFO - Started process (PID=18721) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:47:15.090+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:47:15.107+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:47:15.107+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:47:15.272+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:47:15.270+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:47:15.275+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:47:15.348+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.271 seconds
[2023-03-21T00:47:46.001+0000] {processor.py:153} INFO - Started process (PID=18788) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:47:46.003+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:47:46.005+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:47:46.005+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:47:46.099+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:47:46.096+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:47:46.103+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:47:46.209+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.215 seconds
[2023-03-21T00:48:16.422+0000] {processor.py:153} INFO - Started process (PID=18852) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:48:16.424+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:48:16.426+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:48:16.425+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:48:16.471+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:48:16.468+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:48:16.472+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:48:16.529+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.113 seconds
[2023-03-21T00:48:46.710+0000] {processor.py:153} INFO - Started process (PID=18934) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:48:46.712+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:48:46.713+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:48:46.713+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:48:46.755+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:48:46.753+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:48:46.756+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:48:46.787+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.084 seconds
[2023-03-21T00:49:17.031+0000] {processor.py:153} INFO - Started process (PID=19014) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:49:17.033+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:49:17.034+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:49:17.034+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:49:17.087+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:49:17.085+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:49:17.088+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:49:17.120+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.109 seconds
[2023-03-21T00:49:47.371+0000] {processor.py:153} INFO - Started process (PID=19085) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:49:47.374+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:49:47.385+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:49:47.385+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:49:47.442+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:49:47.440+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:49:47.443+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:49:47.539+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.184 seconds
[2023-03-21T00:50:17.632+0000] {processor.py:153} INFO - Started process (PID=19151) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:50:17.634+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:50:17.636+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:50:17.636+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:50:17.681+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:50:17.678+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:50:17.682+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:50:17.718+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.094 seconds
[2023-03-21T00:50:48.017+0000] {processor.py:153} INFO - Started process (PID=19231) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:50:48.018+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:50:48.020+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:50:48.020+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:50:48.065+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:50:48.063+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:50:48.067+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:50:48.108+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-21T00:51:18.323+0000] {processor.py:153} INFO - Started process (PID=19304) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:51:18.325+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:51:18.326+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:51:18.326+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:51:18.398+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:51:18.396+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:51:18.399+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:51:18.424+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.105 seconds
[2023-03-21T00:51:48.624+0000] {processor.py:153} INFO - Started process (PID=19377) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:51:48.626+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:51:48.627+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:51:48.627+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:51:48.663+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:51:48.661+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:51:48.664+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:51:48.703+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.084 seconds
[2023-03-21T00:52:19.347+0000] {processor.py:153} INFO - Started process (PID=19450) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:52:19.349+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:52:19.352+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:52:19.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:52:19.399+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:52:19.397+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:52:19.401+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:52:19.452+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.112 seconds
[2023-03-21T00:52:50.037+0000] {processor.py:153} INFO - Started process (PID=19523) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:52:50.043+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:52:50.045+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:52:50.045+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:52:50.098+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:52:50.096+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:52:50.100+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:52:50.165+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.135 seconds
[2023-03-21T00:53:20.478+0000] {processor.py:153} INFO - Started process (PID=19597) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:53:20.479+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:53:20.480+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:53:20.480+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:53:20.521+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:53:20.520+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:53:20.522+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:53:20.544+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-21T00:53:51.037+0000] {processor.py:153} INFO - Started process (PID=19670) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:53:51.040+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:53:51.041+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:53:51.041+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:53:51.085+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:53:51.083+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:53:51.087+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:53:51.146+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.116 seconds
[2023-03-21T00:54:21.576+0000] {processor.py:153} INFO - Started process (PID=19752) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:54:21.578+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:54:21.580+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:54:21.580+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:54:21.620+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:54:21.619+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:54:21.621+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:54:21.695+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.124 seconds
[2023-03-21T00:54:51.976+0000] {processor.py:153} INFO - Started process (PID=19832) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:54:51.978+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:54:51.980+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:54:51.979+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:54:52.022+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:54:52.020+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:54:52.023+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:54:52.067+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.097 seconds
[2023-03-21T00:55:22.266+0000] {processor.py:153} INFO - Started process (PID=19905) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:55:22.273+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:55:22.277+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:55:22.277+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:55:22.358+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:55:22.356+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:55:22.362+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:55:22.417+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.159 seconds
[2023-03-21T00:55:52.816+0000] {processor.py:153} INFO - Started process (PID=19977) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:55:52.817+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:55:52.818+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:55:52.818+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:55:52.845+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:55:52.844+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:55:52.846+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:55:52.865+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-21T00:56:23.199+0000] {processor.py:153} INFO - Started process (PID=20051) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:56:23.201+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:56:23.203+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:56:23.203+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:56:23.233+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:56:23.231+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:56:23.235+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:56:23.274+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-21T00:56:54.129+0000] {processor.py:153} INFO - Started process (PID=20134) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:56:54.131+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:56:54.134+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:56:54.133+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:56:54.195+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:56:54.193+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:56:54.196+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:56:54.246+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.125 seconds
[2023-03-21T00:57:24.561+0000] {processor.py:153} INFO - Started process (PID=20214) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:57:24.564+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:57:24.566+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:57:24.565+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:57:24.614+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:57:24.612+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:57:24.615+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:57:24.645+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.097 seconds
[2023-03-21T00:57:55.339+0000] {processor.py:153} INFO - Started process (PID=20287) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:57:55.346+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:57:55.349+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:57:55.349+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:57:55.416+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:57:55.414+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:57:55.418+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:57:55.482+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.150 seconds
[2023-03-21T00:58:26.086+0000] {processor.py:153} INFO - Started process (PID=20361) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:58:26.088+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:58:26.089+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:58:26.089+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:58:26.127+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:58:26.125+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:58:26.129+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:58:26.183+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.101 seconds
[2023-03-21T00:58:56.866+0000] {processor.py:153} INFO - Started process (PID=20434) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:58:56.873+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:58:56.875+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:58:56.874+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:58:56.929+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:58:56.927+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:58:56.932+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:58:56.990+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.136 seconds
[2023-03-21T00:59:27.053+0000] {processor.py:153} INFO - Started process (PID=20506) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:59:27.054+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:59:27.055+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:59:27.055+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:59:27.085+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:59:27.084+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:59:27.086+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:59:27.120+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-21T00:59:57.313+0000] {processor.py:153} INFO - Started process (PID=20595) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:59:57.316+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T00:59:57.320+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:59:57.320+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:59:57.431+0000] {logging_mixin.py:137} INFO - [2023-03-21T00:59:57.429+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T00:59:57.433+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T00:59:57.553+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.250 seconds
[2023-03-21T01:00:27.726+0000] {processor.py:153} INFO - Started process (PID=20660) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:00:27.728+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:00:27.731+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:00:27.730+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:00:27.776+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:00:27.774+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:00:27.777+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:00:27.821+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.107 seconds
[2023-03-21T01:00:58.044+0000] {processor.py:153} INFO - Started process (PID=20740) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:00:58.045+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:00:58.047+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:00:58.047+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:00:58.077+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:00:58.076+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:00:58.078+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:00:58.127+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.089 seconds
[2023-03-21T01:01:28.184+0000] {processor.py:153} INFO - Started process (PID=20812) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:01:28.185+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:01:28.187+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:01:28.186+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:01:28.218+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:01:28.216+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:01:28.219+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:01:28.302+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.123 seconds
[2023-03-21T01:01:58.387+0000] {processor.py:153} INFO - Started process (PID=20885) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:01:58.388+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:01:58.389+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:01:58.389+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:01:58.430+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:01:58.429+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:01:58.431+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:01:58.458+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.075 seconds
[2023-03-21T01:02:28.661+0000] {processor.py:153} INFO - Started process (PID=20957) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:02:28.664+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:02:28.665+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:02:28.665+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:02:28.713+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:02:28.711+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:02:28.715+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:02:28.792+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.137 seconds
[2023-03-21T01:02:59.142+0000] {processor.py:153} INFO - Started process (PID=21026) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:02:59.144+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:02:59.146+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:02:59.146+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:02:59.190+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:02:59.188+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:02:59.191+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:02:59.219+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.084 seconds
[2023-03-21T01:03:29.305+0000] {processor.py:153} INFO - Started process (PID=21100) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:03:29.307+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:03:29.308+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:03:29.308+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:03:29.343+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:03:29.342+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:03:29.344+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:03:29.372+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-21T01:04:00.228+0000] {processor.py:153} INFO - Started process (PID=21173) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:04:00.229+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:04:00.230+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:04:00.230+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:04:00.263+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:04:00.262+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:04:00.264+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:04:00.287+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.064 seconds
[2023-03-21T01:04:31.144+0000] {processor.py:153} INFO - Started process (PID=21246) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:04:31.153+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:04:31.155+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:04:31.155+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:04:31.196+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:04:31.194+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:04:31.197+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:04:31.257+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.120 seconds
[2023-03-21T01:05:01.570+0000] {processor.py:153} INFO - Started process (PID=21319) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:05:01.572+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:05:01.574+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:05:01.574+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:05:01.621+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:05:01.619+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:05:01.624+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:05:01.679+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.116 seconds
[2023-03-21T01:05:32.412+0000] {processor.py:153} INFO - Started process (PID=21392) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:05:32.414+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:05:32.416+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:05:32.416+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:05:32.462+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:05:32.460+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:05:32.463+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:05:32.509+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.103 seconds
[2023-03-21T01:06:02.621+0000] {processor.py:153} INFO - Started process (PID=21465) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:06:02.628+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:06:02.631+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:06:02.630+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:06:02.720+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:06:02.718+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:06:02.722+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:06:02.811+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.197 seconds
[2023-03-21T01:06:33.781+0000] {processor.py:153} INFO - Started process (PID=21539) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:06:33.794+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:06:33.796+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:06:33.795+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:06:34.266+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:06:34.263+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:06:34.268+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:06:34.343+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.569 seconds
[2023-03-21T01:07:04.814+0000] {processor.py:153} INFO - Started process (PID=21612) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:07:04.816+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:07:04.817+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:07:04.817+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:07:04.858+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:07:04.856+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:07:04.860+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:07:04.901+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.093 seconds
[2023-03-21T01:07:36.092+0000] {processor.py:153} INFO - Started process (PID=21686) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:07:36.094+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:07:36.191+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:07:36.191+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:07:36.800+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:07:36.798+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:07:36.801+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:07:36.903+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.818 seconds
[2023-03-21T01:08:07.280+0000] {processor.py:153} INFO - Started process (PID=21743) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:08:07.284+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:08:07.291+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:08:07.289+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:08:07.371+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:08:07.353+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:08:07.373+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:08:07.431+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.159 seconds
[2023-03-21T01:08:37.553+0000] {processor.py:153} INFO - Started process (PID=21817) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:08:37.554+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:08:37.555+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:08:37.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:08:37.592+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:08:37.590+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:08:37.593+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:08:37.613+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-21T01:09:07.849+0000] {processor.py:153} INFO - Started process (PID=21890) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:09:07.862+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:09:07.864+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:09:07.864+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:09:07.932+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:09:07.929+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:09:07.933+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:09:07.993+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.150 seconds
[2023-03-21T01:09:38.203+0000] {processor.py:153} INFO - Started process (PID=21962) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:09:38.205+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:09:38.208+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:09:38.208+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:09:38.248+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:09:38.247+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:09:38.250+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:09:38.288+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-21T01:10:09.294+0000] {processor.py:153} INFO - Started process (PID=22019) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:10:09.308+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:10:09.313+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:10:09.313+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:10:09.729+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:10:09.727+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:10:09.730+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:10:09.898+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.618 seconds
[2023-03-21T01:10:40.143+0000] {processor.py:153} INFO - Started process (PID=22092) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:10:40.146+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:10:40.151+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:10:40.147+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:10:40.244+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:10:40.241+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:10:40.245+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:10:40.318+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.182 seconds
[2023-03-21T01:11:10.638+0000] {processor.py:153} INFO - Started process (PID=22164) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:11:10.640+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:11:10.643+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:11:10.642+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:11:10.698+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:11:10.695+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:11:10.704+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:11:10.856+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.226 seconds
[2023-03-21T01:11:41.304+0000] {processor.py:153} INFO - Started process (PID=22230) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:11:41.317+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:11:41.319+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:11:41.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:11:41.379+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:11:41.377+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:11:41.381+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:11:41.429+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.140 seconds
[2023-03-21T01:12:11.776+0000] {processor.py:153} INFO - Started process (PID=22303) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:12:11.778+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:12:11.780+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:12:11.779+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:12:11.825+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:12:11.822+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:12:11.826+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:12:11.879+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.109 seconds
[2023-03-21T01:12:42.049+0000] {processor.py:153} INFO - Started process (PID=22385) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:12:42.052+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:12:42.058+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:12:42.058+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:12:42.102+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:12:42.100+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:12:42.103+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:12:42.140+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.097 seconds
[2023-03-21T01:13:12.694+0000] {processor.py:153} INFO - Started process (PID=22458) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:13:12.696+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:13:12.698+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:13:12.697+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:13:12.735+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:13:12.733+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:13:12.736+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:13:12.775+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.087 seconds
[2023-03-21T01:13:42.982+0000] {processor.py:153} INFO - Started process (PID=22524) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:13:42.986+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:13:42.989+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:13:42.988+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:13:43.228+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:13:43.226+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:13:43.236+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:13:43.465+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.492 seconds
[2023-03-21T01:14:13.813+0000] {processor.py:153} INFO - Started process (PID=22588) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:14:13.842+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:14:13.854+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:14:13.853+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:14:14.070+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:14:14.067+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:14:14.071+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:14:14.206+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.430 seconds
[2023-03-21T01:14:44.769+0000] {processor.py:153} INFO - Started process (PID=22661) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:14:44.770+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:14:44.771+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:14:44.771+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:14:44.802+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:14:44.800+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:14:44.803+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:14:44.827+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-21T01:15:15.234+0000] {processor.py:153} INFO - Started process (PID=22734) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:15:15.236+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:15:15.238+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:15:15.238+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:15:15.318+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:15:15.314+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:15:15.329+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:15:15.416+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.198 seconds
[2023-03-21T01:15:45.665+0000] {processor.py:153} INFO - Started process (PID=22806) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:15:45.668+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:15:45.671+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:15:45.671+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:15:45.714+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:15:45.704+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:15:45.716+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:15:45.766+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.113 seconds
[2023-03-21T01:16:16.313+0000] {processor.py:153} INFO - Started process (PID=22880) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:16:16.314+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:16:16.315+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:16:16.315+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:16:16.338+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:16:16.337+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:16:16.339+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:16:16.364+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-21T01:16:46.798+0000] {processor.py:153} INFO - Started process (PID=22953) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:16:46.800+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:16:46.802+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:16:46.801+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:16:46.845+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:16:46.841+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:16:46.846+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:16:46.889+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.097 seconds
[2023-03-21T01:17:17.221+0000] {processor.py:153} INFO - Started process (PID=23027) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:17:17.227+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:17:17.231+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:17:17.230+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:17:17.276+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:17:17.272+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:17:17.283+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:17:17.341+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.126 seconds
[2023-03-21T01:17:47.709+0000] {processor.py:153} INFO - Started process (PID=23101) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:17:47.713+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:17:47.715+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:17:47.714+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:17:47.998+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:17:47.996+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:17:47.999+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:17:48.070+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.368 seconds
[2023-03-21T01:18:18.333+0000] {processor.py:153} INFO - Started process (PID=23166) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:18:18.346+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:18:18.348+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:18:18.348+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:18:18.438+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:18:18.436+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:18:18.440+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:18:18.538+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.214 seconds
[2023-03-21T01:18:48.693+0000] {processor.py:153} INFO - Started process (PID=23229) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:18:48.697+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:18:48.705+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:18:48.704+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:18:48.851+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:18:48.842+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:18:48.856+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:18:48.937+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.250 seconds
[2023-03-21T01:19:24.488+0000] {processor.py:153} INFO - Started process (PID=23292) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:19:24.491+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:19:24.493+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:19:24.493+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:19:24.840+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:19:24.820+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:19:24.845+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:19:24.944+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.520 seconds
[2023-03-21T01:19:55.457+0000] {processor.py:153} INFO - Started process (PID=23365) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:19:55.459+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:19:55.460+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:19:55.460+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:19:55.495+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:19:55.493+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:19:55.496+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:19:55.530+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.079 seconds
[2023-03-21T01:20:26.077+0000] {processor.py:153} INFO - Started process (PID=23438) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:20:26.079+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:20:26.081+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:20:26.081+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:20:26.149+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:20:26.147+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:20:26.150+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:20:26.197+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.126 seconds
[2023-03-21T01:20:57.169+0000] {processor.py:153} INFO - Started process (PID=23511) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:20:57.188+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:20:57.191+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:20:57.190+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:20:57.407+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:20:57.405+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:20:57.417+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:20:57.505+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.353 seconds
[2023-03-21T01:21:27.723+0000] {processor.py:153} INFO - Started process (PID=23573) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:21:27.725+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:21:27.728+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:21:27.727+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:21:27.793+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:21:27.791+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:21:27.795+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:21:27.834+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.118 seconds
[2023-03-21T01:21:58.240+0000] {processor.py:153} INFO - Started process (PID=23647) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:21:58.241+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:21:58.253+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:21:58.243+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:21:58.483+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:21:58.481+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:21:58.488+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:21:58.557+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.323 seconds
[2023-03-21T01:22:28.776+0000] {processor.py:153} INFO - Started process (PID=23720) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:22:28.780+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:22:28.784+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:22:28.783+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:22:28.834+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:22:28.831+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:22:28.837+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:22:28.924+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.173 seconds
[2023-03-21T01:22:59.774+0000] {processor.py:153} INFO - Started process (PID=23777) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:22:59.776+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:22:59.779+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:22:59.778+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:22:59.836+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:22:59.834+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:22:59.839+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:22:59.894+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.128 seconds
[2023-03-21T01:23:30.167+0000] {processor.py:153} INFO - Started process (PID=23850) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:23:30.178+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:23:30.192+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:23:30.192+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:23:30.269+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:23:30.267+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:23:30.270+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:23:30.337+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.182 seconds
[2023-03-21T01:24:00.510+0000] {processor.py:153} INFO - Started process (PID=23923) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:24:00.511+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:24:00.512+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:24:00.512+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:24:00.547+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:24:00.545+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:24:00.548+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:24:00.581+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.077 seconds
[2023-03-21T01:24:31.338+0000] {processor.py:153} INFO - Started process (PID=23996) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:24:31.347+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:24:31.353+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:24:31.352+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:24:32.000+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:24:31.998+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:24:32.032+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:24:32.156+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.835 seconds
[2023-03-21T01:25:02.791+0000] {processor.py:153} INFO - Started process (PID=24053) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:25:02.793+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:25:02.795+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:25:02.795+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:25:02.846+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:25:02.844+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:25:02.847+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:25:02.896+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.110 seconds
[2023-03-21T01:25:33.941+0000] {processor.py:153} INFO - Started process (PID=24126) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:25:33.949+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:25:33.965+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:25:33.965+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:25:34.226+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:25:34.224+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:25:34.228+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:25:34.323+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.387 seconds
[2023-03-21T01:26:04.483+0000] {processor.py:153} INFO - Started process (PID=24183) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:26:04.484+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:26:04.486+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:26:04.486+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:26:04.522+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:26:04.521+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:26:04.523+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:26:04.548+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-21T01:26:34.913+0000] {processor.py:153} INFO - Started process (PID=24256) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:26:34.925+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:26:34.926+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:26:34.926+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:26:35.016+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:26:35.014+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:26:35.018+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:26:35.122+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.218 seconds
[2023-03-21T01:27:05.491+0000] {processor.py:153} INFO - Started process (PID=24312) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:27:05.494+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:27:05.505+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:27:05.504+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:27:05.570+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:27:05.568+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:27:05.572+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:27:05.671+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.188 seconds
[2023-03-21T01:27:36.300+0000] {processor.py:153} INFO - Started process (PID=24385) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:27:36.302+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:27:36.304+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:27:36.304+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:27:36.653+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:27:36.650+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:27:36.654+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:27:36.727+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.436 seconds
[2023-03-21T01:28:07.269+0000] {processor.py:153} INFO - Started process (PID=24449) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:28:07.275+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:28:07.288+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:28:07.287+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:28:07.354+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:28:07.352+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:28:07.361+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:28:07.466+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.204 seconds
[2023-03-21T01:28:38.256+0000] {processor.py:153} INFO - Started process (PID=24516) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:28:38.261+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:28:38.263+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:28:38.263+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:28:38.593+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:28:38.590+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:28:38.607+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:28:38.675+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.430 seconds
[2023-03-21T01:29:10.038+0000] {processor.py:153} INFO - Started process (PID=24583) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:29:10.040+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:29:10.176+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:29:10.175+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:29:11.166+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:29:11.164+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:29:11.313+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:29:11.514+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 1.493 seconds
[2023-03-21T01:29:45.103+0000] {processor.py:153} INFO - Started process (PID=24646) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:29:45.120+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:29:45.144+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:29:45.144+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:29:46.167+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:29:46.165+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:29:46.191+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:29:46.299+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 1.209 seconds
[2023-03-21T01:30:16.392+0000] {processor.py:153} INFO - Started process (PID=24718) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:30:16.408+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:30:16.411+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:30:16.410+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:30:16.562+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:30:16.560+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:30:16.563+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:30:16.719+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.334 seconds
[2023-03-21T01:30:47.930+0000] {processor.py:153} INFO - Started process (PID=24777) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:30:47.939+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:30:47.946+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:30:47.946+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:30:48.078+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:30:48.076+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:30:48.080+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:30:48.143+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.219 seconds
[2023-03-21T01:31:18.727+0000] {processor.py:153} INFO - Started process (PID=24848) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:31:18.739+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:31:18.740+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:31:18.740+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:31:18.843+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:31:18.840+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:31:18.844+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:31:18.924+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.204 seconds
[2023-03-21T01:31:49.586+0000] {processor.py:153} INFO - Started process (PID=24911) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:31:49.620+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:31:49.639+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:31:49.639+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:31:49.913+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:31:49.911+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:31:49.915+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:31:50.007+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.431 seconds
[2023-03-21T01:32:23.444+0000] {processor.py:153} INFO - Started process (PID=24977) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:32:23.461+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:32:23.479+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:32:23.478+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:32:23.625+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:32:23.623+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:32:23.626+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:32:23.716+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.289 seconds
[2023-03-21T01:32:55.804+0000] {processor.py:153} INFO - Started process (PID=25052) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:32:55.813+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:32:55.830+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:32:55.829+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:32:56.569+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:32:56.564+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:32:56.575+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:32:56.664+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.867 seconds
[2023-03-21T01:33:27.042+0000] {processor.py:153} INFO - Started process (PID=25109) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:33:27.045+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:33:27.047+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:33:27.046+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:33:27.083+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:33:27.081+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:33:27.085+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:33:27.133+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.097 seconds
[2023-03-21T01:33:57.636+0000] {processor.py:153} INFO - Started process (PID=25183) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:33:57.638+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:33:57.639+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:33:57.639+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:33:57.680+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:33:57.678+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:33:57.683+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:33:57.728+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.100 seconds
[2023-03-21T01:34:28.432+0000] {processor.py:153} INFO - Started process (PID=25257) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:34:28.435+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:34:28.439+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:34:28.438+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:34:28.478+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:34:28.476+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:34:28.481+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:34:28.526+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.103 seconds
[2023-03-21T01:34:59.094+0000] {processor.py:153} INFO - Started process (PID=25330) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:34:59.096+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:34:59.098+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:34:59.098+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:34:59.128+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:34:59.126+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:34:59.130+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:34:59.160+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-21T01:35:29.478+0000] {processor.py:153} INFO - Started process (PID=25396) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:35:29.480+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:35:29.482+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:35:29.482+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:35:29.525+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:35:29.523+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:35:29.527+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:35:29.590+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.125 seconds
[2023-03-21T01:36:01.204+0000] {processor.py:153} INFO - Started process (PID=25459) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:36:01.208+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:36:01.211+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:36:01.210+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:36:01.354+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:36:01.351+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:36:01.362+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:36:01.443+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.261 seconds
[2023-03-21T01:36:31.918+0000] {processor.py:153} INFO - Started process (PID=25531) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:36:31.929+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:36:31.932+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:36:31.932+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:36:32.026+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:36:32.023+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:36:32.028+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:36:32.131+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.220 seconds
[2023-03-21T01:37:02.727+0000] {processor.py:153} INFO - Started process (PID=25587) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:37:02.737+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:37:02.744+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:37:02.743+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:37:02.855+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:37:02.853+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:37:02.857+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:37:02.986+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.267 seconds
[2023-03-21T01:37:33.237+0000] {processor.py:153} INFO - Started process (PID=25660) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:37:33.239+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:37:33.241+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:37:33.241+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:37:33.347+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:37:33.330+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:37:33.348+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:37:33.424+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.196 seconds
[2023-03-21T01:38:03.866+0000] {processor.py:153} INFO - Started process (PID=25733) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:38:03.869+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:38:03.874+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:38:03.874+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:38:03.964+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:38:03.960+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:38:03.965+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:38:04.039+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.180 seconds
[2023-03-21T01:38:34.095+0000] {processor.py:153} INFO - Started process (PID=25806) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:38:34.096+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:38:34.098+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:38:34.098+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:38:34.141+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:38:34.139+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:38:34.142+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:38:34.173+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-21T01:39:04.712+0000] {processor.py:153} INFO - Started process (PID=25878) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:39:04.725+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:39:04.748+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:39:04.748+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:39:05.452+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:39:05.449+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:39:05.456+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:39:05.532+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.828 seconds
[2023-03-21T01:39:35.738+0000] {processor.py:153} INFO - Started process (PID=25935) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:39:35.740+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:39:35.742+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:39:35.742+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:39:35.789+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:39:35.787+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:39:35.790+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:39:35.845+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.113 seconds
[2023-03-21T01:40:06.448+0000] {processor.py:153} INFO - Started process (PID=26007) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:40:06.457+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:40:06.460+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:40:06.459+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:40:06.599+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:40:06.596+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:40:06.601+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:40:06.705+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.265 seconds
[2023-03-21T01:40:37.022+0000] {processor.py:153} INFO - Started process (PID=26073) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:40:37.036+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:40:37.046+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:40:37.045+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:40:37.146+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:40:37.144+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:40:37.148+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:40:37.298+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.285 seconds
[2023-03-21T01:41:07.612+0000] {processor.py:153} INFO - Started process (PID=26137) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:41:07.613+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:41:07.615+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:41:07.615+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:41:07.667+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:41:07.664+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:41:07.668+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:41:07.718+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.116 seconds
[2023-03-21T01:41:38.300+0000] {processor.py:153} INFO - Started process (PID=26210) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:41:38.302+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:41:38.304+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:41:38.304+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:41:38.371+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:41:38.369+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:41:38.372+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:41:38.415+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.121 seconds
[2023-03-21T01:42:09.652+0000] {processor.py:153} INFO - Started process (PID=26283) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:42:09.656+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:42:09.661+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:42:09.660+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:42:09.708+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:42:09.706+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:42:09.709+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:42:09.783+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.137 seconds
[2023-03-21T01:42:39.903+0000] {processor.py:153} INFO - Started process (PID=26352) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:42:39.906+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:42:39.908+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:42:39.907+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:42:39.943+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:42:39.942+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:42:39.944+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:42:39.968+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-21T01:43:10.224+0000] {processor.py:153} INFO - Started process (PID=26425) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:43:10.225+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:43:10.226+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:43:10.226+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:43:10.258+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:43:10.256+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:43:10.259+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:43:10.287+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.068 seconds
[2023-03-21T01:43:41.045+0000] {processor.py:153} INFO - Started process (PID=26499) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:43:41.046+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:43:41.047+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:43:41.047+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:43:41.076+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:43:41.075+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:43:41.077+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:43:41.102+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.061 seconds
[2023-03-21T01:44:11.666+0000] {processor.py:153} INFO - Started process (PID=26572) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:44:11.668+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:44:11.670+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:44:11.669+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:44:11.713+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:44:11.709+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:44:11.714+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:44:11.768+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.109 seconds
[2023-03-21T01:44:42.072+0000] {processor.py:153} INFO - Started process (PID=26645) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:44:42.091+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:44:42.096+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:44:42.095+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:44:42.216+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:44:42.214+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:44:42.218+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:44:42.284+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.227 seconds
[2023-03-21T01:45:12.782+0000] {processor.py:153} INFO - Started process (PID=26718) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:45:12.787+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:45:12.789+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:45:12.789+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:45:12.845+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:45:12.843+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:45:12.846+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:45:12.905+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.133 seconds
[2023-03-21T01:45:43.694+0000] {processor.py:153} INFO - Started process (PID=26789) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:45:43.697+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:45:43.698+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:45:43.698+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:45:43.748+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:45:43.746+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:45:43.750+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:45:43.796+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.117 seconds
[2023-03-21T01:46:14.262+0000] {processor.py:153} INFO - Started process (PID=26862) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:46:14.263+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:46:14.265+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:46:14.265+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:46:14.302+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:46:14.300+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:46:14.304+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:46:14.347+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-21T01:46:44.984+0000] {processor.py:153} INFO - Started process (PID=26934) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:46:44.986+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:46:44.987+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:46:44.987+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:46:45.022+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:46:45.020+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:46:45.023+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:46:45.048+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-21T01:47:15.890+0000] {processor.py:153} INFO - Started process (PID=27016) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:47:15.892+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:47:15.894+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:47:15.894+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:47:15.948+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:47:15.946+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:47:15.951+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:47:16.031+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.148 seconds
[2023-03-21T01:47:46.706+0000] {processor.py:153} INFO - Started process (PID=27097) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:47:46.710+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:47:46.712+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:47:46.712+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:47:46.825+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:47:46.823+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:47:46.829+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:47:46.892+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.193 seconds
[2023-03-21T01:48:17.241+0000] {processor.py:153} INFO - Started process (PID=27170) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:48:17.244+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:48:17.246+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:48:17.245+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:48:17.290+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:48:17.289+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:48:17.291+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:48:17.318+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-21T01:48:47.448+0000] {processor.py:153} INFO - Started process (PID=27243) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:48:47.450+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:48:47.451+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:48:47.451+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:48:47.540+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:48:47.538+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:48:47.541+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:48:47.612+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.171 seconds
[2023-03-21T01:49:18.359+0000] {processor.py:153} INFO - Started process (PID=27314) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:49:18.361+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:49:18.362+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:49:18.362+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:49:18.399+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:49:18.397+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:49:18.405+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:49:18.455+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.101 seconds
[2023-03-21T01:49:49.421+0000] {processor.py:153} INFO - Started process (PID=27397) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:49:49.423+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:49:49.425+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:49:49.424+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:49:49.477+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:49:49.474+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:49:49.479+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:49:49.521+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.109 seconds
[2023-03-21T01:50:19.780+0000] {processor.py:153} INFO - Started process (PID=27477) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:50:19.781+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:50:19.782+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:50:19.782+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:50:19.819+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:50:19.817+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:50:19.820+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:50:19.846+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-21T01:50:50.340+0000] {processor.py:153} INFO - Started process (PID=27550) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:50:50.341+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:50:50.343+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:50:50.343+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:50:50.379+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:50:50.377+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:50:50.379+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:50:50.405+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.069 seconds
[2023-03-21T01:51:20.578+0000] {processor.py:153} INFO - Started process (PID=27623) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:51:20.580+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:51:20.583+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:51:20.582+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:51:20.614+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:51:20.613+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:51:20.615+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:51:20.672+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.099 seconds
[2023-03-21T01:51:51.408+0000] {processor.py:153} INFO - Started process (PID=27705) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:51:51.409+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:51:51.411+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:51:51.410+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:51:51.449+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:51:51.446+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:51:51.451+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:51:51.493+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-21T01:52:22.313+0000] {processor.py:153} INFO - Started process (PID=27778) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:52:22.315+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:52:22.317+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:52:22.317+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:52:22.374+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:52:22.372+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:52:22.376+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:52:22.419+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.113 seconds
[2023-03-21T01:52:53.160+0000] {processor.py:153} INFO - Started process (PID=27858) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:52:53.161+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:52:53.167+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:52:53.166+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:52:53.206+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:52:53.204+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:52:53.207+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:52:53.234+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-21T01:53:23.530+0000] {processor.py:153} INFO - Started process (PID=27931) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:53:23.532+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:53:23.533+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:53:23.533+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:53:23.587+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:53:23.585+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:53:23.591+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:53:23.692+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.166 seconds
[2023-03-21T01:53:53.856+0000] {processor.py:153} INFO - Started process (PID=28004) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:53:53.858+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:53:53.859+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:53:53.859+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:53:53.890+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:53:53.889+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:53:53.891+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:53:53.922+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-21T01:54:24.197+0000] {processor.py:153} INFO - Started process (PID=28077) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:54:24.199+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:54:24.201+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:54:24.200+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:54:24.239+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:54:24.238+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:54:24.240+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:54:24.270+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.077 seconds
[2023-03-21T01:54:54.403+0000] {processor.py:153} INFO - Started process (PID=28150) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:54:54.407+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:54:54.408+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:54:54.408+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:54:54.440+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:54:54.439+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:54:54.441+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:54:54.464+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-21T01:55:24.565+0000] {processor.py:153} INFO - Started process (PID=28222) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:55:24.566+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:55:24.567+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:55:24.567+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:55:24.596+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:55:24.594+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:55:24.597+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:55:24.621+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.059 seconds
[2023-03-21T01:55:55.062+0000] {processor.py:153} INFO - Started process (PID=28295) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:55:55.063+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:55:55.064+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:55:55.064+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:55:55.130+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:55:55.129+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:55:55.131+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:55:55.153+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-21T01:56:25.913+0000] {processor.py:153} INFO - Started process (PID=28384) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:56:25.917+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:56:25.919+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:56:25.919+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:56:26.059+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:56:26.057+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:56:26.060+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:56:26.100+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.193 seconds
[2023-03-21T01:56:56.766+0000] {processor.py:153} INFO - Started process (PID=28450) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:56:56.768+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:56:56.770+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:56:56.770+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:56:56.890+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:56:56.887+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:56:56.898+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:56:56.961+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.210 seconds
[2023-03-21T01:57:27.638+0000] {processor.py:153} INFO - Started process (PID=28513) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:57:27.641+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:57:27.652+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:57:27.652+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:57:27.757+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:57:27.754+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:57:27.766+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:57:27.828+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.197 seconds
[2023-03-21T01:57:59.651+0000] {processor.py:153} INFO - Started process (PID=28584) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:57:59.662+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:57:59.732+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:57:59.731+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:58:00.585+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:58:00.582+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:58:00.600+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:58:00.726+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 1.081 seconds
[2023-03-21T01:58:30.888+0000] {processor.py:153} INFO - Started process (PID=28656) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:58:30.889+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:58:30.890+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:58:30.890+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:58:30.927+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:58:30.925+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:58:30.928+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:58:30.958+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-21T01:59:01.020+0000] {processor.py:153} INFO - Started process (PID=28729) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:59:01.027+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:59:01.029+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:59:01.029+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:59:01.090+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:59:01.088+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:59:01.092+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:59:01.158+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.145 seconds
[2023-03-21T01:59:31.872+0000] {processor.py:153} INFO - Started process (PID=28801) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:59:31.875+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T01:59:31.876+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:59:31.876+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:59:31.929+0000] {logging_mixin.py:137} INFO - [2023-03-21T01:59:31.927+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T01:59:31.931+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T01:59:31.996+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.130 seconds
[2023-03-21T02:00:02.210+0000] {processor.py:153} INFO - Started process (PID=28874) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:00:02.212+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:00:02.214+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:00:02.213+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:00:02.257+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:00:02.255+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:00:02.259+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:00:02.295+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-21T02:00:32.551+0000] {processor.py:153} INFO - Started process (PID=28947) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:00:32.552+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:00:32.554+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:00:32.554+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:00:32.588+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:00:32.586+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:00:32.590+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:00:32.626+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-21T02:01:02.905+0000] {processor.py:153} INFO - Started process (PID=29020) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:01:02.907+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:01:02.909+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:01:02.909+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:01:02.957+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:01:02.955+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:01:02.958+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:01:02.994+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.099 seconds
[2023-03-21T02:01:33.500+0000] {processor.py:153} INFO - Started process (PID=29093) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:01:33.502+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:01:33.503+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:01:33.503+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:01:33.545+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:01:33.544+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:01:33.546+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:01:33.568+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.072 seconds
[2023-03-21T02:02:04.081+0000] {processor.py:153} INFO - Started process (PID=29166) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:02:04.083+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:02:04.094+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:02:04.093+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:02:04.145+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:02:04.143+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:02:04.147+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:02:04.231+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.157 seconds
[2023-03-21T02:02:35.123+0000] {processor.py:153} INFO - Started process (PID=29240) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:02:35.124+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:02:35.126+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:02:35.126+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:02:35.163+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:02:35.161+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:02:35.164+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:02:35.212+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-21T02:03:05.695+0000] {processor.py:153} INFO - Started process (PID=29313) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:03:05.697+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:03:05.698+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:03:05.698+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:03:05.736+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:03:05.734+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:03:05.737+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:03:05.774+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.086 seconds
[2023-03-21T02:03:35.953+0000] {processor.py:153} INFO - Started process (PID=29386) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:03:35.955+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:03:35.957+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:03:35.956+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:03:36.241+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:03:36.239+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:03:36.242+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:03:36.368+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.425 seconds
[2023-03-21T02:04:06.605+0000] {processor.py:153} INFO - Started process (PID=29452) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:04:06.612+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:04:06.616+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:04:06.615+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:04:06.731+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:04:06.729+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:04:06.733+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:04:06.796+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.197 seconds
[2023-03-21T02:04:37.423+0000] {processor.py:153} INFO - Started process (PID=29516) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:04:37.432+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:04:37.437+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:04:37.436+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:04:37.844+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:04:37.841+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:04:37.847+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:04:37.941+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.525 seconds
[2023-03-21T02:05:15.718+0000] {processor.py:153} INFO - Started process (PID=29589) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:05:15.730+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:05:15.744+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:05:15.743+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:05:16.116+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:05:16.113+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:05:16.126+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:05:16.240+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.538 seconds
[2023-03-21T02:05:47.868+0000] {processor.py:153} INFO - Started process (PID=29662) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:05:47.881+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:05:47.887+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:05:47.887+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:05:48.071+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:05:48.066+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:05:48.079+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:05:48.203+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.354 seconds
[2023-03-21T02:06:18.686+0000] {processor.py:153} INFO - Started process (PID=29718) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:06:18.690+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:06:18.693+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:06:18.693+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:06:18.788+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:06:18.785+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:06:18.789+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:06:18.867+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.196 seconds
[2023-03-21T02:06:48.989+0000] {processor.py:153} INFO - Started process (PID=29791) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:06:48.990+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:06:48.992+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:06:48.992+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:06:49.033+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:06:49.030+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:06:49.035+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:06:49.073+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.091 seconds
[2023-03-21T02:07:19.843+0000] {processor.py:153} INFO - Started process (PID=29865) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:07:19.845+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:07:19.854+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:07:19.853+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:07:19.962+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:07:19.960+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:07:19.964+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:07:20.051+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.216 seconds
[2023-03-21T02:07:50.274+0000] {processor.py:153} INFO - Started process (PID=29938) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:07:50.275+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:07:50.278+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:07:50.277+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:07:50.324+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:07:50.322+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:07:50.325+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:07:50.363+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-21T02:08:20.926+0000] {processor.py:153} INFO - Started process (PID=30011) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:08:20.928+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:08:20.930+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:08:20.929+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:08:20.977+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:08:20.975+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:08:20.979+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:08:21.026+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.107 seconds
[2023-03-21T02:08:51.497+0000] {processor.py:153} INFO - Started process (PID=30084) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:08:51.503+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:08:51.506+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:08:51.505+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:08:51.553+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:08:51.551+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:08:51.555+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:08:51.598+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.107 seconds
[2023-03-21T02:09:22.074+0000] {processor.py:153} INFO - Started process (PID=30150) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:09:22.077+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:09:22.079+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:09:22.079+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:09:22.138+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:09:22.136+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:09:22.140+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:09:22.220+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.159 seconds
[2023-03-21T02:09:52.833+0000] {processor.py:153} INFO - Started process (PID=30215) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:09:52.837+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:09:52.841+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:09:52.840+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:09:52.888+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:09:52.885+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:09:52.901+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:09:52.944+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.118 seconds
[2023-03-21T02:10:23.235+0000] {processor.py:153} INFO - Started process (PID=30287) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:10:23.236+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:10:23.238+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:10:23.238+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:10:23.279+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:10:23.277+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:10:23.280+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:10:23.332+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.106 seconds
[2023-03-21T02:10:53.859+0000] {processor.py:153} INFO - Started process (PID=30357) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:10:53.861+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:10:53.863+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:10:53.863+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:10:53.949+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:10:53.946+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:10:53.952+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:10:54.017+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.165 seconds
[2023-03-21T02:11:25.024+0000] {processor.py:153} INFO - Started process (PID=30430) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:11:25.026+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:11:25.028+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:11:25.028+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:11:25.092+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:11:25.089+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:11:25.094+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:11:25.155+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.139 seconds
[2023-03-21T02:11:55.330+0000] {processor.py:153} INFO - Started process (PID=30496) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:11:55.332+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:11:55.334+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:11:55.334+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:11:55.370+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:11:55.368+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:11:55.370+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:11:55.402+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-21T02:12:25.642+0000] {processor.py:153} INFO - Started process (PID=30569) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:12:25.644+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:12:25.646+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:12:25.645+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:12:25.697+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:12:25.695+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:12:25.699+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:12:25.782+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.159 seconds
[2023-03-21T02:12:56.465+0000] {processor.py:153} INFO - Started process (PID=30633) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:12:56.481+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:12:56.483+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:12:56.482+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:12:56.592+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:12:56.589+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:12:56.604+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:12:56.701+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.245 seconds
[2023-03-21T02:13:27.512+0000] {processor.py:153} INFO - Started process (PID=30711) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:13:27.524+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:13:27.526+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:13:27.525+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:13:27.570+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:13:27.568+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:13:27.572+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:13:27.631+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.129 seconds
[2023-03-21T02:13:58.031+0000] {processor.py:153} INFO - Started process (PID=30778) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:13:58.033+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:13:58.044+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:13:58.044+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:13:58.123+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:13:58.120+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:13:58.124+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:13:58.210+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.185 seconds
[2023-03-21T02:14:28.558+0000] {processor.py:153} INFO - Started process (PID=30851) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:14:28.563+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:14:28.565+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:14:28.564+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:14:28.607+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:14:28.605+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:14:28.609+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:14:28.651+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.099 seconds
[2023-03-21T02:14:59.460+0000] {processor.py:153} INFO - Started process (PID=30924) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:14:59.462+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:14:59.463+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:14:59.463+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:14:59.499+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:14:59.497+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:14:59.500+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:14:59.532+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.077 seconds
[2023-03-21T02:15:29.657+0000] {processor.py:153} INFO - Started process (PID=30997) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:15:29.659+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:15:29.661+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:15:29.661+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:15:29.696+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:15:29.694+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:15:29.696+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:15:29.726+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.074 seconds
[2023-03-21T02:16:00.372+0000] {processor.py:153} INFO - Started process (PID=31086) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:16:00.374+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:16:00.375+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:16:00.375+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:16:00.416+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:16:00.414+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:16:00.417+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:16:00.448+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-21T02:16:30.898+0000] {processor.py:153} INFO - Started process (PID=31159) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:16:30.901+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:16:30.903+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:16:30.902+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:16:30.955+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:16:30.951+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:16:30.962+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:16:31.032+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.150 seconds
[2023-03-21T02:17:01.138+0000] {processor.py:153} INFO - Started process (PID=31233) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:17:01.144+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:17:01.148+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:17:01.147+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:17:01.189+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:17:01.187+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:17:01.190+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:17:01.235+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.103 seconds
[2023-03-21T02:17:32.189+0000] {processor.py:153} INFO - Started process (PID=31306) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:17:32.191+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:17:32.192+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:17:32.192+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:17:32.226+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:17:32.225+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:17:32.227+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:17:32.257+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.073 seconds
[2023-03-21T02:18:02.712+0000] {processor.py:153} INFO - Started process (PID=31379) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:18:02.713+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:18:02.714+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:18:02.714+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:18:02.744+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:18:02.742+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:18:02.745+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:18:02.773+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.066 seconds
[2023-03-21T02:18:33.571+0000] {processor.py:153} INFO - Started process (PID=31461) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:18:33.575+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:18:33.581+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:18:33.580+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:18:33.624+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:18:33.622+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:18:33.625+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:18:33.657+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.095 seconds
[2023-03-21T02:19:03.883+0000] {processor.py:153} INFO - Started process (PID=31534) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:19:03.886+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:19:03.888+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:19:03.887+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:19:03.999+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:19:03.996+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:19:04.001+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:19:04.081+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.211 seconds
[2023-03-21T02:19:34.204+0000] {processor.py:153} INFO - Started process (PID=31599) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:19:34.206+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:19:34.207+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:19:34.207+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:19:34.239+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:19:34.237+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:19:34.240+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:19:34.270+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-21T02:20:04.494+0000] {processor.py:153} INFO - Started process (PID=31672) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:20:04.496+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:20:04.497+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:20:04.497+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:20:04.523+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:20:04.522+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:20:04.524+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:20:04.551+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-21T02:20:34.728+0000] {processor.py:153} INFO - Started process (PID=31752) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:20:34.730+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:20:34.731+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:20:34.731+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:20:34.773+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:20:34.771+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:20:34.774+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:20:34.854+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.132 seconds
[2023-03-21T02:21:05.684+0000] {processor.py:153} INFO - Started process (PID=31825) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:21:05.686+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:21:05.688+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:21:05.688+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:21:05.734+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:21:05.732+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:21:05.736+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:21:05.787+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.122 seconds
[2023-03-21T02:21:36.582+0000] {processor.py:153} INFO - Started process (PID=31898) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:21:36.583+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:21:36.585+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:21:36.585+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:21:36.662+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:21:36.659+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:21:36.664+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:21:36.718+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.147 seconds
[2023-03-21T02:22:07.499+0000] {processor.py:153} INFO - Started process (PID=31971) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:22:07.501+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:22:07.502+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:22:07.502+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:22:07.535+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:22:07.534+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:22:07.536+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:22:07.570+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-21T02:22:37.829+0000] {processor.py:153} INFO - Started process (PID=32051) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:22:37.830+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:22:37.832+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:22:37.832+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:22:37.867+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:22:37.865+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:22:37.868+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:22:37.895+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.071 seconds
[2023-03-21T02:23:08.381+0000] {processor.py:153} INFO - Started process (PID=32124) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:23:08.383+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:23:08.384+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:23:08.384+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:23:08.409+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:23:08.408+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:23:08.410+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:23:08.430+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.052 seconds
[2023-03-21T02:23:39.062+0000] {processor.py:153} INFO - Started process (PID=32197) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:23:39.066+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:23:39.068+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:23:39.068+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:23:39.121+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:23:39.119+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:23:39.123+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:23:39.169+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.123 seconds
[2023-03-21T02:24:09.333+0000] {processor.py:153} INFO - Started process (PID=32271) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:24:09.336+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:24:09.340+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:24:09.340+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:24:09.410+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:24:09.408+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:24:09.412+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:24:09.470+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.146 seconds
[2023-03-21T02:24:40.242+0000] {processor.py:153} INFO - Started process (PID=32344) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:24:40.247+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:24:40.251+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:24:40.251+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:24:40.319+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:24:40.316+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:24:40.320+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:24:40.449+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.213 seconds
[2023-03-21T02:25:11.501+0000] {processor.py:153} INFO - Started process (PID=32417) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:25:11.503+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:25:11.505+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:25:11.505+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:25:11.549+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:25:11.547+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:25:11.551+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:25:11.591+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.103 seconds
[2023-03-21T02:25:42.428+0000] {processor.py:153} INFO - Started process (PID=32490) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:25:42.431+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:25:42.434+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:25:42.434+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:25:42.490+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:25:42.488+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:25:42.491+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:25:42.537+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.114 seconds
[2023-03-21T02:26:13.380+0000] {processor.py:153} INFO - Started process (PID=32563) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:26:13.389+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:26:13.391+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:26:13.391+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:26:13.440+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:26:13.438+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:26:13.441+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:26:13.521+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.147 seconds
[2023-03-21T02:26:43.614+0000] {processor.py:153} INFO - Started process (PID=32636) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:26:43.618+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:26:43.621+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:26:43.620+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:26:43.671+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:26:43.669+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:26:43.672+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:26:43.706+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.101 seconds
[2023-03-21T02:27:14.319+0000] {processor.py:153} INFO - Started process (PID=32703) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:27:14.322+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:27:14.324+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:27:14.324+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:27:14.385+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:27:14.383+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:27:14.386+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:27:14.426+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.117 seconds
[2023-03-21T02:27:44.944+0000] {processor.py:153} INFO - Started process (PID=301) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:27:44.945+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:27:44.947+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:27:44.946+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:27:44.982+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:27:44.980+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:27:44.983+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:27:45.020+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.083 seconds
[2023-03-21T02:28:15.205+0000] {processor.py:153} INFO - Started process (PID=381) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:28:15.207+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:28:15.218+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:28:15.218+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:28:15.360+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:28:15.358+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:28:15.362+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:28:15.408+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.236 seconds
[2023-03-21T02:28:45.531+0000] {processor.py:153} INFO - Started process (PID=450) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:28:45.534+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:28:45.538+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:28:45.538+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:28:45.613+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:28:45.609+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:28:45.615+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:28:45.688+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.165 seconds
[2023-03-21T02:29:16.045+0000] {processor.py:153} INFO - Started process (PID=516) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:29:16.050+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:29:16.055+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:29:16.052+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:29:16.132+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:29:16.129+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:29:16.133+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:29:16.175+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.141 seconds
[2023-03-21T02:29:46.235+0000] {processor.py:153} INFO - Started process (PID=582) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:29:46.245+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:29:46.249+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:29:46.249+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:29:46.509+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:29:46.507+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:29:46.511+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:29:46.559+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.331 seconds
[2023-03-21T02:30:17.297+0000] {processor.py:153} INFO - Started process (PID=646) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:30:17.298+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:30:17.303+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:30:17.302+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:30:17.342+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:30:17.340+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:30:17.343+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:30:17.375+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.086 seconds
[2023-03-21T02:30:48.314+0000] {processor.py:153} INFO - Started process (PID=719) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:30:48.317+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:30:48.320+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:30:48.319+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:30:48.365+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:30:48.363+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:30:48.366+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:30:48.402+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.101 seconds
[2023-03-21T02:31:19.121+0000] {processor.py:153} INFO - Started process (PID=792) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:31:19.138+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:31:19.144+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:31:19.143+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:31:19.325+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:31:19.323+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:31:19.328+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:31:19.437+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.328 seconds
[2023-03-21T02:31:49.617+0000] {processor.py:153} INFO - Started process (PID=859) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:31:49.621+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:31:49.623+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:31:49.623+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:31:49.678+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:31:49.675+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:31:49.687+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:31:49.800+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.192 seconds
[2023-03-21T02:32:20.234+0000] {processor.py:153} INFO - Started process (PID=923) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:32:20.236+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:32:20.237+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:32:20.237+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:32:20.279+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:32:20.276+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:32:20.281+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:32:20.321+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.092 seconds
[2023-03-21T02:32:51.251+0000] {processor.py:153} INFO - Started process (PID=997) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:32:51.252+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:32:51.253+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:32:51.253+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:32:51.278+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:32:51.277+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:32:51.279+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:32:51.300+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.054 seconds
[2023-03-21T02:33:21.524+0000] {processor.py:153} INFO - Started process (PID=1079) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:33:21.537+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:33:21.538+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:33:21.538+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:33:21.580+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:33:21.578+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:33:21.581+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:33:21.637+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.132 seconds
[2023-03-21T02:33:52.100+0000] {processor.py:153} INFO - Started process (PID=1149) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:33:52.102+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:33:52.105+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:33:52.105+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:33:52.136+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:33:52.134+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:33:52.138+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:33:52.167+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.072 seconds
[2023-03-21T02:34:22.373+0000] {processor.py:153} INFO - Started process (PID=1216) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:34:22.375+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:34:22.377+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:34:22.376+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:34:22.420+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:34:22.418+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:34:22.421+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:34:22.455+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.094 seconds
[2023-03-21T02:34:52.976+0000] {processor.py:153} INFO - Started process (PID=1290) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:34:52.978+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:34:52.979+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:34:52.979+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:34:53.010+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:34:53.009+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:34:53.011+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:34:53.033+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-21T02:35:23.689+0000] {processor.py:153} INFO - Started process (PID=1363) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:35:23.692+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:35:23.694+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:35:23.694+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:35:23.760+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:35:23.759+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:35:23.761+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:35:23.803+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.122 seconds
[2023-03-21T02:35:54.289+0000] {processor.py:153} INFO - Started process (PID=1443) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:35:54.292+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:35:54.301+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:35:54.300+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:35:54.480+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:35:54.478+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:35:54.482+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:35:54.662+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.393 seconds
[2023-03-21T02:36:24.997+0000] {processor.py:153} INFO - Started process (PID=1510) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:36:25.021+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:36:25.023+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:36:25.022+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:36:25.199+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:36:25.196+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:36:25.208+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:36:25.269+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.279 seconds
[2023-03-21T02:36:56.084+0000] {processor.py:153} INFO - Started process (PID=1581) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:36:56.095+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:36:56.098+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:36:56.098+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:36:56.197+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:36:56.195+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:36:56.201+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:36:56.287+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.210 seconds
[2023-03-21T02:37:26.714+0000] {processor.py:153} INFO - Started process (PID=1653) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:37:26.717+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:37:26.719+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:37:26.719+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:37:26.827+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:37:26.825+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:37:26.830+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:37:26.894+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.205 seconds
[2023-03-21T02:37:57.182+0000] {processor.py:153} INFO - Started process (PID=1725) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:37:57.184+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:37:57.185+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:37:57.185+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:37:57.243+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:37:57.241+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:37:57.246+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:37:57.355+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.179 seconds
[2023-03-21T02:38:27.916+0000] {processor.py:153} INFO - Started process (PID=1798) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:38:27.918+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:38:27.924+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:38:27.923+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:38:27.997+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:38:27.995+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:38:27.998+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:38:28.082+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.189 seconds
[2023-03-21T02:38:58.688+0000] {processor.py:153} INFO - Started process (PID=1871) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:38:58.689+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:38:58.690+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:38:58.690+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:38:58.716+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:38:58.715+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:38:58.717+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:38:58.745+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-21T02:39:29.519+0000] {processor.py:153} INFO - Started process (PID=1944) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:39:29.521+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:39:29.528+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:39:29.528+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:39:29.564+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:39:29.562+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:39:29.565+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:39:29.598+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-21T02:40:00.217+0000] {processor.py:153} INFO - Started process (PID=2019) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:40:00.218+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:40:00.220+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:40:00.219+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:40:00.256+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:40:00.254+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:40:00.257+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:40:00.288+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-21T02:40:31.030+0000] {processor.py:153} INFO - Started process (PID=2092) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:40:31.032+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:40:31.033+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:40:31.033+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:40:31.066+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:40:31.064+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:40:31.067+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:40:31.118+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.094 seconds
[2023-03-21T02:41:01.679+0000] {processor.py:153} INFO - Started process (PID=2181) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:41:01.682+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:41:01.684+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:41:01.684+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:41:01.731+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:41:01.728+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:41:01.732+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:41:01.792+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.120 seconds
[2023-03-21T02:41:32.026+0000] {processor.py:153} INFO - Started process (PID=2254) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:41:32.028+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:41:32.031+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:41:32.030+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:41:32.068+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:41:32.066+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:41:32.069+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:41:32.104+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-21T02:42:02.612+0000] {processor.py:153} INFO - Started process (PID=2327) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:42:02.614+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:42:02.616+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:42:02.616+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:42:02.652+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:42:02.651+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:42:02.653+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:42:02.707+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.099 seconds
[2023-03-21T02:42:33.202+0000] {processor.py:153} INFO - Started process (PID=2401) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:42:33.203+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:42:33.204+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:42:33.204+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:42:33.246+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:42:33.244+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:42:33.247+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:42:33.281+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.083 seconds
[2023-03-21T02:43:03.776+0000] {processor.py:153} INFO - Started process (PID=2474) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:43:03.778+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:43:03.785+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:43:03.783+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:43:03.833+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:43:03.831+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:43:03.835+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:43:03.875+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.105 seconds
[2023-03-21T02:43:34.626+0000] {processor.py:153} INFO - Started process (PID=2547) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:43:34.627+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:43:34.629+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:43:34.628+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:43:34.666+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:43:34.664+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:43:34.667+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:43:34.696+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.076 seconds
[2023-03-21T02:44:04.866+0000] {processor.py:153} INFO - Started process (PID=2620) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:44:04.867+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:44:04.868+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:44:04.868+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:44:04.892+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:44:04.891+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:44:04.893+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:44:04.917+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.055 seconds
[2023-03-21T02:44:35.595+0000] {processor.py:153} INFO - Started process (PID=2694) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:44:35.597+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:44:35.598+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:44:35.598+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:44:35.641+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:44:35.639+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:44:35.643+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:44:35.673+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-21T02:45:06.013+0000] {processor.py:153} INFO - Started process (PID=2767) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:45:06.014+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:45:06.016+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:45:06.015+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:45:06.055+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:45:06.054+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:45:06.057+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:45:06.086+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.079 seconds
[2023-03-21T02:45:36.611+0000] {processor.py:153} INFO - Started process (PID=2840) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:45:36.614+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:45:36.616+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:45:36.616+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:45:36.681+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:45:36.679+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:45:36.683+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:45:36.732+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.136 seconds
[2023-03-21T02:46:06.879+0000] {processor.py:153} INFO - Started process (PID=2929) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:46:06.881+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:46:06.882+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:46:06.882+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:46:06.922+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:46:06.920+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:46:06.924+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:46:06.957+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-21T02:46:37.507+0000] {processor.py:153} INFO - Started process (PID=3002) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:46:37.513+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:46:37.518+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:46:37.518+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:46:37.598+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:46:37.595+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:46:37.616+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:46:37.687+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.187 seconds
[2023-03-21T02:47:08.120+0000] {processor.py:153} INFO - Started process (PID=3074) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:47:08.122+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:47:08.131+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:47:08.130+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:47:08.188+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:47:08.186+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:47:08.189+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:47:08.345+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.232 seconds
[2023-03-21T02:47:38.696+0000] {processor.py:153} INFO - Started process (PID=3147) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:47:38.698+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:47:38.699+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:47:38.699+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:47:38.728+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:47:38.726+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:47:38.729+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:47:38.757+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.067 seconds
[2023-03-21T02:48:09.186+0000] {processor.py:153} INFO - Started process (PID=3221) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:48:09.188+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:48:09.190+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:48:09.189+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:48:09.237+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:48:09.235+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:48:09.238+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:48:09.311+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.134 seconds
[2023-03-21T02:48:39.553+0000] {processor.py:153} INFO - Started process (PID=3293) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:48:39.554+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:48:39.555+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:48:39.555+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:48:39.598+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:48:39.597+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:48:39.599+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:48:39.626+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-21T02:49:09.755+0000] {processor.py:153} INFO - Started process (PID=3366) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:49:09.756+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:49:09.759+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:49:09.758+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:49:09.803+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:49:09.801+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:49:09.804+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:49:09.835+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.087 seconds
[2023-03-21T02:49:39.929+0000] {processor.py:153} INFO - Started process (PID=3436) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:49:39.931+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:49:39.934+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:49:39.933+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:49:39.975+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:49:39.971+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:49:39.977+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:49:40.005+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.086 seconds
[2023-03-21T02:50:10.180+0000] {processor.py:153} INFO - Started process (PID=3519) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:50:10.183+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:50:10.184+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:50:10.184+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:50:10.223+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:50:10.221+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:50:10.225+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:50:10.255+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.079 seconds
[2023-03-21T02:50:40.597+0000] {processor.py:153} INFO - Started process (PID=3599) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:50:40.598+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:50:40.600+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:50:40.600+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:50:40.633+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:50:40.632+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:50:40.634+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:50:40.658+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.070 seconds
[2023-03-21T02:51:11.013+0000] {processor.py:153} INFO - Started process (PID=3672) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:51:11.014+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:51:11.016+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:51:11.016+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:51:11.047+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:51:11.045+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:51:11.048+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:51:11.070+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-21T02:51:41.234+0000] {processor.py:153} INFO - Started process (PID=3745) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:51:41.236+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:51:41.238+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:51:41.238+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:51:41.281+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:51:41.279+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:51:41.282+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:51:41.332+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.105 seconds
[2023-03-21T02:52:12.840+0000] {processor.py:153} INFO - Started process (PID=3817) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:52:12.843+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:52:12.853+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:52:12.853+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:52:13.156+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:52:13.154+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:52:13.165+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:52:13.309+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.477 seconds
[2023-03-21T02:52:43.721+0000] {processor.py:153} INFO - Started process (PID=3891) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:52:43.723+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:52:43.724+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:52:43.724+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:52:43.755+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:52:43.753+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:52:43.756+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:52:43.782+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.065 seconds
[2023-03-21T02:53:14.140+0000] {processor.py:153} INFO - Started process (PID=3964) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:53:14.142+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:53:14.143+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:53:14.143+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:53:14.172+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:53:14.170+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:53:14.173+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:53:14.199+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.063 seconds
[2023-03-21T02:53:44.750+0000] {processor.py:153} INFO - Started process (PID=4046) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:53:44.752+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:53:44.754+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:53:44.754+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:53:44.796+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:53:44.795+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:53:44.798+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:53:44.833+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.094 seconds
[2023-03-21T02:54:15.633+0000] {processor.py:153} INFO - Started process (PID=4119) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:54:15.635+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:54:15.638+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:54:15.637+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:54:15.703+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:54:15.701+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:54:15.705+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:54:15.773+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.148 seconds
[2023-03-21T02:54:45.902+0000] {processor.py:153} INFO - Started process (PID=4199) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:54:45.904+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:54:45.907+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:54:45.906+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:54:45.970+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:54:45.968+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:54:45.971+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:54:46.046+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.149 seconds
[2023-03-21T02:55:16.565+0000] {processor.py:153} INFO - Started process (PID=4272) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:55:16.601+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:55:16.618+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:55:16.618+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:55:16.874+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:55:16.871+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:55:16.885+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:55:17.014+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.467 seconds
[2023-03-21T02:55:48.036+0000] {processor.py:153} INFO - Started process (PID=4345) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:55:48.038+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:55:48.039+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:55:48.039+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:55:48.083+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:55:48.080+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:55:48.084+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:55:48.119+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.090 seconds
[2023-03-21T02:56:19.045+0000] {processor.py:153} INFO - Started process (PID=4421) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:56:19.046+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:56:19.048+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:56:19.048+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:56:19.147+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:56:19.145+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:56:19.148+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:56:19.195+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.164 seconds
[2023-03-21T02:56:49.902+0000] {processor.py:153} INFO - Started process (PID=4502) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:56:49.903+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:56:49.905+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:56:49.905+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:56:49.939+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:56:49.937+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:56:49.941+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:56:49.977+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.080 seconds
[2023-03-21T02:57:20.581+0000] {processor.py:153} INFO - Started process (PID=4575) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:57:20.583+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:57:20.585+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:57:20.585+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:57:20.621+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:57:20.620+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:57:20.622+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:57:20.652+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-21T02:57:51.681+0000] {processor.py:153} INFO - Started process (PID=4655) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:57:51.684+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:57:51.687+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:57:51.687+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:57:51.752+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:57:51.750+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:57:51.753+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:57:51.798+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.126 seconds
[2023-03-21T02:58:22.162+0000] {processor.py:153} INFO - Started process (PID=4728) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:58:22.163+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:58:22.164+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:58:22.164+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:58:22.187+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:58:22.185+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:58:22.188+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:58:22.209+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.050 seconds
[2023-03-21T02:58:52.758+0000] {processor.py:153} INFO - Started process (PID=4801) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:58:52.760+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:58:52.761+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:58:52.761+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:58:52.790+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:58:52.788+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:58:52.791+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:58:52.819+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.067 seconds
[2023-03-21T02:59:23.007+0000] {processor.py:153} INFO - Started process (PID=4874) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:59:23.016+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:59:23.019+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:59:23.019+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:59:23.069+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:59:23.067+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:59:23.071+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:59:23.118+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.119 seconds
[2023-03-21T02:59:53.739+0000] {processor.py:153} INFO - Started process (PID=4946) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:59:53.741+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T02:59:53.744+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:59:53.744+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:59:53.782+0000] {logging_mixin.py:137} INFO - [2023-03-21T02:59:53.780+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T02:59:53.782+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T02:59:53.813+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.082 seconds
[2023-03-21T03:00:24.076+0000] {processor.py:153} INFO - Started process (PID=5019) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:00:24.078+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:00:24.080+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:00:24.080+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:00:24.113+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:00:24.111+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:00:24.114+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:00:24.153+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.085 seconds
[2023-03-21T03:00:54.310+0000] {processor.py:153} INFO - Started process (PID=5101) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:00:54.312+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:00:54.313+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:00:54.313+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:00:54.345+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:00:54.343+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:00:54.346+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:00:54.374+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.068 seconds
[2023-03-21T03:01:24.485+0000] {processor.py:153} INFO - Started process (PID=5181) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:01:24.486+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:01:24.488+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:01:24.488+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:01:24.530+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:01:24.528+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:01:24.532+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:01:24.567+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.088 seconds
[2023-03-21T03:01:55.079+0000] {processor.py:153} INFO - Started process (PID=5252) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:01:55.080+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:01:55.082+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:01:55.082+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:01:55.110+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:01:55.108+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:01:55.111+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:01:55.137+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-21T03:02:25.503+0000] {processor.py:153} INFO - Started process (PID=5325) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:02:25.505+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:02:25.507+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:02:25.507+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:02:25.564+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:02:25.562+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:02:25.566+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:02:25.613+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.126 seconds
[2023-03-21T03:02:55.905+0000] {processor.py:153} INFO - Started process (PID=5398) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:02:55.906+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:02:55.907+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:02:55.907+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:02:55.933+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:02:55.931+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:02:55.934+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:02:56.039+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.138 seconds
[2023-03-21T03:03:26.939+0000] {processor.py:153} INFO - Started process (PID=5471) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:03:26.940+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:03:26.941+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:03:26.941+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:03:26.970+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:03:26.969+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:03:26.971+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:03:26.996+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-21T03:03:57.700+0000] {processor.py:153} INFO - Started process (PID=5560) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:03:57.704+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:03:57.705+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:03:57.705+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:03:57.750+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:03:57.749+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:03:57.752+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:03:57.781+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.087 seconds
[2023-03-21T03:04:28.087+0000] {processor.py:153} INFO - Started process (PID=5633) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:04:28.089+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:04:28.090+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:04:28.090+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:04:28.124+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:04:28.123+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:04:28.125+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:04:28.155+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.072 seconds
[2023-03-21T03:04:59.025+0000] {processor.py:153} INFO - Started process (PID=5706) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:04:59.033+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:04:59.037+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:04:59.037+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:04:59.205+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:04:59.203+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:04:59.206+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:04:59.259+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.241 seconds
[2023-03-21T03:05:29.414+0000] {processor.py:153} INFO - Started process (PID=5779) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:05:29.415+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:05:29.416+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:05:29.416+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:05:29.433+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:05:29.432+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:05:29.434+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:05:29.451+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.040 seconds
[2023-03-21T03:05:59.805+0000] {processor.py:153} INFO - Started process (PID=5852) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:05:59.806+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:05:59.807+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:05:59.807+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:05:59.838+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:05:59.837+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:05:59.839+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:05:59.863+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.062 seconds
[2023-03-21T03:06:30.094+0000] {processor.py:153} INFO - Started process (PID=5936) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:06:30.096+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:06:30.097+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:06:30.097+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:06:30.132+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:06:30.130+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:06:30.133+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:06:30.168+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.078 seconds
[2023-03-21T03:07:01.008+0000] {processor.py:153} INFO - Started process (PID=6016) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:07:01.033+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:07:01.050+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:07:01.049+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:07:01.241+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:07:01.239+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:07:01.247+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:07:01.343+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.342 seconds
[2023-03-21T03:07:31.539+0000] {processor.py:153} INFO - Started process (PID=6082) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:07:31.540+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:07:31.542+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:07:31.542+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:07:31.594+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:07:31.592+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:07:31.595+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:07:31.664+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.135 seconds
[2023-03-21T03:08:01.766+0000] {processor.py:153} INFO - Started process (PID=6146) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:08:01.776+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:08:01.779+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:08:01.778+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:08:01.841+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:08:01.839+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:08:01.843+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:08:01.887+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.125 seconds
[2023-03-21T03:08:32.104+0000] {processor.py:153} INFO - Started process (PID=6219) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:08:32.106+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:08:32.107+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:08:32.107+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:08:32.152+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:08:32.149+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:08:32.153+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:08:32.203+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.104 seconds
[2023-03-21T03:09:02.404+0000] {processor.py:153} INFO - Started process (PID=6291) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:09:02.405+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:09:02.406+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:09:02.406+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:09:02.422+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:09:02.421+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:09:02.423+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:09:02.518+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.117 seconds
[2023-03-21T03:09:32.729+0000] {processor.py:153} INFO - Started process (PID=6380) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:09:32.730+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:09:32.731+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:09:32.731+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:09:32.769+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:09:32.767+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:09:32.770+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:09:32.798+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.074 seconds
[2023-03-21T03:10:03.389+0000] {processor.py:153} INFO - Started process (PID=6453) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:10:03.391+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:10:03.392+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:10:03.392+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:10:03.491+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:10:03.489+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:10:03.494+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:10:03.544+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.163 seconds
[2023-03-21T03:10:34.115+0000] {processor.py:153} INFO - Started process (PID=6527) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:10:34.116+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:10:34.118+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:10:34.118+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:10:34.159+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:10:34.157+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:10:34.160+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:10:34.188+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.077 seconds
[2023-03-21T03:11:04.323+0000] {processor.py:153} INFO - Started process (PID=6600) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:11:04.324+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:11:04.325+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:11:04.325+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:11:04.351+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:11:04.350+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:11:04.352+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:11:04.369+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.050 seconds
[2023-03-21T03:11:34.917+0000] {processor.py:153} INFO - Started process (PID=6675) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:11:34.920+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:11:34.921+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:11:34.921+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:11:34.960+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:11:34.959+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:11:34.961+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:11:34.980+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.067 seconds
[2023-03-21T03:12:06.156+0000] {processor.py:153} INFO - Started process (PID=6742) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:12:06.158+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:12:06.171+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:12:06.171+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:12:06.237+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:12:06.235+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:12:06.239+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:12:06.300+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.162 seconds
[2023-03-21T03:12:36.676+0000] {processor.py:153} INFO - Started process (PID=6806) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:12:36.679+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:12:36.695+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:12:36.694+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:12:36.795+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:12:36.793+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:12:36.798+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:12:36.895+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.236 seconds
[2023-03-21T03:13:07.515+0000] {processor.py:153} INFO - Started process (PID=6878) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:13:07.517+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:13:07.517+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:13:07.517+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:13:07.543+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:13:07.542+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:13:07.544+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:13:07.568+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.057 seconds
[2023-03-21T03:13:38.520+0000] {processor.py:153} INFO - Started process (PID=6944) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:13:38.526+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:13:38.529+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:13:38.529+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:13:39.568+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:13:39.566+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:13:39.584+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:13:39.661+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 1.158 seconds
[2023-03-21T03:14:10.327+0000] {processor.py:153} INFO - Started process (PID=7008) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:14:10.328+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:14:10.329+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:14:10.329+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:14:10.361+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:14:10.359+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:14:10.362+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:14:10.383+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.060 seconds
[2023-03-21T03:14:40.987+0000] {processor.py:153} INFO - Started process (PID=7081) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:14:40.988+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:14:40.990+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:14:40.989+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:14:41.011+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:14:41.010+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:14:41.012+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:14:41.035+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.051 seconds
[2023-03-21T03:15:11.346+0000] {processor.py:153} INFO - Started process (PID=7154) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:15:11.358+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T03:15:11.360+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:15:11.359+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:15:11.621+0000] {logging_mixin.py:137} INFO - [2023-03-21T03:15:11.619+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T03:15:11.636+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T03:15:11.716+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 0.378 seconds
[2023-03-21T04:17:27.213+0000] {processor.py:153} INFO - Started process (PID=7192) to work on /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T04:17:27.235+0000] {processor.py:743} INFO - Processing file /opt/airflow/dags/gcs_to_bq.py for tasks to queue
[2023-03-21T04:17:27.261+0000] {logging_mixin.py:137} INFO - [2023-03-21T04:17:27.260+0000] {dagbag.py:538} INFO - Filling up the DagBag from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T04:17:30.269+0000] {logging_mixin.py:137} INFO - [2023-03-21T04:17:30.266+0000] {dagbag.py:343} ERROR - Failed to import: /opt/airflow/dags/gcs_to_bq.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagbag.py", line 339, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 724, in exec_module
  File "<frozen importlib._bootstrap_external>", line 860, in get_code
  File "<frozen importlib._bootstrap_external>", line 791, in source_to_code
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/gcs_to_bq.py", line 105
    function_code = """
import base64
import json
from google.cloud import storage
from google.cloud import bigquery

def process_message(event, context):
    message = base64.b64decode(event['data']).decode('utf-8')
    data = json.loads(message)
    input_bucket_name = data['input_bucket_name']
    input_object_name = data['input_object_name']
    output_bucket_name = data['output_bucket_name']
    output_object_name = data['output_object_name']
    client = storage.Client()
    input_bucket = client.bucket(input_bucket_name)
    input_blob = input_bucket.blob(input_object_name)
    output_bucket = client.bucket(output_bucket_name)
    output_blob = output_bucket.blob(output_object_name)
    df = pd.read_csv(input_blob.download_as_string())
                       
             
           
                                
                                 

                                    
                                                             
                              
                                                 
                                                 
                                                   
                                                   
                             
                                                   
                                                     
                                                     
                                                        
                                                    ^
SyntaxError: EOF while scanning triple-quoted string literal
[2023-03-21T04:17:30.276+0000] {processor.py:755} WARNING - No viable dags retrieved from /opt/airflow/dags/gcs_to_bq.py
[2023-03-21T04:17:30.937+0000] {processor.py:175} INFO - Processing /opt/airflow/dags/gcs_to_bq.py took 3.731 seconds
